#coding=UTF-8
from enum import Enum
import langchain
from typing import Dict, Any, Generator, List
from langchain.output_parsers import RetryOutputParser  # æ­£ç¡®å¯¼å…¥è·¯å¾„
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableSequence, RunnableBranch, \
    RunnableAssign, RunnableParallel
from langchain.prompts import PromptTemplate
from langchain_nvidia_ai_endpoints import ChatNVIDIA
import re
import time
import os
import json
import img_preprocess
# æ–°å¢Pydanticæ¨¡å‹
from pydantic import BaseModel, Field
from typing import List
# ä¿®æ”¹è§£æå™¨é…ç½®
from langchain.output_parsers import OutputFixingParser
from langchain_core.output_parsers import PydanticOutputParser

NVIDIA_API_KEY = "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"


# ---------- çŠ¶æ€æšä¸¾ä¿®æ­£ ----------
class TourismState(Enum):
    INIT = "åˆå§‹åŒ–"
    SELECT_SPOT = "é€‰æ‹©æ™¯ç‚¹"
    SELECT_ROUTE = "é€‰æ‹©è·¯çº¿"


# ---------- å¤„ç†é“¾ ç¤ºä¾‹ä»£ç  ----------
def build_processing_chain(prompt: PromptTemplate) -> RunnableSequence:
    """å¤„ç†é“¾æ¨¡ç‰ˆ"""
    llm = create_llm()
    return (
            RunnablePassthrough()
            | prompt
            | llm  # ç§»é™¤äº†.streamç›´æ¥è°ƒç”¨
            | RunnableLambda(lambda chunk: chunk.content)  # æå–contentå­—æ®µ
    )


# ---------- å“åº”ç”Ÿæˆ ----------
def format_streaming_response(content: str, options: list = None) -> Generator[Dict[str, Any], None, None]:
    """ä¿®æ­£åçš„æµå¼å“åº”ç”Ÿæˆå™¨"""
    # æŒ‰è‡ªç„¶è¯­ä¹‰åˆ†å‰²ï¼ˆæ ‡ç‚¹ç»“å°¾ä¸ºåˆ†å‰²ç‚¹ï¼‰
    segments = re.findall(r'.+?[ã€‚ï¼ï¼Ÿ\n]|.+?(?=\s|$)', content)
    
    for seg in segments:
        yield {
            "messages": [{
                "role": "assistant",
                "content": seg.strip() + '  \n'  # Markdownæ¢è¡Œ
            }],
            "options": options or []
        }
        time.sleep(0.1)  # æ›´è‡ªç„¶çš„æµå¼é€Ÿåº¦

# ---------- Markdownæ¸²æŸ“ æœªå®è£…----------
def render_markdown(content: str) -> str:
    """å®‰å…¨æ¸²æŸ“Markdownå†…å®¹"""
    sanitized = re.sub(r'[^\w\s\-\.\!\?\u4e00-\u9fa5]', '', content)
    return f"```markdown\n{sanitized}\n```"


def create_llm():
    """åˆ›å»ºLLMå®ä¾‹"""
    # NIMæ¨¡ç‰ˆä»£ç 
    return ChatNVIDIA(
        model="meta/llama-3.3-70b-instruct",
        temperature=0.2,
        top_p=0.7,
        max_tokens=1024,
        nvidia_api_key=NVIDIA_API_KEY
    )


# ---------- æ–°å¢é“¾å¼çŠ¶æ€å¤„ç† ----------
def build_state_chain() -> RunnableBranch:
    """æ„å»ºé“¾å¼çŠ¶æ€å¤„ç†å™¨"""
    return RunnableBranch(
        # INITçŠ¶æ€å¤„ç†
        (lambda s: s["step"] == TourismState.INIT,
         RunnableAssign({
             "messages": RunnableLambda(handle_init) | format_streaming_response,
             "options": RunnableLambda(lambda s: parse_options(s["messages"], "spot")),
             "step": lambda _: TourismState.SELECT_SPOT
         })),

        # GET_PREFERENCEçŠ¶æ€å¤„ç† ç§»é™¤
        # (lambda s: s["step"] == TourismState.GET_PREFERENCE,
        #  RunnableAssign({
        #      "messages": RunnableLambda(handle_preference) | format_streaming_response,
        #      "options": RunnableLambda(lambda s: parse_options(s["messages"], "spot")),
        #      "step": lambda _: TourismState.SELECT_SPOT
        #  })),

        # SELECT_SPOTçŠ¶æ€å¤„ç† 
        (lambda s: s["step"] == TourismState.SELECT_SPOT,
         RunnableAssign({
             "messages": RunnableLambda(handle_spot_selection) | format_streaming_response,
             "options": RunnableLambda(lambda s: parse_options(s["messages"], "route")),
             "step": lambda _: TourismState.SELECT_ROUTE
         })),

        # æ–°å¢SELECT_ROUTEçŠ¶æ€å¤„ç†
        (lambda s: s["step"] == TourismState.SELECT_ROUTE,
         RunnableAssign({
             "messages": RunnableLambda(handle_route_selection) | format_streaming_response,
             "step": lambda _: TourismState.INIT
         })),

        # é»˜è®¤å¤„ç† (æ·»åŠ Trueä½œä¸ºé»˜è®¤æ¡ä»¶)
        (True,
         RunnableAssign({
             "messages": RunnableLambda(handle_unknown) | format_streaming_response,
             "step": lambda _: TourismState.INIT
         }))
    )


# ---------- æ›´æ–°çŠ¶æ€æœºå‡½æ•° ----------
def state_machine(state: Dict, user_input: str) -> Generator[Dict, None, None]:
    """å¢å¼ºå‹çŠ¶æ€æœº"""
    current_state = {
        "step": state.get("step", TourismState.INIT),
        # "preference": state.get("preference"),
        "selected_spot": state.get("selected_spot"),
        "selected_route": state.get("selected_route"),
        "options": state.get("options", []),
        "context": state.get("context", {})  # æ–°å¢ä¸Šä¸‹æ–‡å­˜å‚¨
    }
    handlers = {
        TourismState.INIT: handle_init,
        TourismState.SELECT_SPOT: handle_spot_selection,
        TourismState.SELECT_ROUTE: handle_route_selection
    }

    handler = handlers.get(current_state["step"], handle_unknown)
    response_gen = handler(current_state, user_input)

    try:
        while True:
            chunk = next(response_gen)
            yield chunk
    except StopIteration as e:
        # ä¿®æ­£è¿”å›å€¼å¤„ç†
        new_state = e.value if isinstance(e.value, dict) else current_state
        yield {"messages": [{"role": "assistant", "content": ""}], "final_state": new_state}


# ---------- çŠ¶æ€å¤„ç†å‡½æ•°é‡æ„ ----------
def handle_init(state: Dict, user_input: str) -> Generator[Dict, None, Dict]:
    """åˆå§‹åŒ–å¤„ç†"""
    if not user_input.strip():
        welcome_msg = "ğŸ–ï¸ æ¬¢è¿ä½¿ç”¨æ™ºèƒ½æ—…æ¸¸åŠ©æ‰‹ï¼è¯·æè¿°æ‚¨çš„æ—…è¡Œåå¥½ï¼ˆä¾‹å¦‚ï¼šæƒ³çœ‹çš‡å®¶å»ºç­‘ã€å–œæ¬¢è‡ªç„¶é£å…‰ç­‰ï¼‰"
        yield from format_streaming_response(welcome_msg)
        return {**state, "step": TourismState.INIT}

    # åŸæœ‰GET_PREFERENCEå¤„ç†é€»è¾‘
    image_info = get_rag_multi_model_context("")  # ç¤ºä¾‹ä½¿ç”¨ç©ºå›¾ç‰‡è¾“å…¥
    chain = handle_preference_chain()
    result = chain.invoke({
        "preference_info": user_input,
        "image_info": image_info,
    })

    yield from format_streaming_response(result)

    # æ˜ç¡®è¿”å›æ›´æ–°åçš„çŠ¶æ€
    return {
        **state,
        "step": TourismState.SELECT_SPOT,
        "options": parse_options(result, "spot"),
        "context": {
            "preference_info": user_input,
            "image_info": image_info,
            "timestamp": time.strftime("%Y-%m-%d %H:%M")
        }
    }


class SpotOption(BaseModel):
    spots: List[str] = Field(description="æ™¯ç‚¹åˆ—è¡¨ï¼Œæ ¼å¼ï¼šæ™¯ç‚¹åç§°:æ™¯ç‚¹æè¿°")


class RouteOption(BaseModel):
    routes: List[str] = Field(description="è·¯çº¿åˆ—è¡¨ï¼Œæ ¼å¼ï¼šè·¯çº¿åç§°:è·¯çº¿ç‰¹è‰²")


# æ–°å¢è§£ææç¤ºæ¨¡æ¿
OPTION_PROMPTS = {
    "spot": PromptTemplate(
        input_variables=["content"],
        template="""ä»ä¸‹é¢çš„é€‰é¡¹ä¸­ä¸¥æ ¼æŒ‰æ ¼å¼æå–æ™¯ç‚¹åˆ—è¡¨ï¼š
{content}

è¦æ±‚ï¼š
1. å¿…é¡»è¿”å›ä¸¥æ ¼JSONæ ¼å¼ï¼ŒåŒ…å«spotså­—æ®µçš„æ•°ç»„
2. æ¯ä¸ªå…ƒç´ æ ¼å¼ï¼š"æ™¯ç‚¹åç§°:æ™¯ç‚¹æè¿°"
3. ç¤ºä¾‹ï¼š{{"spots": ["æ•…å®«åšç‰©é™¢:æ˜æ¸…çš‡å®¶å®«æ®¿", "å¤©å›å…¬å›­:çš‡å®¶ç¥­ç¥€åœºæ‰€"]}}
4. å¿…é¡»ä½¿ç”¨åŒå¼•å·ï¼Œç¦æ­¢æ¢è¡Œç¬¦
5. ç»“æœåªè¿”å›jsonï¼Œä¸è¦åšå…¶ä»–è¯´æ˜"""
    ),
    "route": PromptTemplate(
        input_variables=["content"],
        template="""ä»ä¸‹é¢çš„è·¯çº¿æ¨èä¸­ä¸¥æ ¼æŒ‰æ ¼å¼æå–è·¯çº¿åˆ—è¡¨ï¼š
{content}

è¦æ±‚ï¼š
1. å¿…é¡»è¿”å›ä¸¥æ ¼JSONæ ¼å¼ï¼ŒåŒ…å«routeså­—æ®µçš„æ•°ç»„  
2. æ¯ä¸ªå…ƒç´ æ ¼å¼ï¼š"è·¯çº¿åç§°:è·¯çº¿ç‰¹è‰²"
3. ç¤ºä¾‹ï¼š{{"routes": ["æ–‡åŒ–ä¹‹æ—…:æ·±åº¦çš‡å®¶å»ºç­‘è®²è§£", "è‡ªç„¶ä¹‹æ—…:å›­æ—æ™¯è§‚æ¸¸è§ˆ"]}}
4. å¿…é¡»ä½¿ç”¨åŒå¼•å·ï¼Œç¦æ­¢æ¢è¡Œç¬¦
5. ç»“æœåªè¿”å›jsonï¼Œä¸è¦åšå…¶ä»–è¯´æ˜"""
    )
}

# ç‹¬ç«‹çš„å°æ¨¡å‹è§£æå™¨
option_parser_llm = ChatNVIDIA(
    model="meta/llama3-8b-instruct",
    temperature=0.1,
    top_p=0.9,
    max_tokens=512,
    nvidia_api_key=NVIDIA_API_KEY
)
# åˆ›å»ºåŸºç¡€è§£æå™¨
base_spot_parser = PydanticOutputParser(pydantic_object=SpotOption)
base_route_parser = PydanticOutputParser(pydantic_object=RouteOption)
# ä½¿ç”¨OutputFixingParseråŒ…è£¹åŸºç¡€è§£æå™¨
spot_parser = OutputFixingParser.from_llm(parser=base_spot_parser, llm=option_parser_llm)
route_parser = OutputFixingParser.from_llm(parser=base_route_parser, llm=option_parser_llm)

option_parser = {
    "spot": (
            RunnableParallel(
                completion=OPTION_PROMPTS["spot"] | option_parser_llm,
                prompt_value=OPTION_PROMPTS["spot"]
            )
            | RunnableLambda(lambda x: spot_parser.parse(x["completion"].content))
            | RunnableLambda(lambda x: x.spots)
    ),
    "route": (
            RunnableParallel(
                completion=OPTION_PROMPTS["route"] | option_parser_llm,
                prompt_value=OPTION_PROMPTS["route"]
            )
            | RunnableLambda(lambda x: route_parser.parse(x["completion"].content))
            | RunnableLambda(lambda x: x.routes)
    )
}


# ä¿®æ”¹è§£æå‡½æ•°
def parse_options(content: str, option_type: str) -> List[str]:
    """ä½¿ç”¨ä¸“ç”¨æç¤ºè§£æé€‰é¡¹"""
    try:
        # è·å–è§£æä¸­é—´ç»“æœç”¨äºè°ƒè¯•
        raw_output = option_parser[option_type].invoke({"content": content})
        return raw_output
    except Exception as e:
        print(f"âš ï¸è§£æå¤±è´¥")
        # å¢å¼ºå‹æ­£åˆ™åŒ¹é…
        patterns = {
            "spot": r"\d+\.\s+([^ï¼š]+)",
            "route": r"\d+\.\s+([^ï¼š]+)"
        }
        return re.findall(patterns[option_type], content)


def get_rag_multi_model_context(query: str) -> str:
    '''æ ¹æ®ç”¨æˆ·è¾“å…¥ä½¿ç”¨RAGå¤šæ¨¡æ€å¾—åˆ°ä¿¡æ¯'''

    query = '"./data/dataImg/0000215a37942b17.jpg"'
    preprocessor = img_preprocess.Preprocessor()
    result = (preprocessor.preprocess_chain | img_preprocess.print_chain | img_preprocess.search_chain).invoke(query)
    print(result.get("matching_location"))

    try:
        print("rag result" + result)
    except Exception as e:
        print(e)
    print("rag matching_location")
    print(result.get("matching_location"))
    return str(result.get("matching_location")[0])


def handle_preference_chain() -> RunnableSequence:
    """ä¸“ç”¨åå¥½å¤„ç†é“¾"""
    llm = create_llm()
    spot_prompt = PromptTemplate(
        input_variables=["preference_info", "image_info"],  # ç¡®ä¿è¾“å…¥å˜é‡åŒ¹é…
        template="""ä½œä¸ºèµ„æ·±æ—…è¡Œè§„åˆ’å¸ˆï¼Œæ ¹æ®ç”¨æˆ·éœ€æ±‚æ¨è3ä¸ªæ™¯ç‚¹ï¼š
    
    ç”¨æˆ·éœ€æ±‚ï¼š{preference_info}
    åŒæ—¶æ ¹æ®ç”¨æˆ·ä¸Šä¼ çš„å›¾ç‰‡æ£€ç´¢åˆ°äº†ä¸‹é¢çš„åœ°æ–¹ï¼š{image_info}

    è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æ¨èï¼š
    1. [æ™¯ç‚¹åç§°]ï¼š[50å­—ç‰¹è‰²æè¿°]
    2. [æ™¯ç‚¹åç§°]ï¼š[50å­—ç‰¹è‰²æè¿°]
    3. [æ™¯ç‚¹åç§°]ï¼š[50å­—ç‰¹è‰²æè¿°]"""
    )
    return (
            RunnablePassthrough()
            | spot_prompt
            | llm.bind(stop=["\n\n"])
            | RunnableLambda(lambda x: x.content)
    )


def handle_preference(state: Dict, image_input: str = None, user_input: str = None) -> Generator[Dict, None, Dict]:
    """åŠ¨æ€ç”Ÿæˆæ™¯ç‚¹æ¨èï¼ˆçŸ¥è¯†åº“å¢å¼ºç‰ˆï¼‰"""
    # ä»çŸ¥è¯†åº“åŠ¨æ€è·å–ä¸Šä¸‹æ–‡
    image_info = get_rag_multi_model_context(image_input)  # RAGå¤šæ¨¡æ€è§£æ

    # æ­£ç¡®åˆå§‹åŒ–å¤„ç†é“¾
    chain = handle_preference_chain()
    result = chain.invoke({
        "preference_info": user_input,
        "image_info": image_info,
    })  # å‚æ•°åœ¨æ­¤å¤„ä¼ é€’
    
    
    # ç±»å‹å®‰å…¨æ£€æŸ¥
    if not isinstance(result, str):
        result = str(result)
    

    # ç±»å‹å®‰å…¨æ£€æŸ¥
    if not isinstance(result, str):
        result = str(result)
    
    # æµå¼è¾“å‡ºå¤„ç†
    yield from format_streaming_response(result)

    return {
        **state,
        "step": TourismState.SELECT_SPOT,
        "options": parse_options(result, "spot"),
        "context": {
            "preference_info": user_input,
            "image_info": image_info,
            "timestamp": time.strftime("%Y-%m-%d %H:%M")
        }
    }


def spot_selection_parse(user_input: str, options: list, context: dict) -> str:
    """ä½¿ç”¨å°æ¨¡å‹è§£ææ™¯ç‚¹é€‰æ‹©"""
    prompt = PromptTemplate(
        template="ä»ç”¨æˆ·è¾“å…¥ä¸­è§£ææ™¯ç‚¹é€‰æ‹©ï¼š\né€‰é¡¹åˆ—è¡¨ï¼š{options}\nç”¨æˆ·è¾“å…¥ï¼š{user_input}\nè¿”å›[æ™¯ç‚¹åç§°ï¼šç‰¹è‰²æè¿°]",
        input_variables=["options", "user_input"]
    )
    chain = prompt | option_parser_llm | StrOutputParser()
    return chain.invoke({"options": options, "user_input": user_input})


def handle_spot_selection_chain() -> RunnableSequence:
    """ä¸“ç”¨æ™¯ç‚¹å¤„ç†é“¾"""
    llm = create_llm()
    route_prompt = PromptTemplate(
        input_variables=["preference_info", "spot_info"],
        template="""ä½œä¸ºèµ„æ·±æ—…è¡Œè§„åˆ’å¸ˆï¼Œæ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œé€‰å®šæ™¯ç‚¹è®¾è®¡3æ¡ç‰¹è‰²æ—…æ¸¸è·¯çº¿:
    ç”¨æˆ·åå¥½ï¼š{preference_info}
    é€‰å®šæ™¯ç‚¹ï¼š{spot_info}

    è¯·è®¾è®¡3æ¡ç‰¹è‰²æ¸¸è§ˆè·¯çº¿ï¼š
    1. [è·¯çº¿åç§°]ï¼š[è·¯çº¿ç‰¹è‰²ä¸äº®ç‚¹]
    2. [è·¯çº¿åç§°]ï¼š[è·¯çº¿ç‰¹è‰²ä¸äº®ç‚¹]
    3. [è·¯çº¿åç§°]ï¼š[è·¯çº¿ç‰¹è‰²ä¸äº®ç‚¹]"""
    )
    return (
            RunnablePassthrough()
            | route_prompt
            | llm
            | StrOutputParser()
    )


def handle_spot_selection(state: Dict, user_input: str) -> Generator[Dict, None, Dict]:
    """åŠ¨æ€ç”Ÿæˆè·¯çº¿æ¨èï¼ˆå¢å¼ºç‰ˆï¼‰"""
    selected_spot = spot_selection_parse(
        user_input,
        state["options"],
        context=state["context"]
    )

    result = handle_spot_selection_chain().invoke({
        "preference_info": state["context"]["preference_info"],
        "spot_info": selected_spot
    })

    # æµå¼è¾“å‡ºå¤„ç†
    yield from format_streaming_response(result)

    return {
        **state,
        "step": TourismState.SELECT_ROUTE,
        "options": parse_options(result, "route"),
        "context": {
            "preference_info": state["context"]["preference_info"],
            "image_info": state["context"]["image_info"],
            "spot_info": selected_spot,
            "timestamp": time.strftime("%Y-%m-%d %H:%M")
        }
    }


def route_selection_parse(user_input: str, options: list, context: dict) -> str:
    """ä½¿ç”¨å°æ¨¡å‹è§£æè·¯çº¿é€‰æ‹©"""
    prompt = PromptTemplate(
        template="ä»ç”¨æˆ·è¾“å…¥è§£æè·¯çº¿é€‰æ‹©ï¼š\né€‰é¡¹åˆ—è¡¨ï¼š{options}\nç”¨æˆ·è¾“å…¥ï¼š{user_input}\nè¿”å›[è·¯çº¿åç§°ï¼šç‰¹è‰²æè¿°]",
        input_variables=["options", "user_input"]
    )
    chain = prompt | option_parser_llm | StrOutputParser()
    return chain.invoke({"options": options, "user_input": user_input})


def handle_route_selection_chain() -> RunnableSequence:
    """ä¸“ç”¨æ”»ç•¥ç”Ÿæˆé“¾"""
    llm = create_llm()
    guide_prompt = PromptTemplate(
        input_variables=["preference_info", "spot_info", "route_info"],
        template="""ä½œä¸ºèµ„æ·±æ—…è¡Œè§„åˆ’å¸ˆï¼Œæ ¹æ®ä¸‹é¢çš„ä¿¡æ¯ï¼Œåšä¸€ä¸ªæ·±åº¦æ—…æ¸¸æ”»ç•¥ç”Ÿæˆï¼š
        ç”¨æˆ·éœ€æ±‚ï¼š{preference_info}
        é€‰å®šæ™¯ç‚¹ï¼š{spot_info}
        é€‰æ‹©è·¯çº¿ï¼š{route_info}
    è¯·åŒ…å«ä»¥ä¸‹å†…å®¹ï¼Œè¾“å‡ºä¸€ä»½æ’ç‰ˆå¥½çš„æ—…æ¸¸æ”»ç•¥ï¼š
    ## è¡Œç¨‹å®‰æ’ï¼ˆç²¾ç¡®åˆ°å°æ—¶ï¼‰
    ## å¿…ç©é¡¹ç›®ï¼ˆåŒ…å«æ¨èæ˜Ÿçº§ï¼‰
    ## é¤é¥®æ¨è
    ## é¢„ç®—ä¼°ç®—
    ## å®ç”¨è´´å£«"""
    )
    return (
            RunnablePassthrough()
            | guide_prompt
            | llm
            | StrOutputParser()
    )


def handle_route_selection(state: Dict, user_input: str) -> Generator[Dict, None, Dict]:
    """åŠ¨æ€ç”Ÿæˆæ”»ç•¥ï¼ˆå¢å¼ºç‰ˆï¼‰"""

    # è§£æç”¨æˆ·é€‰æ‹©çš„æ™¯ç‚¹
    selected_route = route_selection_parse(
        user_input,
        state["options"],
        context=state["context"]
    )

    # ä½ å¥½
    chain = handle_route_selection_chain()
    result = chain.invoke({
        "preference_info": state["context"]["preference_info"],
        "spot_info": state["context"]["spot_info"],
        "route_info": selected_route
    })

    # æµå¼è¾“å‡ºå¤„ç†
    yield from format_streaming_response(result)
    return {
        **state,
        "step": TourismState.INIT,
        "context": {
            "preference_info": state["context"]["preference_info"],
            "spot_info": state["context"]["spot_info"],
            "image_info": state["context"]["image_info"],
            "route_info": selected_route,
            "timestamp": time.strftime("%Y-%m-%d %H:%M")
        }
    }


def handle_unknown(state: Dict, _: str) -> Generator[Dict, None, None]:
    """æœªçŸ¥çŠ¶æ€å¤„ç†"""
    yield from format_streaming_response("âš ï¸ ç³»ç»Ÿé‡åˆ°æœªçŸ¥çŠ¶æ€ï¼Œæ­£åœ¨é‡ç½®...")
    return {**state, "step": TourismState.INIT}


def test_llm_connection():
    """å¤§æ¨¡å‹è¿é€šæ€§æµ‹è¯•"""
    print("\n=== å¼€å§‹å¤§æ¨¡å‹è¿é€šæ€§æµ‹è¯• ===")
    try:
        llm = create_llm()
        test_prompt = PromptTemplate.from_template("è¯·è¯´ï¼š'æµ‹è¯•æˆåŠŸ'")
        chain = test_prompt | llm

        # å¸¦è¶…æ—¶çš„æµ‹è¯•è¯·æ±‚
        response = chain.invoke({"dummy": ""}).content
        if "æµ‹è¯•æˆåŠŸ" in response:
            print(f"âœ… å¤§æ¨¡å‹è¿æ¥æˆåŠŸï¼å“åº”å†…å®¹ï¼š{response}")
            return True
        else:
            print(f"âš ï¸ å¼‚å¸¸å“åº”ï¼š{response}")
            return False
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥ï¼š{str(e)}")
        print("å¯èƒ½åŸå› ï¼š")
        print("1. APIå¯†é’¥æ— æ•ˆ")
        print("2. ç½‘ç»œè¿æ¥å¼‚å¸¸")
        print("3. æ¨¡å‹æœåŠ¡ä¸å¯ç”¨")
        return False


def test_preference_input(prev_state: Dict) -> Dict:
    """æµ‹è¯•åå¥½è¾“å…¥æµç¨‹"""
    print("\n=== æµ‹è¯•åå¥½è¾“å…¥ ===")
    user_input = "æˆ‘æƒ³çœ‹çš‡å®¶å›­æ—"
    response_gen = state_machine(prev_state, user_input)

    full_response = ""
    state = prev_state  # åˆå§‹åŒ–çŠ¶æ€
    try:
        for chunk in response_gen:
            if chunk.get("final_state"):
                state = chunk["final_state"]
                continue
            content = chunk['messages'][0]['content']
            print(f"\rç³»ç»Ÿï¼š{full_response}{content}", end="", flush=True)
            full_response += content
        print("\n" + "-" * 50)
    except StopIteration as e:
        state = e.value if isinstance(e.value, dict) else state

    # éªŒè¯é€‰é¡¹ç”Ÿæˆ
    assert len(state["options"]) >= 1, "åº”ç”Ÿæˆè‡³å°‘ä¸€ä¸ªæ™¯ç‚¹é€‰é¡¹"
    assert state["step"] == TourismState.SELECT_SPOT, "è¾“å…¥åå¥½ååº”è¿›å…¥æ™¯ç‚¹é€‰æ‹©çŠ¶æ€"
    print("âœ… åå¥½è¾“å…¥æµ‹è¯•é€šè¿‡")
    return state


def test_spot_selection(prev_state: Dict) -> Dict:
    """æµ‹è¯•æ™¯ç‚¹é€‰æ‹©æµç¨‹"""
    print("\n=== æµ‹è¯•æ™¯ç‚¹é€‰æ‹© ===")
    user_input = "é€‰ç¬¬ä¸€ä¸ªæ™¯ç‚¹"
    response_gen = state_machine(prev_state, user_input)

    full_response = ""
    state = prev_state
    try:
        for chunk in response_gen:
            if chunk.get("final_state"):
                state = chunk["final_state"]
                continue
            content = chunk['messages'][0]['content']
            print(f"\rç³»ç»Ÿï¼š{full_response}{content}", end="", flush=True)
            full_response += content
        print("\n" + "-" * 50)
    except StopIteration as e:
        state = e.value if isinstance(e.value, dict) else state

    assert "selected_spot" in state, "åº”è®°å½•å·²é€‰æ™¯ç‚¹"
    assert len(state["options"]) >= 1, "åº”ç”Ÿæˆè‡³å°‘ä¸€ä¸ªè·¯çº¿é€‰é¡¹"
    print("âœ… æ™¯ç‚¹é€‰æ‹©æµ‹è¯•é€šè¿‡")
    return state


def test_route_selection(prev_state: Dict) -> Dict:
    """æµ‹è¯•è·¯çº¿é€‰æ‹©æµç¨‹"""
    print("\n=== æµ‹è¯•è·¯çº¿é€‰æ‹© ===")
    user_input = "é€‰æ–‡åŒ–è·¯çº¿"
    response_gen = state_machine(prev_state, user_input)

    full_response = ""
    state = prev_state  # å…³é”®ä¿®å¤ï¼šåˆå§‹åŒ–çŠ¶æ€
    try:
        for chunk in response_gen:
            if chunk.get("final_state"):
                state = chunk["final_state"]
                continue
            content = chunk['messages'][0]['content']
            print(f"\rç³»ç»Ÿï¼š{full_response}{content}", end="", flush=True)
            full_response += content
        print("\n" + "-" * 50)
    except StopIteration as e:
        state = e.value if isinstance(e.value, dict) else state

    # éªŒè¯æœ€ç»ˆçŠ¶æ€
    assert state["step"] == TourismState.INIT, "å®Œæˆæµç¨‹ååº”é‡ç½®çŠ¶æ€"
    assert "selected_route" in state, "åº”è®°å½•å·²é€‰è·¯çº¿"
    print("âœ… è·¯çº¿é€‰æ‹©æµ‹è¯•é€šè¿‡")
    return state


# ---------- æ–°å¢é“¾å¼çŠ¶æ€æœºå®ç° ----------
def new_state_machine(state: Dict, user_input: str) -> Generator[Dict, None, None]:
    """é“¾å¼çŠ¶æ€æœºï¼ˆæ–°å¢å®ç°ï¼‰"""
    processing_chain = (
            RunnablePassthrough.assign(
                # é¢„å¤„ç†ç”¨æˆ·è¾“å…¥
                processed_input=lambda x: x["user_input"].strip()
            )
            | build_state_chain()
            | RunnableAssign({
        "final_output": lambda x: x["messages"][-1]["content"]
    })
    )

    try:
        # æ‰§è¡Œå¤„ç†é“¾
        result = processing_chain.invoke({
            **state,
            "user_input": user_input,
            "messages": []
        })

        # æµå¼è¾“å‡ºå¤„ç†
        yield from result["messages"]
        yield {"final_state": result}

    except Exception as e:
        yield from format_streaming_response(f"âš ï¸ å¤„ç†å¼‚å¸¸ï¼š{str(e)}")
        yield {"final_state": state}


# æ—§çš„äº¤äº’æ¼”ç¤ºå‡½æ•°
def interactive_demo():
    """å‘½ä»¤è¡Œäº¤äº’æ¼”ç¤º"""
    print("ğŸ›« æ—…æ¸¸åŠ©æ‰‹ æ§åˆ¶å°äº¤äº’æ¨¡å¼ï¼ˆè¾“å…¥exité€€å‡ºï¼‰")
    state = {"step": TourismState.INIT}

    while True:
        try:
            # æ ¹æ®çŠ¶æ€æç¤ºè¾“å…¥
            if state["step"] == TourismState.INIT:
                user_input = input("\n> è¯·æè¿°æ‚¨çš„æ—…æ¸¸åå¥½ï¼ˆå¦‚ï¼šæƒ³çœ‹çš‡å®¶å»ºç­‘ï¼‰ï¼š")
            elif state["step"] == TourismState.SELECT_SPOT:
                print("\næ¨èæ™¯ç‚¹ï¼š")
                for i, spot in enumerate(state["options"], 1):
                    print(f"  {i}. {spot}")
                user_input = input("> è¯·é€‰æ‹©æ™¯ç‚¹ç¼–å·æˆ–åç§°ï¼š")
            elif state["step"] == TourismState.SELECT_ROUTE:
                print("\næ¨èè·¯çº¿ï¼š")
                for i, route in enumerate(state["options"], 1):
                    print(f"  {i}. {route}")
                user_input = input("> è¯·é€‰æ‹©è·¯çº¿ç¼–å·æˆ–åç§°ï¼š")

            if user_input.lower() == "exit":
                break

            # å¤„ç†è¾“å…¥
            full_response = ""
            response_gen = state_machine(state, user_input)
            for chunk in response_gen:
                if chunk.get("final_state"):
                    state = chunk["final_state"]
                    continue
                content = chunk['messages'][0]['content']
                print(f"\rç³»ç»Ÿï¼š{full_response}{content}", end="", flush=True)
                full_response += content

            # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
            if state["step"] == TourismState.INIT and "selected_route" in state:
                print("\n\nâœ… è¡Œç¨‹è§„åˆ’å®Œæˆï¼")
                print("-" * 50)
                print(full_response)
                print("-" * 50)
                state = {"step": TourismState.INIT}  # é‡ç½®çŠ¶æ€

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
            break


def test_option_parser():
    """æµ‹è¯•é€‰é¡¹è§£æå™¨"""
    print("\n=== æµ‹è¯•é€‰é¡¹è§£æå™¨ ===")
    test_content = """ç³»ç»Ÿï¼š1. æ•…å®«åšç‰©é™¢ï¼šä¸­å›½æ˜æ¸…ä¸¤ä»£çš‡å®¶å®«æ®¿å»ºç­‘ç¾¤ï¼Œä¿å­˜ç€ä¸°å¯Œçš„å†å²æ–‡åŒ–é—äº§å’Œçè´µæ–‡ç‰©ã€‚2. å¤©å›å…¬å›­ï¼šåŒ—äº¬è‘—åçš„çš‡å®¶ç¥­ç¥€åœºæ‰€ï¼Œæ‹¥æœ‰ç²¾ç¾çš„å»ºç­‘å’Œä¼˜ç¾çš„è‡ªç„¶ç¯å¢ƒã€‚3. é¢å’Œå›­ï¼šä¸­å›½æœ€å¤§çš„çš‡å®¶å®«è‹‘ï¼Œæ‹¥æœ‰æ¹–å…‰å±±è‰²å’Œç²¾ç¾çš„å»ºç­‘ï¼Œæ˜¯ä¼‘é—²å’Œæ–‡åŒ–ä½“éªŒçš„ç†æƒ³ä¹‹åœ°ã€‚"""

    try:
        # æµ‹è¯•æ­£å¼è§£æå™¨
        print("\nğŸ”§ ä½¿ç”¨æ­£å¼è§£æå™¨ï¼š")
        options = parse_options(test_content, "spot")
        print("è§£æç»“æœï¼š")
        for i, opt in enumerate(options, 1):
            print(f"  {i}. {opt}")

        # æµ‹è¯•å¤‡ç”¨è§£ææ–¹æ¡ˆ
        print("\nğŸ”„ è§¦å‘å¤‡ç”¨æ­£åˆ™æ–¹æ¡ˆï¼š")
        raise Exception("æ¨¡æ‹ŸJSONè§£æå¤±è´¥")  # å¼ºåˆ¶è§¦å‘å¤‡ç”¨æ–¹æ¡ˆ
    except Exception as e:
        print(f"âš ï¸ æ•…æ„è§¦å‘å¼‚å¸¸ï¼š{str(e)}")
        options = re.findall(r"\d+\.\s+\*\*(.*?)\*\*", test_content)
        print("å¤‡ç”¨æ–¹æ¡ˆç»“æœï¼š")
        for i, opt in enumerate(options, 1):
            print(f"  {i}. {opt}")

    print("\nâœ… è§£ææµ‹è¯•å®Œæˆ")


langchain.debug = False
# ---------- æ›´æ–°ä¸»æµç¨‹ ----------
if __name__ == "__main__":
    print("å¼€å§‹å’¯~")
    # åˆå§‹åŒ–å¤§æ¨¡å‹
    llm = create_llm()
    print("âœ… å¤§æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    # test_option_parser()
    # å¯åŠ¨äº¤äº’æ¨¡å¼
    # interactive_demo_with_chain() ä½¿ç”¨çŠ¶æ€é“¾çš„demoï¼Œéœ€è¦DEBUG
    interactive_demo()  # æ—§çŠ¶æ€æœºå‡½æ•°
    # get_rag_multi_model_context("")

