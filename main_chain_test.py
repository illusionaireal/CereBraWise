# %!pip install websockets
# %pip install langchain
# %pip install langchain_nvidia_ai_endpoints
# %pip install langchain_community gradio
# %pip install faiss-cpu
# %pip install pyppeteer
# %pip install markupsafe

from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
import gradio as gr
from enum import Enum
from langchain_community.vectorstores import FAISS
import re
import time
import asyncio
import os
from markupsafe import Markup
from pyppeteer import launch

# åˆå§‹åŒ–å‘é‡å­˜å‚¨
vector_store = None

def init_vector_store():
    global vector_store
    embeddings = NVIDIAEmbeddings(
        model="NV-Embed-QA",   
        nvidia_api_key="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"
    )
    
    documents = [
        "åŒ—äº¬æ•…å®«æ˜¯ä¸­å›½æ˜æ¸…ä¸¤ä»£çš„çš‡å®¶å®«æ®¿",
        "ä¸Šæµ·å¤–æ»©æ˜¯è‘—åçš„å†å²å»ºç­‘ç¾¤",
        "è¥¿å®‰å…µé©¬ä¿‘æ˜¯ç§¦å§‹çš‡çš„é™ªè‘¬å‘",
        "æ­å·è¥¿æ¹–æœ‰åæ™¯åŒ…æ‹¬è‹å ¤æ˜¥æ™“ç­‰",
        "å¹¿å·å¡”æ˜µç§°å°è›®è…°ï¼Œé«˜600ç±³"
    ]
    
    vector_store = FAISS.from_texts(
        texts=documents,
        embedding=embeddings,
        metadatas=[{"source": "knowledge"}] * len(documents)
    )
    return vector_store

# çŠ¶æ€æšä¸¾ä¼˜åŒ–
class State(Enum):
    INIT = 0
    GET_PREFERENCE = 1
    SELECT_SPOT = 2 
    SELECT_ROUTE = 3
    SHOW_GUIDE = 4

# åˆå§‹åŒ–LLM
llm = ChatNVIDIA(
    model="deepseek-ai/deepseek-r1",
    temperature=0.6,
    top_p=0.7,
    max_tokens=4096,
    nvidia_api_key="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"
)

# ä¼˜åŒ–æç¤ºæ¨¡æ¿
spot_prompt = PromptTemplate(
    input_variables=["context", "preference"],
    template="""ä½œä¸ºæ—…è¡Œè§„åˆ’ä¸“å®¶ï¼Œæ ¹æ®ä»¥ä¸‹ä¿¡æ¯æ¨è3ä¸ªæ™¯ç‚¹ï¼š

èƒŒæ™¯çŸ¥è¯†ï¼š
{context}

ç”¨æˆ·åå¥½ï¼š{preference}

è¦æ±‚ï¼š
1. æŒ‰ç¼–å·åˆ—å‡ºé€‰é¡¹ï¼Œæ ¼å¼ï¼š
   1. [æ™¯ç‚¹åç§°]ï¼š[50å­—ç®€ä»‹]
   2. [æ™¯ç‚¹åç§°]ï¼š[50å­—ç®€ä»‹]
   3. [æ™¯ç‚¹åç§°]ï¼š[50å­—ç®€ä»‹]
2. æ¯ä¸ªæ™¯ç‚¹ç”¨1è¡Œæè¿°"""
)

route_prompt = PromptTemplate(
    input_variables=["preference", "spot"],
    template="""ä½œä¸ºä¸€ä½ä¸“ä¸šçš„æ—…æ¸¸è§„åˆ’å¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯è®¾è®¡æ¸¸ç©è·¯çº¿ï¼š

ç”¨æˆ·åå¥½ï¼š{preference}
é€‰æ‹©çš„æ™¯ç‚¹ï¼š{spot}

è¯·è®¾è®¡3ç§ä¸åŒç‰¹è‰²çš„æ¸¸ç©è·¯çº¿ï¼š

è¾“å‡ºè¦æ±‚ï¼š
1. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºä¸‰ä¸ªè·¯çº¿ï¼š
   1. [è·¯çº¿åç§°]ï¼š[ç‰¹è‰²æè¿°ï¼ŒåŒ…å«æ¸¸ç©æ—¶é—´ã€ç‰¹è‰²ä½“éªŒç­‰]
   2. [è·¯çº¿åç§°]ï¼š[ç‰¹è‰²æè¿°ï¼ŒåŒ…å«æ¸¸ç©æ—¶é—´ã€ç‰¹è‰²ä½“éªŒç­‰]
   3. [è·¯çº¿åç§°]ï¼š[ç‰¹è‰²æè¿°ï¼ŒåŒ…å«æ¸¸ç©æ—¶é—´ã€ç‰¹è‰²ä½“éªŒç­‰]
2. æ¯ä¸ªè·¯çº¿å¿…é¡»åŒ…å«å…·ä½“çš„æ—¶é—´å®‰æ’
3. æ¯ä¸ªè·¯çº¿æè¿°æ§åˆ¶åœ¨80å­—ä»¥å†…
4. ç¡®ä¿è¾“å‡ºæ ¼å¼è§„èŒƒï¼Œä¸è¦æœ‰å¤šä½™çš„æ ‡ç‚¹ç¬¦å·

ç¤ºä¾‹è¾“å‡ºï¼š
1. æ–‡åŒ–æ¢ç´¢è·¯çº¿ï¼šæ—¶é•¿1å¤©ï¼Œä¸Šåˆå‚è§‚å¤ªå’Œæ®¿ï¼Œä¸‹åˆæ¸¸è§ˆçå®é¦†ï¼Œä½“éªŒçš‡å®¶æ–‡åŒ–
2. æ‘„å½±ç²¾é€‰è·¯çº¿ï¼šæ—¶é•¿2å¤©åŠï¼Œæ¸…æ™¨æ‹æ‘„é‡‘å…‰ä¸‡å­—å»Šï¼Œåˆåå–æ™¯å¾¡èŠ±å›­ï¼Œè®°å½•å®«å»·ä¹‹ç¾
3. æ·±åº¦ä½“éªŒè·¯çº¿ï¼šæ—¶é•¿3å¤©ï¼Œä¸“ä¸šè®²è§£å¤ªå’Œæ®¿ï¼Œåˆé¤å¾¡è†³æˆ¿ï¼Œä¸‹åˆæ¬£èµå®«å»·æ–‡ç‰©"""
)

guide_prompt = PromptTemplate(
    input_variables=["preference", "spot", "route"],
    template="""ç”¨æˆ·åå¥½ï¼š{preference}
é€‰æ‹©çš„æ™¯ç‚¹ï¼š{spot}
é€‰æ‹©çš„è·¯çº¿ï¼š{route}

è¯·åˆ›å»ºè¯¦ç»†æ”»ç•¥ï¼š
åŒ…å«ï¼š
- è¡Œç¨‹å®‰æ’ï¼ˆæ—¶é—´è¡¨ï¼‰
- å¿…ç©é¡¹ç›®
- ç¾é£Ÿæ¨è
- ä½å®¿å»ºè®®
- é¢„ç®—ä¼°ç®—
è¾“å‡ºæ ¼å¼ï¼šMarkdown"""
)

def parse_options(text):
    """è§£æå¸¦ç¼–å·çš„é€‰é¡¹"""
    return re.findall(r"\d+\.\s+(.*?):", text)

def format_response(message, options=None, markup=False):
    """æ„å»ºæ ‡å‡†åŒ–å“åº”"""
    response = {"message": message}
    if options:
        response["options"] = options
    if markup:
        response["message"] = f"```markdown\n{message}\n```"
    return response

async def get_ai_designed_css(md_content):
    """è°ƒç”¨å¤§æ¨¡å‹ç”ŸæˆCSSæ ·å¼"""
    css_llm = ChatNVIDIA(
        model="deepseek-ai/deepseek-r1",
        temperature=0.7,
        nvidia_api_key="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"
    )
    
    prompt = f"""ä½œä¸ºä¸“ä¸šå¹³é¢è®¾è®¡å¸ˆ,è¯·ä¸ºä»¥ä¸‹Markdownå†…å®¹è®¾è®¡æµ·æŠ¥æ ·å¼:
    {md_content}
    
    è¦æ±‚:
    1. ä½¿ç”¨ä¼˜é›…çš„æ¸å˜èƒŒæ™¯
    2. åˆç†çš„å­—ä½“å±‚çº§å’Œé—´è·
    3. é‡è¦å†…å®¹çªå‡ºæ˜¾ç¤º
    4. é€‚åˆ1080x1920çš„æ‰‹æœºæµ·æŠ¥å°ºå¯¸
    
    è¯·ç›´æ¥è¿”å›CSSä»£ç (ä¸è¦è§£é‡Š)ã€‚
    """
    
    response = await css_llm.agenerate([prompt])
    css = response.generations[0].text
    return css

async def render_md_to_image(md_content, output_path):
    """å°†Markdownæ¸²æŸ“ä¸ºå›¾ç‰‡"""
    css = await get_ai_designed_css(md_content)
    
    html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <style>
            {css}
            /* åŸºç¡€æ ·å¼ */
            body {{
                margin: 0;
                padding: 40px;
                width: 1080px;
                min-height: 1920px;
            }}
            /* MarkdownåŸºç¡€æ ·å¼ */
            h1 {{ margin-bottom: 1em; }}
            p {{ line-height: 1.6; }}
        </style>
    </head>
    <body>
        <div class="poster-container">
            {Markup(md_content)}
        </div>
    </body>
    </html>
    """
    
    try:
        browser = await launch({
            'headless': True,
            'args': ['--no-sandbox', '--disable-setuid-sandbox']
        })
        page = await browser.newPage()
        await page.setViewport({"width": 1080, "height": 1920})
        await page.setContent(html)
        
        # ä½¿ç”¨å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•ä½œä¸ºåŸºå‡†è·¯å¾„
        current_dir = os.path.dirname(os.path.abspath(__file__))
        full_output_path = os.path.join(current_dir, output_path)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(full_output_path), exist_ok=True)
        
        await page.screenshot({
            "path": full_output_path,
            "fullPage": True,
            "type": "jpeg",
            "quality": 90
        })
    except Exception as e:
        print(f"å›¾ç‰‡ç”Ÿæˆé”™è¯¯: {str(e)}")
        raise e
    finally:
        if 'browser' in locals():
            await browser.close()

async def process_step(state, user_input):
    """æ ¸å¿ƒå¤„ç†é€»è¾‘"""
    new_state = state.copy()
    
    try:
        # åˆå§‹çŠ¶æ€
        if state["step"] == State.INIT:
            new_state = {
                "step": State.GET_PREFERENCE,
                "preference": None,
                "spot": None,
                "route": None
            }
            return new_state, format_response("ğŸ–ï¸ è¯·é—®æ‚¨æƒ³è¦ä»€ä¹ˆæ ·çš„æ—…æ¸¸å‘¢ï¼Ÿ")
            
        # è·å–æ—…æ¸¸åå¥½
        elif state["step"] == State.GET_PREFERENCE:
            new_state["preference"] = user_input
            
            # ä½¿ç”¨çŸ¥è¯†åº“å¢å¼ºç”¨æˆ·è¾“å…¥
            if vector_store is None:
                init_vector_store()
            docs = vector_store.similarity_search(user_input, k=3)
            context = "\n".join([d.page_content for d in docs])
            
            # æ„å»ºæ­£ç¡®çš„è¾“å…¥å­—å…¸
            spot_chain = (
                RunnablePassthrough()
                | spot_prompt 
                | llm 
                | RunnableLambda(lambda x: x.content)
            )
            
            result = spot_chain.invoke({
                "context": context,
                "preference": user_input
            })
            
            spots = parse_options(result)
            new_state["step"] = State.SELECT_SPOT
            return new_state, format_response(result, spots)
            
        # è·¯çº¿æ¨èæ­¥éª¤
        elif state["step"] == State.SELECT_SPOT:
            new_state["spot"] = user_input
            
            route_chain = (
                RunnablePassthrough()
                | route_prompt
                | llm
                | RunnableLambda(lambda x: x.content)
            )
            
            result = route_chain.invoke({
                "preference": state.get("preference", ""),
                "spot": user_input
            })
            
            routes = parse_options(result)
            new_state["step"] = State.SELECT_ROUTE
            return new_state, format_response(result, routes)
            
        # æ”»ç•¥ç”Ÿæˆæ­¥éª¤
        elif state["step"] == State.SELECT_ROUTE:
            new_state["route"] = user_input
            
            guide_chain = (
                RunnablePassthrough()
                | guide_prompt
                | llm
                | RunnableLambda(lambda x: x.content)
            )
            
            result = guide_chain.invoke({
                "preference": state.get("preference", ""),
                "spot": state.get("spot", ""),
                "route": user_input
            })

            # æå–markdownå†…å®¹
            md_content = result.strip("```markdown\n").strip("```")
            
            # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶å
            output_path = f"travel_guide_{int(time.time())}.jpg"
            
            # æ¸²æŸ“ä¸ºå›¾ç‰‡
            await render_md_to_image(md_content, output_path)
            
            new_state["step"] = State.INIT
            return new_state, format_response(
                f"æ”»ç•¥å·²ç”Ÿæˆ!\n\n{result}\n\nå›¾ç‰‡å·²ä¿å­˜è‡³: {output_path}", 
                markup=True
            )
            
    except Exception as e:
        print(f"Error details: {e}")
        return state, format_response(f"âš ï¸ å‡ºé”™ï¼š{str(e)}ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    return state, format_response("æœªçŸ¥çŠ¶æ€")

# æµ‹è¯•ä»£ç 
async def test_travel_assistant():
    print("å¼€å§‹æµ‹è¯•...")
    
    # åˆå§‹åŒ–çŠ¶æ€
    state = {"step": State.INIT}
    print("åˆå§‹çŠ¶æ€:", state)
    
    # æµ‹è¯•åˆå§‹çŠ¶æ€
    state, response = await process_step(state, "")
    print("åˆå§‹å“åº”:", response)
    print("æ›´æ–°åçŠ¶æ€:", state)
    print("ç­‰å¾…15ç§’...")
    for i in range(15, 0, -1):
        print(f"\rå€’è®¡æ—¶: {i}ç§’", end="", flush=True)
        await asyncio.sleep(1)
    print("\nç­‰å¾…å®Œæˆ")

    # æµ‹è¯•è·å–åå¥½
    print("\næµ‹è¯•è·å–åå¥½...")
    state, response = await process_step(state, "æˆ‘æƒ³å»ä¸€ä¸ªå†å²æ–‡åŒ–æ™¯ç‚¹ï¼Œæœ€å¥½èƒ½ä½“éªŒå¤ä»£å®«å»·æ–‡åŒ–")
    print("åå¥½å“åº”:", response)
    print("æ›´æ–°åçŠ¶æ€:", state)
    print("ç­‰å¾…15ç§’...")
    for i in range(15, 0, -1):
        print(f"\rå€’è®¡æ—¶: {i}ç§’", end="", flush=True)
        await asyncio.sleep(1)
    print("\nç­‰å¾…å®Œæˆ")

    # æµ‹è¯•é€‰æ‹©æ™¯ç‚¹
    print("\næµ‹è¯•é€‰æ‹©æ™¯ç‚¹...")
    state, response = await process_step(state, "åŒ—äº¬æ•…å®«")
    print("æ™¯ç‚¹å“åº”:", response)
    print("æ›´æ–°åçŠ¶æ€:", state)
    print("ç­‰å¾…15ç§’...")
    for i in range(15, 0, -1):
        print(f"\rå€’è®¡æ—¶: {i}ç§’", end="", flush=True)
        await asyncio.sleep(1)
    print("\nç­‰å¾…å®Œæˆ")

    # æµ‹è¯•é€‰æ‹©è·¯çº¿
    print("\næµ‹è¯•é€‰æ‹©è·¯çº¿...")
    state, response = await process_step(state, "æ•…å®«æ·±åº¦ä¸€æ—¥æ¸¸")
    print("è·¯çº¿å“åº”:", response)
    print("æ›´æ–°åçŠ¶æ€:", state)
if __name__ == "__main__":
    asyncio.run(test_travel_assistant())
