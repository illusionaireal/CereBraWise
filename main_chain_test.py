# %!pip install websockets
# %pip install langchain
# %pip install langchain_nvidia_ai_endpoints
# %pip install langchain_community gradio
# %pip install faiss-cpu
# %pip install pyppeteer
# %pip install markupsafe

from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings
import gradio as gr
from enum import Enum
from langchain_community.vectorstores import FAISS
import re
import time
import asyncio
import os
from markupsafe import Markup
from pyppeteer import launch

# åˆå§‹åŒ–å‘é‡å­˜å‚¨
vector_store = None


def init_vector_store():
    global vector_store
    embeddings = NVIDIAEmbeddings(
        model="NV-Embed-QA",
        nvidia_api_key="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"
    )

    documents = [
        "åŒ—äº¬æ•…å®«æ˜¯ä¸­å›½æ˜æ¸…ä¸¤ä»£çš„çš‡å®¶å®«æ®¿",
        "ä¸Šæµ·å¤–æ»©æ˜¯è‘—åçš„å†å²å»ºç­‘ç¾¤",
        "è¥¿å®‰å…µé©¬ä¿‘æ˜¯ç§¦å§‹çš‡çš„é™ªè‘¬å‘",
        "æ­å·è¥¿æ¹–æœ‰åæ™¯åŒ…æ‹¬è‹å ¤æ˜¥æ™“ç­‰",
        "å¹¿å·å¡”æ˜µç§°å°è›®è…°ï¼Œé«˜600ç±³"
    ]

    vector_store = FAISS.from_texts(
        texts=documents,
        embedding=embeddings,
        metadatas=[{"source": "knowledge"}] * len(documents)
    )
    return vector_store


# çŠ¶æ€æšä¸¾ä¼˜åŒ–
class State(Enum):
    INIT = 0
    GET_PREFERENCE = 1
    SELECT_SPOT = 2
    SELECT_ROUTE = 3
    SHOW_GUIDE = 4


# ========== åˆå§‹åŒ–LLM ==========
llm = ChatNVIDIA(
    model="deepseek-ai/deepseek-r1",
    temperature=0.6,
    top_p=0.7,
    max_tokens=4096,
    nvidia_api_key="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"
)


# æ–°å¢è§£æå™¨ç»„ä»¶
class SmartOptionParser:
    def __init__(self):
        self.parser_llm = ChatNVIDIA(
            model="mistralai/mistral-7b-instruct-v0.2",
            temperature=0.1,
            max_tokens=20,
            nvidia_api_key="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"
        )
        self.prompt_template = ChatPromptTemplate.from_messages([
            ("system", "æ ¹æ®ç”¨æˆ·è¾“å…¥è§£æå‡ºé€‰é¡¹ç¼–å·ï¼ˆæ•°å­—ï¼‰ã€‚åªéœ€è¿”å›æ•°å­—ï¼Œä¸è¦è§£é‡Šã€‚\né€‰é¡¹åˆ—è¡¨ï¼š{options}"),
            ("human", "{input}")
        ])
        self.chain = self.prompt_template | self.parser_llm | StrOutputParser()

    def parse_async(self, user_input: str, options: list) -> int:
        try:
            response = self.chain.ainvoke({
                "input": user_input,
                "options": "\n".join([f"{i + 1}. {opt}" for i, opt in enumerate(options)])
            })

            # å¤„ç†ä¸­æ–‡æ•°å­—
            chinese_num_map = {"ä¸€": 1, "äºŒ": 2, "ä¸‰": 3, "å››": 4, "äº”": 5}
            if response.strip() in chinese_num_map:
                return chinese_num_map[response.strip()]

            return int(re.search(r"\d+", response).group())
        except Exception as e:
            print(f"æ™ºèƒ½è§£æå¤±è´¥: {str(e)}, åŸå§‹å“åº”: {response}")
            raise ValueError("æ— æ³•ç†è§£æ‚¨çš„é€‰æ‹©ï¼Œè¯·ä½¿ç”¨æ•°å­—ç¼–å·é€‰æ‹©ï¼ˆå¦‚ï¼š1ã€2ã€3ï¼‰")
# åˆå§‹åŒ–è§£æå™¨
option_parser = SmartOptionParser()


# ä¼˜åŒ–æç¤ºæ¨¡æ¿
spot_prompt = PromptTemplate(
    input_variables=["context", "preference"],
    template="""ä½œä¸ºæ—…è¡Œè§„åˆ’ä¸“å®¶ï¼Œæ ¹æ®ä»¥ä¸‹ä¿¡æ¯æ¨è3ä¸ªæ™¯ç‚¹ï¼š

èƒŒæ™¯çŸ¥è¯†ï¼š
{context}

ç”¨æˆ·åå¥½ï¼š{preference}

è¦æ±‚ï¼š
1. æŒ‰ç¼–å·åˆ—å‡ºé€‰é¡¹ï¼Œæ ¼å¼ï¼š
   1. [æ™¯ç‚¹åç§°]ï¼š[50å­—ç®€ä»‹]
   2. [æ™¯ç‚¹åç§°]ï¼š[50å­—ç®€ä»‹]
   3. [æ™¯ç‚¹åç§°]ï¼š[50å­—ç®€ä»‹]
2. æ¯ä¸ªæ™¯ç‚¹ç”¨1è¡Œæè¿°"""
)

route_prompt = PromptTemplate(
    input_variables=["preference", "spot"],
    template="""ä½œä¸ºä¸€ä½ä¸“ä¸šçš„æ—…æ¸¸è§„åˆ’å¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯è®¾è®¡æ¸¸ç©è·¯çº¿ï¼š

ç”¨æˆ·åå¥½ï¼š{preference}
é€‰æ‹©çš„æ™¯ç‚¹ï¼š{spot}

è¯·è®¾è®¡3ç§ä¸åŒç‰¹è‰²çš„æ¸¸ç©è·¯çº¿ï¼š

è¾“å‡ºè¦æ±‚ï¼š
1. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºä¸‰ä¸ªè·¯çº¿ï¼š
   1. [è·¯çº¿åç§°]ï¼š[ç‰¹è‰²æè¿°ï¼ŒåŒ…å«æ¸¸ç©æ—¶é—´ã€ç‰¹è‰²ä½“éªŒç­‰]
   2. [è·¯çº¿åç§°]ï¼š[ç‰¹è‰²æè¿°ï¼ŒåŒ…å«æ¸¸ç©æ—¶é—´ã€ç‰¹è‰²ä½“éªŒç­‰]
   3. [è·¯çº¿åç§°]ï¼š[ç‰¹è‰²æè¿°ï¼ŒåŒ…å«æ¸¸ç©æ—¶é—´ã€ç‰¹è‰²ä½“éªŒç­‰]
2. æ¯ä¸ªè·¯çº¿å¿…é¡»åŒ…å«å…·ä½“çš„æ—¶é—´å®‰æ’
3. æ¯ä¸ªè·¯çº¿æè¿°æ§åˆ¶åœ¨80å­—ä»¥å†…
4. ç¡®ä¿è¾“å‡ºæ ¼å¼è§„èŒƒï¼Œä¸è¦æœ‰å¤šä½™çš„æ ‡ç‚¹ç¬¦å·

ç¤ºä¾‹è¾“å‡ºï¼š
1. æ–‡åŒ–æ¢ç´¢è·¯çº¿ï¼šæ—¶é•¿1å¤©ï¼Œä¸Šåˆå‚è§‚å¤ªå’Œæ®¿ï¼Œä¸‹åˆæ¸¸è§ˆçå®é¦†ï¼Œä½“éªŒçš‡å®¶æ–‡åŒ–
2. æ‘„å½±ç²¾é€‰è·¯çº¿ï¼šæ—¶é•¿2å¤©åŠï¼Œæ¸…æ™¨æ‹æ‘„é‡‘å…‰ä¸‡å­—å»Šï¼Œåˆåå–æ™¯å¾¡èŠ±å›­ï¼Œè®°å½•å®«å»·ä¹‹ç¾
3. æ·±åº¦ä½“éªŒè·¯çº¿ï¼šæ—¶é•¿3å¤©ï¼Œä¸“ä¸šè®²è§£å¤ªå’Œæ®¿ï¼Œåˆé¤å¾¡è†³æˆ¿ï¼Œä¸‹åˆæ¬£èµå®«å»·æ–‡ç‰©"""
)

guide_prompt = PromptTemplate(
    input_variables=["preference", "spot", "route"],
    template="""ç”¨æˆ·åå¥½ï¼š{preference}
é€‰æ‹©çš„æ™¯ç‚¹ï¼š{spot}
é€‰æ‹©çš„è·¯çº¿ï¼š{route}

è¯·åˆ›å»ºè¯¦ç»†æ”»ç•¥ï¼š
åŒ…å«ï¼š
- è¡Œç¨‹å®‰æ’ï¼ˆæ—¶é—´è¡¨ï¼‰
- å¿…ç©é¡¹ç›®
- ç¾é£Ÿæ¨è
- ä½å®¿å»ºè®®
- é¢„ç®—ä¼°ç®—
è¾“å‡ºæ ¼å¼ï¼šMarkdown"""
)

def generate_guide(state):
    """ç”Ÿæˆæ—…æ¸¸æ”»ç•¥"""
    try:
        guide_chain = (
            RunnablePassthrough()
            | guide_prompt
            | llm.stream
            | RunnableLambda(lambda x: x.content)
        )

        result = guide_chain.invoke({
            "preference": state.get("preference", ""),
            "spot": state.get("spot", ""),
            "route": state.get("route", "")
        })

        # æå–markdownå†…å®¹
        md_content = result.strip("```markdown\n").strip("```")

        # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶å
        output_path = f"travel_guide_{int(time.time())}.jpg"

        # æ¸²æŸ“ä¸ºå›¾ç‰‡
        render_md_to_image(md_content, output_path)

        new_state = state.copy()
        new_state["step"] = State.INIT
        return new_state, format_response(
            f"æ”»ç•¥å·²ç”Ÿæˆ!\n\n{result}\n\nå›¾ç‰‡å·²ä¿å­˜è‡³: {output_path}",
            markup=True
        )
    except Exception as e:
        print(f"æ”»ç•¥ç”Ÿæˆé”™è¯¯: {str(e)}")
        return state, format_response(f"âš ï¸ æ”»ç•¥ç”Ÿæˆå¤±è´¥ï¼š{str(e)}ï¼Œè¯·é‡è¯•")
        
# ä¿®æ”¹parse_optionså‡½æ•°å¢å¼ºå®¹é”™æ€§
def parse_options(text):
    """è§£æå¸¦ç¼–å·çš„é€‰é¡¹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    # æ‰©å±•åŒ¹é…æ¨¡å¼ï¼Œæ”¯æŒä¸­æ–‡ç¼–å·å’Œä¸åŒåˆ†éš”ç¬¦
    return re.findall(r"(?:\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\.\s+(.*?)[ï¼š:]", text)



def format_response(message, options=None, markup=False):
    """æ„å»ºæ ‡å‡†åŒ–å“åº”ï¼ˆç¬¦åˆå®˜æ–¹æ¶ˆæ¯æ ¼å¼ï¼‰"""
    response = {
        "messages": [{
            "role": "assistant", 
            "content": f"```markdown\n{message}\n```" if markup else message
        }]
    }
    if options:
        response["options"] = [{"label": opt, "value": str(i+1)} for i, opt in enumerate(options)]
    return response


def get_ai_designed_css(md_content):
    """è°ƒç”¨å¤§æ¨¡å‹ç”ŸæˆCSSæ ·å¼"""
    css_llm = ChatNVIDIA(
        model="deepseek-ai/deepseek-r1",
        temperature=0.7,
        nvidia_api_key=API_KEY
    )

    prompt = f"""ä½œä¸ºä¸“ä¸šå¹³é¢è®¾è®¡å¸ˆ,è¯·ä¸ºä»¥ä¸‹Markdownå†…å®¹è®¾è®¡æµ·æŠ¥æ ·å¼:
    {md_content}
    
    è¦æ±‚:
    1. ä½¿ç”¨ä¼˜é›…çš„æ¸å˜èƒŒæ™¯
    2. åˆç†çš„å­—ä½“å±‚çº§å’Œé—´è·
    3. é‡è¦å†…å®¹çªå‡ºæ˜¾ç¤º
    4. é€‚åˆ1080x1920çš„æ‰‹æœºæµ·æŠ¥å°ºå¯¸
    
    è¯·ç›´æ¥è¿”å›CSSä»£ç (ä¸è¦è§£é‡Š)ã€‚
    """

    response = css_llm.agenerate([prompt])
    css = response.generations[0].text
    return css


def render_md_to_image(md_content, output_path):
    """å°†Markdownæ¸²æŸ“ä¸ºå›¾ç‰‡"""
    try:
        # åŠ å¼ºå†…å®¹è¿‡æ»¤ï¼ˆä¿®å¤ä¸­æ–‡é”™è¯¯å…³é”®ç‚¹ï¼‰
        md_content = re.sub(r'[^\x00-\x7F\u4e00-\u9fa5\s\.\,\!\?\-\:\/\ï¼ˆ\ï¼‰\ã€Š\ã€‹]', '', md_content)

        # è·å–CSSæ—¶æ·»åŠ ä¸¥æ ¼è¿‡æ»¤
        css = get_ai_designed_css(md_content)
        css = re.findall(r'/\*.*?\*/|{[^{}]*}|.*?{.*?}', css, re.DOTALL)[0]  # æå–æœ‰æ•ˆCSSéƒ¨åˆ†

        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <style>
                {css}
                /* å¼ºåˆ¶åŸºç¡€æ ·å¼ */
                body {{
                    margin: 0 !important;
                    padding: 40px !important;
                    width: 1080px !important;
                    min-height: 1920px !important;
                    font-family: "Microsoft YaHei", sans-serif !important;
                }}
                h1 {{ margin-bottom: 1em !important; }}
                p {{ line-height: 1.6 !important; }}
            </style>
        </head>
        <body>
            <div class="poster-container">
                {Markup(md_content)}
            </div>
        </body>
        </html>
        """

        # ä½¿ç”¨æ›´å¯é çš„æ–‡ä»¶è·¯å¾„å¤„ç†
        current_dir = os.path.dirname(os.path.abspath(__file__))
        full_output_path = os.path.join(current_dir, output_path.lstrip('./'))

        browser =  launch({
            'headless': True,
            'args': ['--no-sandbox', '--disable-setuid-sandbox']
        })
        page = browser.newPage()
        page.setViewport({"width": 1080, "height": 1920})
        page.setContent(html, waitUntil='networkidle0')  # å¢åŠ åŠ è½½ç­‰å¾…

        page.screenshot({
            "path": full_output_path,
            "fullPage": True,
            "type": "jpeg",
            "quality": 90
        })
    except Exception as e:
        print(f"å›¾ç‰‡ç”Ÿæˆé”™è¯¯: {str(e)}")
        raise e
    finally:
        if 'browser' in locals():
            browser.close()


def process_step(state, user_input):
    """æ ¸å¿ƒå¤„ç†é€»è¾‘"""
    new_state = state.copy()

    try:
        # åˆå§‹çŠ¶æ€
        if state["step"] == State.INIT:
            new_state = {
                "step": State.GET_PREFERENCE,
                "preference": None,
                "spot": None,
                "route": None,
                "last_user_input": None
            }
            return new_state, format_response("ğŸ–ï¸ è¯·é—®æ‚¨æƒ³è¦ä»€ä¹ˆæ ·çš„æ—…æ¸¸å‘¢ï¼Ÿ")

        # è·å–æ—…æ¸¸åå¥½
        elif state["step"] == State.GET_PREFERENCE:
            new_state["preference"] = user_input

            # ä½¿ç”¨çŸ¥è¯†åº“å¢å¼ºç”¨æˆ·è¾“å…¥
            if vector_store is None:
                init_vector_store()
            docs = vector_store.similarity_search(user_input, k=3)
            context = "\n".join([d.page_content for d in docs])

            # æ„å»ºæ­£ç¡®çš„è¾“å…¥å­—å…¸
            spot_chain = (
                    RunnablePassthrough()
                    | spot_prompt
                    | llm.stream
                    | RunnableLambda(lambda x: x.content)
            )

            result = spot_chain.invoke({
                "context": context,
                "preference": user_input
            })

            spots = parse_options(result)
            new_state["step"] = State.SELECT_SPOT
            return new_state, format_response(result, spots)

        # è·¯çº¿æ¨èæ­¥éª¤
        elif state["step"] == State.SELECT_SPOT:
            new_state["spot"] = user_input
            new_state["last_user_input"] = user_input

            route_chain = (
                    RunnablePassthrough()
                    | route_prompt
                    | llm.stream
                    | RunnableLambda(lambda x: x.content)
            )

            try:
                result = route_chain.ainvoke({
                    "preference": state.get("preference", ""),
                    "spot": user_input
                })

                print(f"ã€DEBUGã€‘è·¯çº¿æ¨èåŸå§‹è¾“å‡º:\n{result}")
                routes = parse_options(result)  # ä¿ç•™åŸå§‹è§£æä½œä¸ºfallback

                # æ™ºèƒ½è§£æç”¨æˆ·é€‰æ‹©
                selected_index =  option_parser.parse_async(
                    user_input=state["last_user_input"],  # éœ€è¦è®°å½•ç”¨æˆ·åŸå§‹è¾“å…¥
                    options=routes
                )

                if selected_index and routes:
                    new_state["route"] = routes[selected_index - 1]
                    new_state["step"] = State.SELECT_ROUTE
                    return new_state, format_response(result, routes)
            except Exception as e:
                print(f"è·¯çº¿æ¨èé”™è¯¯: {str(e)}")
                return state, format_response(f"âš ï¸ è·¯çº¿ç”Ÿæˆå¤±è´¥ï¼š{str(e)}ï¼Œè¯·é‡æ–°é€‰æ‹©")

        # æ”»ç•¥ç”Ÿæˆæ­¥éª¤
        elif state["step"] == State.SELECT_ROUTE:
            new_state["route"] = user_input

            print(f"User input route: {user_input}")  # è°ƒè¯•è¾“å‡º

            guide_chain = (
                    RunnablePassthrough()
                    | guide_prompt
                    | llm.stream
                    | RunnableLambda(lambda x: x.content)
            )

            print("Guide chain created.")  # è°ƒè¯•è¾“å‡º

            result = guide_chain.invoke({
                "preference": state.get("preference", ""),
                "spot": state.get("spot", ""),
                "route": user_input
            })

            print(f"Raw result from guide_chain: {result}")  # è°ƒè¯•è¾“å‡º

            # æå–markdownå†…å®¹
            md_content = result.strip("```markdown\n").strip("```")
            print(f"Extracted markdown content: {md_content}")  # è°ƒè¯•è¾“å‡º

            # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶å
            output_path = f"travel_guide_{int(time.time())}.jpg"
            print(f"Output image path: {output_path}")  # è°ƒè¯•è¾“å‡º

            # æ¸²æŸ“ä¸ºå›¾ç‰‡
            render_md_to_image(md_content, output_path)
            print(f"Image rendering completed.")  # è°ƒè¯•è¾“å‡º

            new_state["step"] = State.INIT
            return new_state, format_response(
                f"æ”»ç•¥å·²ç”Ÿæˆ!\n\n{result}\n\nå›¾ç‰‡å·²ä¿å­˜è‡³: {output_path}",
                markup=True
            )

    except Exception as e:
        print(f"Error details: {e}")
        return state, format_response(f"âš ï¸ ERRORï¼š{str(e)}ï¼Œè¯·é‡æ–°è¾“å…¥")

    return state, format_response("æœªçŸ¥çŠ¶æ€")


# æµ‹è¯•ä»£ç 
def test_travel_assistant():
    print("å¼€å§‹æµ‹è¯•...")

    # åˆå§‹åŒ–çŠ¶æ€
    state = {"step": State.INIT}
    print("åˆå§‹çŠ¶æ€:", state)

    # æµ‹è¯•åˆå§‹çŠ¶æ€
    new_state, response = process_step(state, "")
    print("åˆå§‹å“åº”:", response)
    print("æ›´æ–°åçŠ¶æ€:", new_state)
    print("ç­‰å¾…15ç§’...")
    for i in range(15, 0, -1):
        print(f"\rå€’è®¡æ—¶: {i}ç§’", end="", flush=True)
        time.sleep(1)
    print("\nç­‰å¾…å®Œæˆ")

    # æµ‹è¯•è·å–åå¥½
    print("\næµ‹è¯•è·å–åå¥½...")
    user_input = "æˆ‘æƒ³å»ä¸€ä¸ªå†å²æ–‡åŒ–æ™¯ç‚¹ï¼Œæœ€å¥½èƒ½ä½“éªŒå¤ä»£å®«å»·æ–‡åŒ–"
    new_state, response_stream = process_step(new_state, user_input)
    print("åå¥½å“åº”: ", end="", flush=True)
    
    # å¤„ç†æµå¼è¾“å‡º
    response_text = ""
    for chunk in response_stream:
        if isinstance(chunk, str):
            response_text += chunk
            print(chunk, end="", flush=True)
    print("\næ›´æ–°åçŠ¶æ€:", new_state)
    
    print("ç­‰å¾…15ç§’...")
    for i in range(15, 0, -1):
        print(f"\rå€’è®¡æ—¶: {i}ç§’", end="", flush=True)
        time.sleep(1)
    print("\nç­‰å¾…å®Œæˆ")

    # æµ‹è¯•é€‰æ‹©æ™¯ç‚¹
    print("\næµ‹è¯•é€‰æ‹©æ™¯ç‚¹...")
    user_input = "æˆ‘é€‰æ‹©ç¬¬2ä¸ªæ™¯ç‚¹"
    new_state, response_stream = process_step(new_state, user_input)
    print("æ™¯ç‚¹å“åº”: ", end="", flush=True)
    
    # å¤„ç†æµå¼è¾“å‡º
    response_text = ""
    for chunk in response_stream:
        if isinstance(chunk, str):
            response_text += chunk
            print(chunk, end="", flush=True)
    print("\næ›´æ–°åçŠ¶æ€:", new_state)
    
    print("ç­‰å¾…15ç§’...")
    for i in range(15, 0, -1):
        print(f"\rå€’è®¡æ—¶: {i}ç§’", end="", flush=True)
        time.sleep(1)
    print("\nç­‰å¾…å®Œæˆ")

    # æµ‹è¯•é€‰æ‹©è·¯çº¿
    print("\næµ‹è¯•é€‰æ‹©è·¯çº¿...")
    user_input = "æˆ‘é€‰æ‹©ç¬¬ä¸‰æ¡è·¯çº¿"
    new_state, response_stream = process_step(new_state, user_input)
    print("è·¯çº¿å“åº”: ", end="", flush=True)
    
    # å¤„ç†æµå¼è¾“å‡º
    response_text = ""
    for chunk in response_stream:
        if isinstance(chunk, str):
            response_text += chunk
            print(chunk, end="", flush=True)
    print("\næ›´æ–°åçŠ¶æ€:", new_state)

if __name__ == "__main__":
    test_travel_assistant()

# def handle_interaction(user_input, history, current_state):
#     try:
#         new_state = current_state.copy()
#
#         # è·å–æµå¼å“åº”
#         response_stream = process_step(current_state, user_input)
#
#         # æ„å»ºå®Œæ•´å“åº”
#         full_response = ""
#         for chunk in response_stream:
#             if isinstance(chunk, dict):
#                 message = chunk.get("message", "")
#                 full_response += message
#                 # å®æ—¶æ›´æ–°èŠå¤©ç•Œé¢
#                 yield (
#                     history + [(user_input, full_response)],
#                     "",
#                     gr.update(visible=bool(chunk.get("options"))),
#                     gr.update(visible=not bool(chunk.get("options"))),
#                     new_state
#                 )
#             else:
#                 full_response += chunk
#                 yield (
#                     history + [(user_input, full_response)],
#                     "",
#                     gr.update(visible=False),
#                     gr.update(visible=True),
#                     new_state
#                 )
#
#     except Exception as e:
#         yield (
#             history + [(user_input, f"âš ï¸ é”™è¯¯ï¼š{str(e)}ï¼Œè¯·é‡è¯•")],
#             "",
#             gr.update(visible=False),
#             gr.update(visible=True),
#             current_state
#         )

        # # ä¿®æ”¹äº‹ä»¶ç»‘å®šä¸ºæµå¼è¾“å‡º
        # input_box.submit(
        #     handle_interaction,
        #     inputs=[input_box, chatbot, state],
        #     outputs=[
        #         chatbot,
        #         input_box,
        #         choices,
        #         input_box,
        #         state
        #     ],
        #     queue=True  # å¯ç”¨é˜Ÿåˆ—ä»¥æ”¯æŒæµå¼è¾“å‡º
        # ).then(
        #     lambda: None,
        #     None,
        #     [input_box],
        #     queue=False
        # )
