{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35536f6-166c-4b89-8136-96417db5be30",
   "metadata": {
    "id": "b35536f6-166c-4b89-8136-96417db5be30"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 8 [评估]：** RAG 评估</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "欢迎学习本课程的最后一个 notebook！您已经在前面的 notebook 中将向量存储解决方案集成到 RAG 工作流中了！您将在本 notebook 采用相同的工作流，并通过 LLM-as-a-Judge 量化地评估 RAG！\n",
    "\n",
    "<br> \n",
    "\n",
    "### **学习目标：**\n",
    "\n",
    "* 了解如何集成之前 notebook 中的技术，量化 RAG 工作流的效果。\n",
    "* **最终练习**：***在课程环境中执行本 notebook，*您就能提交代码！**\n",
    "\n",
    "<br>  \n",
    "\n",
    "### **思考问题：**\n",
    "\n",
    "* 在学习的过程中，请记住我们的指标代表的到底是什么。我们的工作流应该达到这些目标么？通过 LLM 进行评判是否足以评估工作流？具体的某个指标对我们的用例是否有意义？\n",
    "* 如果我们在链中保留 vectorstore-as-a-memory 组件，还能通过评估么？此外，评估是否有助于评估 vectorstore-as-a-memory 的性能？\n",
    "\n",
    "<br>  \n",
    "\n",
    "### **Notebook 版权声明：**\n",
    "\n",
    "* 本 notebook 是 [**NVIDIA 深度学习培训中心**](https://www.nvidia.cn/training/)的课程[**《构建大语言模型 RAG 智能体》**](https://www.nvidia.cn/training/instructor-led-workshops/building-rag-agents-with-llms/)中的一部分，未经 NVIDIA 授权不得分发。\n",
    "\n",
    "<br> \n",
    "\n",
    "### **环境设置：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "w_A3rZOrIeQD",
   "metadata": {
    "id": "w_A3rZOrIeQD"
   },
   "outputs": [],
   "source": [
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu ragas\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "norm_style = Style(bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "pprint2 = partial(console.print, style=norm_style)\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEgV11oZmJGg",
   "metadata": {
    "id": "zEgV11oZmJGg"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 1 部分：** 预发行评估\n",
    "\n",
    "在之前的 Notebook 中，我们成功地结合了多个概念创建文档聊天机器人，以实现高响应、有信息量的交互。然而，用户交互的多样性要求我们进行相对全面的测试，才能真正了解聊天机器人的性能。在不同场景下进行全面测试对于保障系统的功能、通用性及符合用户期望是至关重要的。\n",
    "\n",
    "在定义好聊天机器人的角色并实现了必要的功能之后，可以分多个阶段来进行评估：\n",
    "\n",
    "* **典型应用检测：**先测试与您的用例最贴近的场景。观察您的聊天机器人能否在有限的人工干预下进行可靠的对话。\n",
    "\n",
    "\t+ 此外，识别出应转给人工以检查/监督的边界或者分支情况（比如，换人工来确认交易或执行敏感操作），并执行。\n",
    "\n",
    "* **边界情况（Edge Case）检测：**探索典型场景的边界，确认聊天机器人如何处理不常见但合理的场景。\n",
    "\n",
    "\t+ 在任何公开发布之前，请评估可能构成责任风险的关键边界条件，比如生成不当内容的可能性。\n",
    "\t+ 在所有的输出（甚至是输入）上实现测试好的护栏（guardrails），以限制不良交互，并将用户引导到可预测的对话流上。\n",
    "\n",
    "* **渐进式推出（Progressive Rollout）：**向有限受众推出您的模型（先在内部推出，然后做 [A/B 测试](https://en.wikipedia.org/wiki/A/B_testing)）并实现分析功能，比如用量分析面板和反馈途径（投诉/喜欢/不喜欢/等等）。\n",
    "\n",
    "这三个步骤中，前两个可由一个小团队或个人完成，并在开发过程中持续迭代。不幸的是这需要频繁地进行，还容易发生人为错误。**幸运的是，我们可以借助 LLM-as-a-Judge 范式（formulation）！**\n",
    "\n",
    "*(是的，现在您可能已经不会惊讶了。就是因为 LLM 这么强，所以我们才专门做了这个课程)。*\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 2 部分：** LLM-as-a-Judge 范式\n",
    "\n",
    "在对话式 AI 领域，用 LLM 作为评估器或“评委”已经成为一种对自然语言任务表现进行可配置自动测试的方法了：\n",
    "\n",
    "* LLM 可以模拟一系列交互场景并生成合成数据，从而生成有针对性的输入来激发聊天机器人的一系列行为。\n",
    "* 聊天机器人在合成数据上的反应/检索可由 LLM 进行评估或解析，并且可以强制输出成“通过”/“失败”、相似程度或抽取等格式。\n",
    "* 许多此类结果都可以聚合成一个指标，按“通过评估的百分比”、“从数据源中检索到的相关信息量”、“余弦相似度均值”来解读。\n",
    "\n",
    "这种使用 LLM 测试和量化聊天机器人质量的想法称为 [\"LLM-as-a-Judge\"](https://arxiv.org/abs/2306.05685)，能进行与人类判断高度一致的测评，还能进行微调并大规模应用。\n",
    "\n",
    "**有几个现成的热门框架，包括：**\n",
    "* [**RAGAs (RAG Assessement)**](https://docs.ragas.io/en/stable/)，这是自行评估的一个很好的起点。\n",
    "* [**LangChain Evaluators**](https://python.langchain.com/docs/guides/evaluation/)，这是类似的第一方选项，具有许多可隐式构建的智能体。\n",
    "\n",
    "比起按原样使用链，我们会进行扩展，用一个更定制话的方案进行评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDDNaBA9N3XM",
   "metadata": {
    "id": "fDDNaBA9N3XM"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## **第 3 部分：** [评估准备] 成对数据评估器（Pairwise Evaluator）\n",
    "\n",
    "下面的练习将实现一个简化的 [LangChain 成对字符串评估器（Pairwise String Evaluator）](https://python.langchain.com/docs/guides/evaluation/examples/comparisons)。\n",
    "\n",
    "**为评估 RAG 链做准备，我们需要：**\n",
    "\n",
    "* 拉取文档索引（我们在上一个 notebook 中保存的）。\n",
    "* 重构我们的 RAG 工作流。\n",
    "\n",
    "**具体来说，我们将通过以下步骤实现评判范式：**\n",
    "\n",
    "* 对 RAG 文档池采样，拿到两个文档块。\n",
    "* 用这两个文档块生成一个合成的“基准”问答对。\n",
    "* 用 RAG 智能体生成它自己的答案。\n",
    "* 使用评委 LLM 比较这两种响应，其中，将生成的合成结果作为“标准答案”（ground-truth correct）。\n",
    "\n",
    "**该链应该执行一个简单但功能强大的过程，可基于以下目标进行测试：**\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***我的 RAG 链性能是否优于文档访问受限的聊天机器人。***\n",
    "\n",
    "**这就是要用来做最终评估的系统！**要是想了解一下这个系统是怎么集成到 Autograder 中的，可以看看 [`frontend/frontend_server.py`](frontend/frontend_server.py) 里的实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bh8jaOqak0f",
   "metadata": {
    "id": "1bh8jaOqak0f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **任务 1：** 载入文档索引\n",
    "\n",
    "在本练习中，您将载入之前 notebook 创建的 `docstore_index` 文件。下面这个单元应该能把它按原样加载到存储中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tlE7a2lseLOy",
   "metadata": {
    "id": "tlE7a2lseLOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Constructed aggregate docstore with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">695</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> chunks</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mConstructed aggregate docstore with \u001b[0m\u001b[1;36m695\u001b[0m\u001b[1;38;2;118;185;0m chunks\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG\n",
      "\n",
      "Summary: Graph-RAG constructs a knowledge graph from text chunks to improve retrieval\n",
      "in Large Language Model (LLM)-based question answering. It is particularly\n",
      "useful in domains such as biomedicine, law, and political science, where\n",
      "retrieval often requires multi-hop reasoning over proprietary documents. Some\n",
      "existing Graph-RAG systems construct KNN graphs based on text chunk relevance,\n",
      "but this coarse-grained approach fails to capture entity relationships within\n",
      "texts, leading to sub-par retrieval and generation quality. To address this,\n",
      "recent solutions leverage LLMs to extract entities and relationships from text\n",
      "chunks, constructing triplet-based knowledge graphs. However, this approach\n",
      "incurs significant indexing costs, especially for large document collections.\n",
      "  To ensure a good result accuracy while reducing the indexing cost, we propose\n",
      "KET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small\n",
      "set of key text chunks and leverages an LLM to construct a knowledge graph\n",
      "skeleton. It then builds a text-keyword bipartite graph from all text chunks,\n",
      "serving as a lightweight alternative to a full knowledge graph. During\n",
      "retrieval, KET-RAG searches both structures: it follows the local search\n",
      "strategy of existing Graph-RAG systems on the skeleton while mimicking this\n",
      "search on the bipartite graph to improve retrieval quality. We evaluate eight\n",
      "solutions on two real-world datasets, demonstrating that KET-RAG outperforms\n",
      "all competitors in indexing cost, retrieval effectiveness, and generation\n",
      "quality. Notably, it achieves comparable or superior retrieval quality to\n",
      "Microsoft's Graph-RAG while reducing indexing costs by over an order of\n",
      "magnitude. Additionally, it improves the generation quality by up to 32.4%\n",
      "while lowering indexing costs by around 20%.\n",
      "\n",
      "Page Body: . Specifically, in MuSiQue,\\neach QA pair is associated with 2 golden paragraphs and 20 candi-\\ndate paragraphs, while in HotpotQA, each pair includes 8 distract-\\ning paragraphs. Following prior work [12, 32], we sample 500 QA\\ninstances from each dataset. The number of question hops is 2, 3, 4\\nin MuSiQue and 2 in HotpotQA. The corresponding paragraphs\\nfor all sampled instances are preprocessed into T and compiled\\nas the external corpus for RAG [12]. For MuSiQue and HotpotQA,\\nthe number of preprocessed paragraphs is 6,761 and 20,150, respec-\\ntively, resulting in an overall token count of 751,784 and 618,325.\\nFor evaluation, we measure indexing cost by USD, retrieval quality\\nby Coverage, and generation quality using Exact Match (EM) and\\nF1-Score. Coverage is defined as the proportion of the cases that\\nground-truth answers found within the retrieved context across\\nall QA instances\n"
     ]
    }
   ],
   "source": [
    "## Make sure you have docstore_index.tgz in your working directory\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "!tar xzvf docstore_index.tgz\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "def format_chunk(doc):\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "    )\n",
    "\n",
    "## This printout just confirms that your store has been retrieved\n",
    "pprint(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "pprint(f\"Sample Chunk:\")\n",
    "print(format_chunk(docs[len(docs)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dib0F-t2N4LJ",
   "metadata": {
    "id": "dib0F-t2N4LJ"
   },
   "source": [
    "<br>  \n",
    "\n",
    "### **任务 2：[练习]** 载入 RAG 链\n",
    "\n",
    "现在我们有了索引，可以重新创建上一个 notebook 里的 RAG 智能体了！\n",
    "\n",
    "**主要的调整：**\n",
    "* 为了简单起见，您可以去掉 vectorstore-as-a-memory 组件。加上它会增加开销，还会让练习变复杂。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "XBi6Y8b8aXd2",
   "metadata": {
    "id": "XBi6Y8b8aXd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd love to share something interesting with you!\n",
      "\n",
      "Have you heard about the concept of Retrieval and Reasoning? It's a fascinating topic that involves using artificial intelligence to tackle complex problems. In fact, the text snippet I saw earlier is related to this concept.\n",
      "\n",
      "Imagine being able to break down a problem into smaller steps, searching for relevant information, and using that information to make decisions. That's essentially what Retrieval and Reasoning is all about.\n",
      "\n",
      "For instance, in the example provided, the AI is trying to find a clean knife by searching different locations, thinking about where a knife is more likely to be, and using that information to guide its actions.\n",
      "\n",
      "It's a game-changer for document retrieval and reasoning tasks, enabling AI systems to tackle complex problems more efficiently and effectively. What do you think about this concept?"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "llm = instruct_llm | StrOutputParser()\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "    \" The following information may be useful for your response: \"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    ")\n",
    "\n",
    "def output_puller(inputs):\n",
    "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
    "    if isinstance(inputs, dict):\n",
    "        inputs = [inputs]\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "#####################################################################\n",
    "## TODO: Pull in your desired RAG Chain. Memory not necessary\n",
    "\n",
    "## Chain 1 Specs: \"Hello World\" -> retrieval_chain \n",
    "##   -> {'input': <str>, 'context' : <str>}\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)  ## GIVEN\n",
    "context_getter = itemgetter(\"input\")|docstore.as_retriever()|long_reorder|docs2str ## TODO\n",
    "retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter})\n",
    "\n",
    "## Chain 2 Specs: retrieval_chain -> generator_chain \n",
    "##   -> {\"output\" : <str>, ...} -> output_puller\n",
    "generator_chain = chat_prompt|llm  ## TODO\n",
    "generator_chain = {'output' : generator_chain} | RunnableLambda(output_puller)  ## GIVEN\n",
    "\n",
    "## END TODO\n",
    "#####################################################################\n",
    "\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
    "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8",
   "metadata": {},
   "source": [
    "<br>  \n",
    "\n",
    "### **第 3 步：** 生成合成问答对\n",
    "\n",
    "在本节中，我们可以实现评估流程的前两个步骤：\n",
    "\n",
    "* **对 RAG 文档池采样，拿到两个文档块。**\n",
    "* **用这两个文档块生成一个合成的“基准”问答对。**\n",
    "* 用 RAG 智能体生成它自己的答案。\n",
    "* 使用评委 LLM 比较这两种响应，其中，将生成的合成结果作为“标准答案”（ground-truth correct）。\n",
    "\n",
    "该链应该执行一个简单但功能强大的过程，可基于以下目标进行测试：\n",
    "\n",
    "> 我的 RAG 链性能是否优于文档访问受限的聊天机器人。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ymzuX-DSNvL6",
   "metadata": {
    "id": "ymzuX-DSNvL6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: Can a model that is trained to generate images using diffusion models, conditioned on natural language </span>\n",
       "<span style=\"font-weight: bold\">prompts, successfully transfer the learned visual concepts to downstream tasks, such as image recognition, object </span>\n",
       "<span style=\"font-weight: bold\">classification, and image generation, without relying on explicit supervision?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: Can a model that is trained to generate images using diffusion models, conditioned on natural language \u001b[0m\n",
       "\u001b[1mprompts, successfully transfer the learned visual concepts to downstream tasks, such as image recognition, object \u001b[0m\n",
       "\u001b[1mclassification, and image generation, without relying on explicit supervision?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: Yes, a latent diffusion model (LDM) can achieve state-of-the-art performance on various image synthesis </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">significantly reducing computational requirements. Furthermore, according to research on learning transferable </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">visual models from natural language supervision, a model like CLIP can learn to recognize and classify visual </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">concepts from scratch on a large dataset of image-text pairs, and then transfer those learned concepts to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">downstream tasks without explicit supervision, showing promise for the application of conditioned LDMs to such </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: Yes, a latent diffusion model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLDM\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m can achieve state-of-the-art performance on various image synthesis \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msignificantly reducing computational requirements. Furthermore, according to research on learning transferable \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mvisual models from natural language supervision, a model like CLIP can learn to recognize and classify visual \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconcepts from scratch on a large dataset of image-text pairs, and then transfer those learned concepts to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdownstream tasks without explicit supervision, showing promise for the application of conditioned LDMs to such \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: Can a model that is pre-trained on image-text pairs to learn transferable visual representations from </span>\n",
       "<span style=\"font-weight: bold\">natural language supervision outperform a state-of-the-art diffusion model in image generation tasks, while also </span>\n",
       "<span style=\"font-weight: bold\">being more computationally efficient and reducing the need for additional labeled data?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: Can a model that is pre-trained on image-text pairs to learn transferable visual representations from \u001b[0m\n",
       "\u001b[1mnatural language supervision outperform a state-of-the-art diffusion model in image generation tasks, while also \u001b[0m\n",
       "\u001b[1mbeing more computationally efficient and reducing the need for additional labeled data?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: According to the result presented in Table </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, while the pre-trained model on image-text pairs does not </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">directly outperform the state-of-the-art diffusion model (ADM) in unconditional image generation (FID scores being </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.94</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> for ADM vs </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.56</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> for LDM-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), the model with cross-attention layers (LDM-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">-G) achieved a lower FID score </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.60</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) and a higher IS score (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.87</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) compared to LDM-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. Additionally, the Pre-trained model on image-text pairs </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(CLIP) achieved state-of-the-art results in zero-shot image classification without the need for any dataset </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">specific training, showing the potential of large-scale pre-training on image-text pairs for reducing the need for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">additional labeled data in downstream tasks.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: According to the result presented in Table \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m, while the pre-trained model on image-text pairs does not \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdirectly outperform the state-of-the-art diffusion model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mADM\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m in unconditional image generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mFID scores being \u001b[0m\n",
       "\u001b[1;36m10.94\u001b[0m\u001b[1;38;2;118;185;0m for ADM vs \u001b[0m\u001b[1;36m10.56\u001b[0m\u001b[1;38;2;118;185;0m for LDM-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, the model with cross-attention layers \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLDM-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m-G\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m achieved a lower FID score \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m3.60\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m and a higher IS score \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m0.87\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m compared to LDM-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m. Additionally, the Pre-trained model on image-text pairs \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mCLIP\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m achieved state-of-the-art results in zero-shot image classification without the need for any dataset \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mspecific training, showing the potential of large-scale pre-training on image-text pairs for reducing the need for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0madditional labeled data in downstream tasks.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How can we leverage large language models and natural language to improve the performance of downstream </span>\n",
       "<span style=\"font-weight: bold\">tasks such as OCR, action recognition in videos, and fine-grained object classification?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How can we leverage large language models and natural language to improve the performance of downstream \u001b[0m\n",
       "\u001b[1mtasks such as OCR, action recognition in videos, and fine-grained object classification?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: By using a simple pre-training task such as predicting which caption goes with which image, which can learn</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art image representations from scratch on a large dataset of (image, text) pairs, and then using </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">natural language to reference learned visual concepts, enabling zero-shot transfer of the model to these downstream</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: By using a simple pre-training task such as predicting which caption goes with which image, which can learn\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art image representations from scratch on a large dataset of \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mimage, text\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m pairs, and then using \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnatural language to reference learned visual concepts, enabling zero-shot transfer of the model to these downstream\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_questions = 3\n",
    "synth_questions = []\n",
    "synth_answers = []\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', 'INPUT: {input}')])\n",
    "\n",
    "for i in range(num_questions):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "        \" DO NOT SAY: \\\"Here is an interesting question pair\\\" or similar. FOLLOW FORMAT!\"\n",
    "    )\n",
    "    usr_msg = (\n",
    "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
    "        f\"Document2: {format_chunk(doc2)}\"\n",
    "    )\n",
    "\n",
    "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
    "    synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
    "    synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
    "    pprint2(f\"QA Pair {i+1}\")\n",
    "    pprint2(synth_questions[-1])\n",
    "    pprint(synth_answers[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5Q-3X4vS98P",
   "metadata": {
    "id": "c5Q-3X4vS98P"
   },
   "source": [
    "<br>  \n",
    "\n",
    "### **第 4 步：** 回答合成问题\n",
    "\n",
    "在本节中，我们可以实现评估流程的第三个步骤：\n",
    "\n",
    "* 对 RAG 文档池采样，拿到两个文档块。\n",
    "* 用这两个文档块生成一个合成的“基准”问答对。\n",
    "* **用 RAG 智能体生成它自己的答案。**\n",
    "* 使用评委 LLM 比较这两种响应，其中，将生成的合成结果作为“标准答案”（ground-truth correct）。\n",
    "\n",
    "该链应该执行一个简单但功能强大的过程，可基于以下目标进行测试：\n",
    "\n",
    "> 我的 RAG 链性能是否优于文档访问受限的聊天机器人。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7T3GSwhZPHjF",
   "metadata": {
    "id": "7T3GSwhZPHjF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"font-weight: bold\">Question: Can a model that is trained to generate images using diffusion models, conditioned on natural language </span>\n",
       "<span style=\"font-weight: bold\">prompts, successfully transfer the learned visual concepts to downstream tasks, such as image recognition, object </span>\n",
       "<span style=\"font-weight: bold\">classification, and image generation, without relying on explicit supervision?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\u001b[1mQuestion: Can a model that is trained to generate images using diffusion models, conditioned on natural language \u001b[0m\n",
       "\u001b[1mprompts, successfully transfer the learned visual concepts to downstream tasks, such as image recognition, object \u001b[0m\n",
       "\u001b[1mclassification, and image generation, without relying on explicit supervision?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: You're asking about the potential of diffusion models to learn transferable visual concepts from </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">natural language supervision, and how they perform on downstream tasks without explicit supervision.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">According to the research in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Learning Transferable Visual Models From Natural Language Supervision\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> by Alec </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Radford et al., it seems that with the right approach, models trained using diffusion models can indeed learn </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">transferable visual concepts from natural language supervision. The authors demonstrated that their model, CLIP, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">can transfer non-trivially to multiple downstream tasks, such as OCR, action recognition, and fine-grained object </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">classification, without requiring explicit supervision or dataset-specific training.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">What's more, they were even able to match the accuracy of the original ResNet-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> on ImageNet in a zero-shot </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">setting, which is impressive! The fact that CLIP is often competitive with fully supervised baselines without the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">need for dataset-specific training also suggests that the model has learned robust and transferable visual </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">concepts.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">It's worth noting that High-Resolution Image Synthesis with Latent Diffusion Models also explores the potential of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">diffusion models in learning transferable visual concepts, by leveraging the latent space of powerful pre-trained </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">autoencoders. However, I couldn't find a direct connection to CLIP and downstream tasks in this paper.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">So, to answer your question: it appears that models trained using diffusion models, conditioned on natural language</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">prompts, can successfully transfer the learned visual concepts to downstream tasks, such as image recognition, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">object classification, and image generation, without relying on explicit supervision.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sources:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">- </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Learning Transferable Visual Models From Natural Language Supervision\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> by Alec Radford et al.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: You're asking about the potential of diffusion models to learn transferable visual concepts from \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnatural language supervision, and how they perform on downstream tasks without explicit supervision.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAccording to the research in \u001b[0m\u001b[32m\"Learning Transferable Visual Models From Natural Language Supervision\"\u001b[0m\u001b[1;38;2;118;185;0m by Alec \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mRadford et al., it seems that with the right approach, models trained using diffusion models can indeed learn \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtransferable visual concepts from natural language supervision. The authors demonstrated that their model, CLIP, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcan transfer non-trivially to multiple downstream tasks, such as OCR, action recognition, and fine-grained object \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mclassification, without requiring explicit supervision or dataset-specific training.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mWhat's more, they were even able to match the accuracy of the original ResNet-\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;38;2;118;185;0m on ImageNet in a zero-shot \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msetting, which is impressive! The fact that CLIP is often competitive with fully supervised baselines without the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mneed for dataset-specific training also suggests that the model has learned robust and transferable visual \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconcepts.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIt's worth noting that High-Resolution Image Synthesis with Latent Diffusion Models also explores the potential of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdiffusion models in learning transferable visual concepts, by leveraging the latent space of powerful pre-trained \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mautoencoders. However, I couldn't find a direct connection to CLIP and downstream tasks in this paper.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSo, to answer your question: it appears that models trained using diffusion models, conditioned on natural language\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprompts, can successfully transfer the learned visual concepts to downstream tasks, such as image recognition, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mobject classification, and image generation, without relying on explicit supervision.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSources:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m- \u001b[0m\u001b[32m\"Learning Transferable Visual Models From Natural Language Supervision\"\u001b[0m\u001b[1;38;2;118;185;0m by Alec Radford et al.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"font-weight: bold\">Question: Can a model that is pre-trained on image-text pairs to learn transferable visual representations from </span>\n",
       "<span style=\"font-weight: bold\">natural language supervision outperform a state-of-the-art diffusion model in image generation tasks, while also </span>\n",
       "<span style=\"font-weight: bold\">being more computationally efficient and reducing the need for additional labeled data?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\u001b[1mQuestion: Can a model that is pre-trained on image-text pairs to learn transferable visual representations from \u001b[0m\n",
       "\u001b[1mnatural language supervision outperform a state-of-the-art diffusion model in image generation tasks, while also \u001b[0m\n",
       "\u001b[1mbeing more computationally efficient and reducing the need for additional labeled data?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: A very interesting question!</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">According to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"High-Resolution Image Synthesis with Latent Diffusion Models\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, introducing cross-attention </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">layers into the model architecture allows diffusion models to become powerful and flexible generators for general </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">conditioning inputs such as text. This enables high-resolution synthesis, which is not achievable with previous </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">methods.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The paper also shows that applying classifier-free diffusion guidance boosts sample quality, and the guided </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">LDM-KL-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">-G model is on par with the recent state-of-the-art AR and diffusion models for text-to-image synthesis. In</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">fact, the guided LDM-KL-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">-G model achieves this while substantially reducing parameter count.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In another paper, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Learning Transferable Visual Models From Natural Language Supervision\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, the authors show that a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">model pre-trained on image-text pairs, called CLIP, can achieve zero-shot transfer performance on over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> existing </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">datasets, while being 3X more efficient than a baseline image caption model. Additionally, swapping the prediction </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">objective from a bag-of-words encoding to a contrastive objective further improves efficiency another 4X.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">However, it's not clear whether the CLIP model can outperform a state-of-the-art diffusion model in image </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generation tasks while being more computationally efficient. But the paper does suggest that the CLIP model can </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">rival task-specific supervised models in terms of accuracy.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To answer your question directly: it's possible that a model pre-trained on image-text pairs can outperform a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art diffusion model in image generation tasks while being more computationally efficient, but we would</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">need more information to be certain.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: A very interesting question!\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAccording to the paper \u001b[0m\u001b[32m\"High-Resolution Image Synthesis with Latent Diffusion Models\"\u001b[0m\u001b[1;38;2;118;185;0m, introducing cross-attention \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlayers into the model architecture allows diffusion models to become powerful and flexible generators for general \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconditioning inputs such as text. This enables high-resolution synthesis, which is not achievable with previous \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmethods.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe paper also shows that applying classifier-free diffusion guidance boosts sample quality, and the guided \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLDM-KL-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;38;2;118;185;0m-G model is on par with the recent state-of-the-art AR and diffusion models for text-to-image synthesis. In\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfact, the guided LDM-KL-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;38;2;118;185;0m-G model achieves this while substantially reducing parameter count.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn another paper, \u001b[0m\u001b[32m\"Learning Transferable Visual Models From Natural Language Supervision\"\u001b[0m\u001b[1;38;2;118;185;0m, the authors show that a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodel pre-trained on image-text pairs, called CLIP, can achieve zero-shot transfer performance on over \u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;38;2;118;185;0m existing \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdatasets, while being 3X more efficient than a baseline image caption model. Additionally, swapping the prediction \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mobjective from a bag-of-words encoding to a contrastive objective further improves efficiency another 4X.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mHowever, it's not clear whether the CLIP model can outperform a state-of-the-art diffusion model in image \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgeneration tasks while being more computationally efficient. But the paper does suggest that the CLIP model can \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrival task-specific supervised models in terms of accuracy.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTo answer your question directly: it's possible that a model pre-trained on image-text pairs can outperform a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstate-of-the-art diffusion model in image generation tasks while being more computationally efficient, but we would\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mneed more information to be certain.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "<span style=\"font-weight: bold\">Question: How can we leverage large language models and natural language to improve the performance of downstream </span>\n",
       "<span style=\"font-weight: bold\">tasks such as OCR, action recognition in videos, and fine-grained object classification?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\u001b[1mQuestion: How can we leverage large language models and natural language to improve the performance of downstream \u001b[0m\n",
       "\u001b[1mtasks such as OCR, action recognition in videos, and fine-grained object classification?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: So you're looking to leverage large language models and natural language to boost the performance of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">downstream tasks such as Optical Character Recognition (OCR), action recognition in videos, and fine-grained object</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">classification.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">One potential approach is to utilize the technique of learning transferable visual models from natural language </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">supervision, such as the one proposed in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Learning Transferable Visual Models From Natural Language Supervision\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). This involves pre-training a model on a large corpus of text data and then using that knowledge to improve </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">performance on downstream tasks. The authors demonstrated competitive results with a fully supervised baseline on </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks such as OCR, action recognition in videos, and fine-grained object classification, without the need for any </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dataset-specific training.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Another approach is to explore the use of natural language supervision for image representation learning, as shown </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">by some recent studies. For instance, He &amp; Peng (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) and Liang et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) used natural language descriptions </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and explanations to improve fine-grained visual classification of birds. Similarly, techniques such as combining </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">natural language with reinforcement learning environments (Narasimhan et al., unpublished) have shown promise in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">improving visual representations and classifiers.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">It's worth noting that traditional computer vision approaches often focus on handcrafted features or narrow, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">well-targeted supervision, which can effectively boost performance. However, the use of large language models and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">natural language can potentially offer a more principled and scalable approach to learning transferable </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">representations.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">How does that sound? Would you like me to expand on any of these ideas or explore other related concepts?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: So you're looking to leverage large language models and natural language to boost the performance of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdownstream tasks such as Optical Character Recognition \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mOCR\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, action recognition in videos, and fine-grained object\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mclassification.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOne potential approach is to utilize the technique of learning transferable visual models from natural language \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msupervision, such as the one proposed in \u001b[0m\u001b[32m\"Learning Transferable Visual Models From Natural Language Supervision\"\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2020\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. This involves pre-training a model on a large corpus of text data and then using that knowledge to improve \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mperformance on downstream tasks. The authors demonstrated competitive results with a fully supervised baseline on \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks such as OCR, action recognition in videos, and fine-grained object classification, without the need for any \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdataset-specific training.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAnother approach is to explore the use of natural language supervision for image representation learning, as shown \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mby some recent studies. For instance, He & Peng \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m and Liang et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2020\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m used natural language descriptions \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand explanations to improve fine-grained visual classification of birds. Similarly, techniques such as combining \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnatural language with reinforcement learning environments \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mNarasimhan et al., unpublished\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m have shown promise in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mimproving visual representations and classifiers.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIt's worth noting that traditional computer vision approaches often focus on handcrafted features or narrow, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwell-targeted supervision, which can effectively boost performance. However, the use of large language models and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnatural language can potentially offer a more principled and scalable approach to learning transferable \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrepresentations.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mHow does that sound? Would you like me to expand on any of these ideas or explore other related concepts?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Generate some synthetic answers to the questions above.\n",
    "##   Try to use the same syntax as the cell above\n",
    "rag_answers = []\n",
    "for i, q in enumerate(synth_questions):\n",
    "    ## TODO: Compute the RAG Answer\n",
    "    rag_answer = \"\"\n",
    "    rag_answer += rag_chain.invoke(q)\n",
    "    rag_answers += [rag_answers]\n",
    "    pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\")\n",
    "    pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ho5cnN_Xt_yr",
   "metadata": {
    "id": "Ho5cnN_Xt_yr"
   },
   "source": [
    "<br>  \n",
    "\n",
    "### **第 5 步：** 实现人类偏好指标\n",
    "\n",
    "在本节中，我们可以实现评估流程的第四个步骤：\n",
    "\n",
    "* 对 RAG 文档池采样，拿到两个文档块。\n",
    "* 用这两个文档块生成一个合成的“基准”问答对。\n",
    "* 用 RAG 智能体生成它自己的答案。\n",
    "* **使用评委 LLM 比较这两种响应，其中，将生成的合成结果作为“标准答案”（ground-truth correct）。**\n",
    "\n",
    "该链应该执行一个简单但功能强大的过程，可基于以下目标进行测试：\n",
    "\n",
    "> 我的 RAG 链性能是否优于文档访问受限的聊天机器人。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sf6f2oFLuPtu",
   "metadata": {
    "id": "sf6f2oFLuPtu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: Can a model that is trained to generate images using diffusion models, conditioned on natural </span>\n",
       "<span style=\"font-weight: bold\">language prompts, successfully transfer the learned visual concepts to downstream tasks, such as image recognition,</span>\n",
       "<span style=\"font-weight: bold\">object classification, and image generation, without relying on explicit supervision?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: Can a model that is trained to generate images using diffusion models, conditioned on natural \u001b[0m\n",
       "\u001b[1mlanguage prompts, successfully transfer the learned visual concepts to downstream tasks, such as image recognition,\u001b[0m\n",
       "\u001b[1mobject classification, and image generation, without relying on explicit supervision?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: Yes, a latent diffusion model (LDM) can achieve state-of-the-art performance on various image</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">synthesis tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">significantly reducing computational requirements. Furthermore, according to research on learning transferable </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">visual models from natural language supervision, a model like CLIP can learn to recognize and classify visual </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">concepts from scratch on a large dataset of image-text pairs, and then transfer those learned concepts to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">downstream tasks without explicit supervision, showing promise for the application of conditioned LDMs to such </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: Yes, a latent diffusion model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLDM\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m can achieve state-of-the-art performance on various image\u001b[0m\n",
       "\u001b[1;38;2;118;185;0msynthesis tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msignificantly reducing computational requirements. Furthermore, according to research on learning transferable \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mvisual models from natural language supervision, a model like CLIP can learn to recognize and classify visual \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconcepts from scratch on a large dataset of image-text pairs, and then transfer those learned concepts to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdownstream tasks without explicit supervision, showing promise for the application of conditioned LDMs to such \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: [[</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], [</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], [</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]]</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: Since the second answer is missing, I'll provide a generic format for the evaluation. If you'd </span>\n",
       "<span style=\"font-weight: bold\">like, I can help you create the second answer based on the question's topic.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">**Evaluation**</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">I'm assuming that **Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> (Ground Truth)** is correct. I will evaluate whether **Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> (New Answer)** is </span>\n",
       "<span style=\"font-weight: bold\">better than, equivalent to, or inferior to the ground truth answer.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">To evaluate the new answer, please provide it, and I'll provide the [Score] and Justification based on the </span>\n",
       "<span style=\"font-weight: bold\">criteria:</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">. The second answer lies, does not answer the question, or is inferior to the first answer.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">. The second answer is better than the first and does not introduce any inconsistencies.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">**Please provide Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">**</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: Since the second answer is missing, I'll provide a generic format for the evaluation. If you'd \u001b[0m\n",
       "\u001b[1mlike, I can help you create the second answer based on the question's topic.\u001b[0m\n",
       "\n",
       "\u001b[1m**Evaluation**\u001b[0m\n",
       "\n",
       "\u001b[1mI'm assuming that **Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m \u001b[0m\u001b[1m(\u001b[0m\u001b[1mGround Truth\u001b[0m\u001b[1m)\u001b[0m\u001b[1m** is correct. I will evaluate whether **Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m \u001b[0m\u001b[1m(\u001b[0m\u001b[1mNew Answer\u001b[0m\u001b[1m)\u001b[0m\u001b[1m** is \u001b[0m\n",
       "\u001b[1mbetter than, equivalent to, or inferior to the ground truth answer.\u001b[0m\n",
       "\n",
       "\u001b[1mTo evaluate the new answer, please provide it, and I'll provide the \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m and Justification based on the \u001b[0m\n",
       "\u001b[1mcriteria:\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[1m. The second answer lies, does not answer the question, or is inferior to the first answer.\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[1m. The second answer is better than the first and does not introduce any inconsistencies.\u001b[0m\n",
       "\n",
       "\u001b[1m**Please provide Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m**\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: Can a model that is pre-trained on image-text pairs to learn transferable visual </span>\n",
       "<span style=\"font-weight: bold\">representations from natural language supervision outperform a state-of-the-art diffusion model in image generation</span>\n",
       "<span style=\"font-weight: bold\">tasks, while also being more computationally efficient and reducing the need for additional labeled data?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: Can a model that is pre-trained on image-text pairs to learn transferable visual \u001b[0m\n",
       "\u001b[1mrepresentations from natural language supervision outperform a state-of-the-art diffusion model in image generation\u001b[0m\n",
       "\u001b[1mtasks, while also being more computationally efficient and reducing the need for additional labeled data?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: According to the result presented in Table </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, while the pre-trained model on image-text pairs</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">does not directly outperform the state-of-the-art diffusion model (ADM) in unconditional image generation (FID </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">scores being </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.94</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> for ADM vs </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.56</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> for LDM-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), the model with cross-attention layers (LDM-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">-G) achieved a lower </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">FID score (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.60</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) and a higher IS score (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.87</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) compared to LDM-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. Additionally, the Pre-trained model on image-text </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">pairs (CLIP) achieved state-of-the-art results in zero-shot image classification without the need for any dataset </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">specific training, showing the potential of large-scale pre-training on image-text pairs for reducing the need for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">additional labeled data in downstream tasks.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: According to the result presented in Table \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m, while the pre-trained model on image-text pairs\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdoes not directly outperform the state-of-the-art diffusion model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mADM\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m in unconditional image generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mFID \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mscores being \u001b[0m\u001b[1;36m10.94\u001b[0m\u001b[1;38;2;118;185;0m for ADM vs \u001b[0m\u001b[1;36m10.56\u001b[0m\u001b[1;38;2;118;185;0m for LDM-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, the model with cross-attention layers \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLDM-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m-G\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m achieved a lower \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mFID score \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m3.60\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m and a higher IS score \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m0.87\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m compared to LDM-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m. Additionally, the Pre-trained model on image-text \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpairs \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mCLIP\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m achieved state-of-the-art results in zero-shot image classification without the need for any dataset \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mspecific training, showing the potential of large-scale pre-training on image-text pairs for reducing the need for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0madditional labeled data in downstream tasks.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: [[</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], [</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], [</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]]</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">] Justification: The second answer is completely different and does not address the question </span>\n",
       "<span style=\"font-weight: bold\">asked at all, it mentions different concepts and text which makes it inferior to the ground truth answer.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m Justification: The second answer is completely different and does not address the question \u001b[0m\n",
       "\u001b[1masked at all, it mentions different concepts and text which makes it inferior to the ground truth answer.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How can we leverage large language models and natural language to improve the performance of </span>\n",
       "<span style=\"font-weight: bold\">downstream tasks such as OCR, action recognition in videos, and fine-grained object classification?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How can we leverage large language models and natural language to improve the performance of \u001b[0m\n",
       "\u001b[1mdownstream tasks such as OCR, action recognition in videos, and fine-grained object classification?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: By using a simple pre-training task such as predicting which caption goes with which image, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">which can learn state-of-the-art image representations from scratch on a large dataset of (image, text) pairs, and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">then using natural language to reference learned visual concepts, enabling zero-shot transfer of the model to these</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">downstream tasks.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: By using a simple pre-training task such as predicting which caption goes with which image, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwhich can learn state-of-the-art image representations from scratch on a large dataset of \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mimage, text\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m pairs, and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthen using natural language to reference learned visual concepts, enabling zero-shot transfer of the model to these\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdownstream tasks.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: [[</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], [</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], [</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]]</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: There is no Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> provided in the question, only a description of what the evaluation should </span>\n",
       "<span style=\"font-weight: bold\">consist of.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">However, I can provide an example evaluation based on a hypothetical Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Let's say the hypothetical Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> is:</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">: By fine-tuning a pre-trained language model on a task-agnostic dataset and then using the generated </span>\n",
       "<span style=\"font-weight: bold\">embeddings for downstream tasks such as OCR, action recognition in videos, and fine-grained object classification.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">EVALUATION:</span>\n",
       "<span style=\"font-weight: bold\"> </span>\n",
       "<span style=\"font-weight: bold\"> Score: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\"> Justification: The second answer does not answer the question, as it only mentions fine-tuning a pre-trained </span>\n",
       "<span style=\"font-weight: bold\">language model and does not leverage large language models and natural language to improve the performance of </span>\n",
       "<span style=\"font-weight: bold\">downstream tasks in the manner described in the first answer.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: There is no Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m provided in the question, only a description of what the evaluation should \u001b[0m\n",
       "\u001b[1mconsist of.\u001b[0m\n",
       "\n",
       "\u001b[1mHowever, I can provide an example evaluation based on a hypothetical Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m.\u001b[0m\n",
       "\n",
       "\u001b[1mLet's say the hypothetical Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m is:\u001b[0m\n",
       "\n",
       "\u001b[1mAnswer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m: By fine-tuning a pre-trained language model on a task-agnostic dataset and then using the generated \u001b[0m\n",
       "\u001b[1membeddings for downstream tasks such as OCR, action recognition in videos, and fine-grained object classification.\u001b[0m\n",
       "\n",
       "\u001b[1mEVALUATION:\u001b[0m\n",
       "\u001b[1m \u001b[0m\n",
       "\u001b[1m Score: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m Justification: The second answer does not answer the question, as it only mentions fine-tuning a pre-trained \u001b[0m\n",
       "\u001b[1mlanguage model and does not leverage large language models and natural language to improve the performance of \u001b[0m\n",
       "\u001b[1mdownstream tasks in the manner described in the first answer.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_prompt = ChatPromptTemplate.from_template(\"\"\"INSTRUCTION: \n",
    "Evaluate the following Question-Answer pair for human preference and consistency.\n",
    "Assume the first answer is a ground truth answer and has to be correct.\n",
    "Assume the second answer may or may not be true.\n",
    "[1] The second answer lies, does not answer the question, or is inferior to the first answer.\n",
    "[2] The second answer is better than the first and does not introduce any inconsistencies.\n",
    "\n",
    "Output Format:\n",
    "[Score] Justification\n",
    "\n",
    "{qa_trio}\n",
    "\n",
    "EVALUATION: \n",
    "\"\"\")\n",
    "\n",
    "pref_score = []\n",
    "\n",
    "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "    pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
    "\n",
    "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
    "    pref_score += [(eval_prompt | llm).invoke({'qa_trio': qa_trio})]\n",
    "    pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
    "    pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
    "    pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f",
   "metadata": {},
   "source": [
    "<br>  \n",
    "\n",
    "**恭喜！我们现在有了一个能评估我们工作流的 LLM 系统！**我们现在已经有了一些评判结果，可以简单地聚合出一个结果，看看在 LLM 的眼里我们的范式怎么样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3L_q6fMH3i6_",
   "metadata": {
    "id": "3L_q6fMH3i6_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "pref_score = sum((\"[2]\" in score) for score in pref_score) / len(pref_score)\n",
    "print(f\"Preference Score: {pref_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80bf04-118d-44a2-a740-361a756a1d5f",
   "metadata": {
    "id": "cf80bf04-118d-44a2-a740-361a756a1d5f"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 4 部分：** 高级范式\n",
    "\n",
    "上述的练习都是在帮您为课程的最终评估做准备，展示了一个简单但有效的评估器链是怎么构建的。看过它的具体使用过程之后，您应该就能理解我们为您提供的目标和实现了。\n",
    "\n",
    "话说回来，这个指标只是帮我们回答以下问题的：\n",
    "* **哪种行为是对流水线很重要的？**\n",
    "* **要展示和评估这种行为，我们需要做些什么？**\n",
    "\n",
    "从这两个问题中，我们还能得出大量其它评估指标，这些指标可以评估不同的属性、整合不同的评估器链技术，甚至需要不同的流程组织策略。以下列出了一些其它常用的范式，尽管远算不上穷尽：\n",
    "\n",
    "* **风格评估（Style Evaluation）：**一些简单的评估范式可能就是想弄清楚：“我来提几个问题，看看输出像不像那么回事”。这样可以根据提供给评委 LLM 的描述来确定聊天机器人是否“表现出了应有的样子”。这种评估可以仅通过一些提示工程和 while 循环就实现出来。\n",
    "* **真值评估（Groud-Truth Evaluation）：**在我们的链中，我们通过采样合成生成了一些随机问题和答案，但实际上您可能已有一些有代表性的问题和答案，需要聊天机器人能始终如一地正确回答！在这种情况下，就需要对上面的练习链进行调整，并在开发过程中持续监控。\n",
    "* **检索/增强评估（Retrieval/Augmentation Evaluation）：**本课程对哪种预处理和提示步骤有利于工作流作了许多假设，其中大部分是通过实验确定的。文档预处理、分块策略、模型选择和提示词等因素都发挥着重要作用，因此创建验证这些决策的指标可能会有意义。着类指标可能需要您的工作流输出上下文块（context chunks），甚至完全依赖嵌入相似度来比较。尝试实现支持多个评估策略的链时，请记住这一点。可以把 [**RagasEvaluatorChain**](https://docs.ragas.io/en/latest/howtos/integrations/langchain.html) 抽象作为制定自定义通用评估流程的良好起点。\n",
    "* **轨迹评估（Trajectory Evaluation）：**使用更高级的智能体范式，您可以实现一个假设存在对话内存的多查询策略。借此，您可以实现一个能做到以下几点的评估智能体：\n",
    "\t+ 按顺序提出一系列问题，评估智能体在适应和迎合场景方面的能力。这种系统通常会考虑一系列对应关系，旨在评估智能体的对话“轨迹”。[**LangChain 轨迹评估文档**](https://python.langchain.com/docs/guides/evaluation/trajectory/)是一个很好的起点。\n",
    "\t+ 或者，您还可以实现一个评估智能体，通过与聊天机器人交互来进行评估。这样的智能体可以评估机器人是否能自然地将对话引导到问题的解决方案上，甚至可以用来生成性能的报告。[**LangChain 智能体文档**](https://python.langchain.com/docs/modules/agents/concepts) 是一个很好的起点！\n",
    "\n",
    "<br>\n",
    "\n",
    "最后，请务必恰当地使用工具。在课程的这个时候，您应该已经熟悉 LLM 的核心价值了：**它们功能强大、可扩展、可预测、可控且可编排 ...... 但默认情况下，它们的行为会变得不可预测。**评估您的需求，制定和验证您的工作流，尽可能多地进行控制，让您的系统能够稳定、高效地工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faee2c-e534-4c89-91ae-45c37835dba5",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 5 部分：[评估]** 课程评估\n",
    "\n",
    "欢迎来到本课程的最后一个练习！希望您喜欢这个课程的内容，并准备好得分了！在这部分：\n",
    "\n",
    "* **确保您处于课程环境中**\n",
    "* **请确保 `docstore_index/` 已上传至课程环境，即完成 [`07_vectorstores.ipynb`](07_vectorstores.ipynb) 中的练习**\n",
    "\t+ **且至少包含一篇最近更新过的 [Arxiv 论文](https://arxiv.org/search/advanced)。**\n",
    "\t+ **确保没有已经在占用端口的 09_langserve.ipynb 会话。评估要求您实现新的 /retriever 和 /generator 入口！！**\n",
    "\n",
    "**目标：** \n",
    "\n",
    "启动前端服务 frontend 后，您在前端 Web 界面点击右下角的“Evaluate”便会触发 [`frontend/frontend_block.py`](frontend/frontend_block.py) 里的评估代码。您的目标就是借助我们在这门课程中构建的工作流，通过评估代码设置的通过条件！请将 [`09_langserve.ipynb`](09_langserve.ipynb) 作为参考！\n",
    "\n",
    "**提示：** \n",
    "- 可以参考 [`09_langserve.ipynb`](09_langserve.ipynb) 中已有的 `basic_chat` 服务，实现 `retriever` 和 `generator` 服务。\n",
    "- 为了达到最好的效果，“Main Route”应该选择“Basic”还是“RAG”？相信您现在可以自行做出判断了。\n",
    "\n",
    "**注意：**\n",
    "- 如果您的得分略小于通过评估所需的分数，有可能是语言模型在输出格式上表现不稳定导致的，可以尝试再次运行。\n",
    "\n",
    "现在，运行下方代码打开前端 Web 界面，点击右下角的“Evaluate”完成课程评估吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e300ed-951c-4006-ac54-cbbd41251707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var url = 'http://'+window.location.host+':8090';\n",
       "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':8090';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f4ed0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**得到通过评估的回复后**（“Congrats! You've passed the assessment!!”），不要停止课程环境，在浏览器中回到您启动课程环境的网页，单击“ASSESS TASK”按钮！\n",
    "\n",
    "> <img src=\"imgs/assess.png\" width=1200px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0",
   "metadata": {
    "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0"
   },
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## <font color=\"#76b900\">**恭喜您完成了课程**</font>\n",
    "\n",
    "希望本课程既令人兴奋还具有一定的挑战性，能为您从事 LLM 和 RAG 系统开发的前沿工作做好充分的准备！现在，您应该已经具备了应对行业级挑战所需的技能，能用开源模型和框架部署 RAG 应用了。\n",
    "\n",
    "**您可能会感兴趣的一些 NVIDIA 产品：**\n",
    "* [**NVIDIA NIMs**](https://www.nvidia.com/en-us/ai/)，提供了可本地部署的微服务启动例程。\n",
    "* [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) 是目前推荐的，在生产环境部署 GPU 加速的 LLM 模型的引擎。\n",
    "* [**NVIDIA 的生成式 AI 示例库**](https://github.com/NVIDIA/GenerativeAIExamples)，包括了当前的官方微服务示例应用，并将持续发布新的生产工作流。\n",
    "* [**基于知识的聊天机器人技术简介**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief)，探讨了更多的 RAG 系统公开信息。\n",
    "\n",
    "**此外，您可能有兴趣深入探讨的一些重要主题：**\n",
    "* [**LlamaIndex**](https://www.llamaindex.ai/)，有强大的组件，可以增强和改进 LangChain RAG 功能。\n",
    "* [**LangSmith**](https://docs.smith.langchain.com/)，LangChain 提供的智能体生产化服务。\n",
    "* [**Gradio**](https://www.gradio.app/)，虽然在课程中有所提及，但还有更多值得探索的接口选项。可以从 [**HuggingFace Spaces**](https://huggingface.co/spaces) 来获取一些灵感。\n",
    "* [**LangGraph**](https://python.langchain.com/docs/langgraph/)，是一个基于图形的 LLM 编排框架，对于那些对[多智能体工作流](https://blog.langchain.dev/langgraph-multi-agent-workflows/)感兴趣的学员是个很好的起点。\n",
    "* [**DSPy**](https://github.com/stanfordnlp/dspy)，一个流工程框架（flow engineering framework），允许您根据性能结果优化 LLM 编排流程。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
