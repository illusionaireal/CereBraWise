{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qk4Uw_iSr3Mc",
   "metadata": {
    "id": "Qk4Uw_iSr3Mc"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 7:** 使用向量存储实现检索增强生成</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "我们在前面的 notebook 中了解并尝试了嵌入模型。讨论了它在长文档比较中的应用，并以它为主干实现了基于语义的比较。本 notebook 将把这个思路用到检索模型上，探索如何靠*向量存储*来构建自动保存和检索信息的聊天机器人系统。\n",
    "\n",
    "<br>\n",
    "\n",
    "### **学习目标：**\n",
    "\n",
    "* 理解语义相似度系统是怎么方便地实现检索的。\n",
    "* 学会将检索模块整合到聊天模型系统中，以创建检索增强生成（RAG）工作流，用于完成文档检索或对话内存缓冲等任务。\n",
    "\n",
    "<br>  \n",
    "\n",
    "### **思考问题：**\n",
    "\n",
    "* 本 notebook 不会尝试加入层次化推理（hierachical reasoning）或非朴素（non-naive）的 RAG，如规划智能体（palnning agents）。想想需要如何调整才能让这些组件在 LCEL 链中运行。\n",
    "* 思考将向量存储方案用在规模化部署的最好时机是什么，以及什么时候需要用 GPU 进行优化。\n",
    "\n",
    "<br>  \n",
    "\n",
    "### **Notebook 版权声明：**\n",
    "\n",
    "* 本 notebook 是 [**NVIDIA 深度学习培训中心**](https://www.nvidia.cn/training/)的课程[**《构建大语言模型 RAG 智能体》**](https://www.nvidia.cn/training/instructor-led-workshops/building-rag-agents-with-llms/)中的一部分，未经 NVIDIA 授权不得分发。\n",
    "\n",
    "<br> \n",
    "\n",
    "### **环境设置：**"
   ]
  },
  {
   "cell_type": "code",
   "id": "5XmeiiOWtuxC",
   "metadata": {
    "id": "5XmeiiOWtuxC",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# %%capture\n",
    "## ^^ Comment out if you want to see the pip install process\n",
    "\n",
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37fe234-2bdb-4107-8483-efda9aa5e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf",
   "metadata": {
    "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## 第 1 部分：RAG 工作流概述\n",
    "\n",
    "此 notebook 将探索多个范式并给出参考代码，以帮助您开始使用最常见一些的检索增强工作流。具体来说将涵盖以下部分（每个部分各有侧重）：\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***适用于交互式对话的向量存储工作流：***\n",
    "* 为新对话生成语义嵌入。\n",
    "* 将消息正文添加到向量存储以供检索。\n",
    "* 在向量存储中查询相关消息填充到 LLM 上下文中。\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***处理任意文档的工作流：***\n",
    "* **将文档分快并处理成有用信息。**\n",
    "* 为每个**新文档块**生成语义嵌入。\n",
    "* 将**块正文（chunk bodies）**存到向量存储中以供检索。\n",
    "* 在向量存储中查询相关的**块**，用来填充 LLM 上下文。\n",
    "\t+ ***可选：*修改/合成结果以获得更好的 LLM 结果。**\n",
    "\n",
    "<br>\n",
    "\n",
    "> **适用于任意文档目录的扩展工作流：**\n",
    "* 将**每个文档**分为多个块并处理成有用的信息。\n",
    "* 为每个新文档块生成语义嵌入。\n",
    "* 将块正文存到**可扩展的向量数据库中以实现快速检索**。\n",
    "\t+ ***可选：*利用更大系统的层次化结构或元数据结构。**\n",
    "* 在**向量数据库**中查询相关的块来填充 LLM 上下文。\n",
    "\t+ *可选：*修改/合成结果以获得更好的 LLM 结果。\n",
    "\n",
    "<br>  \n",
    "\n",
    "与 RAG 相关的一些重要术语都可以在 [**LlamaIndex Concepts 页面**](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html) 查到，这是学习 LlamaIndex 加载和检索策略的很好的资源。我们强烈建议您在学习此 notebook 的过程中参考它，并鼓励您在课后试试 LlamaIndex 亲手体会它的优缺点！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437",
   "metadata": {
    "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437"
   },
   "source": [
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/data_connection_langchain.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Retrieval | LangChain**🦜️🔗](https://python.langchain.com/docs/modules/data_connection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XaZ20XoeSTD-",
   "metadata": {
    "id": "XaZ20XoeSTD-"
   },
   "source": [
    "----\n",
    "\n",
    "<br>  \n",
    "\n",
    "## **第 2 部分：** 用于对话历史的 RAG\n",
    "\n",
    "在之前的探索中，我们深入研究了文档嵌入模型的功能，并用它来嵌入、存储和比较文本的语义向量表示。尽管我们可以动手将其扩展到向量存储领域，但如果用标准 API 配合框架的话，就能发现它已经替我们完成了很多繁重的工作！\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LRx0XUf_Sdxw",
   "metadata": {
    "id": "LRx0XUf_Sdxw"
   },
   "source": [
    "### **第 1 步：** 创建一段对话\n",
    "\n",
    "想象一段 Llama-13B 聊天智能体和一只名为 Beras 的熊之间的对话。这段对话包含了大量细节和潜在的分支，为我们的研究提供了丰富的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "IUfCuMkoShWI",
   "metadata": {
    "id": "IUfCuMkoShWI"
   },
   "outputs": [],
   "source": [
    "conversation = [  ## This conversation was generated partially by an AI system, and modified to exhibit desirable properties\n",
    "    \"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\",\n",
    "    \"[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America\",\n",
    "    \"[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\",\n",
    "    \"[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for you!\"\n",
    "    \"[Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.\",\n",
    "    \"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching documentaries about them.\"\n",
    "    \"[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just curious, ya know!\",\n",
    "    \"[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains and their significance!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tDL2tAo2Skh2",
   "metadata": {
    "id": "tDL2tAo2Skh2"
   },
   "source": [
    "仍然可以用上一个 notebook 的手动嵌入策略，但我们完全可以让向量数据库替我们做！\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5hIp943mSqGZ",
   "metadata": {
    "id": "5hIp943mSqGZ"
   },
   "source": [
    "### **第 2 步：** 构建向量存储检索器\n",
    "\n",
    "为了流程化对话中的相似性查询，我们可以使用向量存储来帮助我们追踪文本！**向量存储**（Vector Stores）或者叫向量存储系统，对嵌入/比较策略的大部分底层细节做了抽象，为加载和比较向量提供了一个简洁的接口。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pnaOBgexS-kp",
   "metadata": {
    "id": "pnaOBgexS-kp"
   },
   "source": [
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/vector_stores.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Vector Stores | LangChain**🦜️🔗](https://python.langchain.com/docs/modules/data_connection/vectorstores/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DwZUh6kgS5Ki",
   "metadata": {
    "id": "DwZUh6kgS5Ki"
   },
   "source": [
    "<br>\n",
    "\n",
    "除了借助 API 简化流程外，向量存储还在背后实现了连接器（connector）、集成（integration）和优化。我们将从 [**FAISS 向量存储**](https://python.langchain.com/docs/integrations/vectorstores/faiss)开始，它集成了兼容 LangChain 的嵌入模型 [**FAISS (Facebook AI Similarity Search)**](https://github.com/facebookresearch/faiss)，从而允许在本地实现快速可扩展的流程！\n",
    "\n",
    "\n",
    "**具体来说：**\n",
    "\n",
    "1. 我们可以通过 `from_texts` 构造器将对话输入到 [**FAISS 向量存储**](https://python.langchain.com/docs/integrations/vectorstores/faiss)。这样我们的对话数据和嵌入模型就会用来创建索引。\n",
    "2. 然后，这个向量存储就可以作为检索器，支持用 LangChain 运行时 API 来检索文档。\n",
    "\n",
    "以下内容展示了如何构建 FAISS 向量存储并使用 LangChain `vectorstore` API 将其作为检索器使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1kE2-ejoTKKU",
   "metadata": {
    "id": "1kE2-ejoTKKU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.5 ms, sys: 11.8 ms, total: 71.3 ms\n",
      "Wall time: 810 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ^^ This cell will be timed to see how long the conversation embedding takes\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## Streamlined from_texts FAISS vectorstore construction from text list\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "retriever = convstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muN66v5PW5dW",
   "metadata": {
    "id": "muN66v5PW5dW"
   },
   "source": [
    "现在，检索器可以像任何其他可运行的 LangChain 一样用于查询向量存储中的某些相关文档：\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "kNZJTnlEWVYh",
   "metadata": {
    "id": "kNZJTnlEWVYh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you![Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m[\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m\"\u001B[0m\u001B[32m[\u001B[0m\u001B[32mUser\u001B[0m\u001B[32m]\u001B[0m\u001B[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001B[0m\n",
       "\u001B[32mrocky mountains?\"\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m'\u001B[0m\u001B[32m[\u001B[0m\u001B[32mAgent\u001B[0m\u001B[32m]\u001B[0m\u001B[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001B[0m\n",
       "\u001B[32mand their significance!'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m'\u001B[0m\u001B[32m[\u001B[0m\u001B[32mAgent\u001B[0m\u001B[32m]\u001B[0m\u001B[32m I hope you get to visit them someday, Beras! It would be a great adventure for \u001B[0m\n",
       "\u001B[32myou!\u001B[0m\u001B[32m[\u001B[0m\u001B[32mBeras\u001B[0m\u001B[32m]\u001B[0m\u001B[32m Thank you for the suggestion! Ill definitely keep it in mind for the future.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m\"\u001B[0m\u001B[32m[\u001B[0m\u001B[32mAgent\u001B[0m\u001B[32m]\u001B[0m\u001B[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001B[0m\n",
       "\u001B[32monline or watching documentaries about them.\u001B[0m\u001B[32m[\u001B[0m\u001B[32mBeras\u001B[0m\u001B[32m]\u001B[0m\u001B[32m I live in the arctic, so I'm not used to the warm climate \u001B[0m\n",
       "\u001B[32mthere. I was just curious, ya know!\"\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m]\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "SE1eDZTEWScC",
   "metadata": {
    "id": "SE1eDZTEWScC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across North America'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m[\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m'\u001B[0m\u001B[32m[\u001B[0m\u001B[32mAgent\u001B[0m\u001B[32m]\u001B[0m\u001B[32m The Rocky Mountains are a beautiful and majestic range of mountains that stretch \u001B[0m\n",
       "\u001B[32macross North America'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m\"\u001B[0m\u001B[32m[\u001B[0m\u001B[32mAgent\u001B[0m\u001B[32m]\u001B[0m\u001B[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001B[0m\n",
       "\u001B[32monline or watching documentaries about them.\u001B[0m\u001B[32m[\u001B[0m\u001B[32mBeras\u001B[0m\u001B[32m]\u001B[0m\u001B[32m I live in the arctic, so I'm not used to the warm climate \u001B[0m\n",
       "\u001B[32mthere. I was just curious, ya know!\"\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m\"\u001B[0m\u001B[32m[\u001B[0m\u001B[32mUser\u001B[0m\u001B[32m]\u001B[0m\u001B[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001B[0m\n",
       "\u001B[32mrocky mountains?\"\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m'\u001B[0m\u001B[32m[\u001B[0m\u001B[32mAgent\u001B[0m\u001B[32m]\u001B[0m\u001B[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001B[0m\n",
       "\u001B[32mand their significance!'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m]\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtNCEXLYTVf4",
   "metadata": {
    "id": "mtNCEXLYTVf4"
   },
   "source": [
    "如我们所见，检索工具从我们的查询中找到了一些语义相关的文档。您可能会注意到，不是所有文档都有用或清晰。比如，如果不是出于上下文，检索询问*“您的姓名”*时把*“Beras”*检索出来可能不是个好事。提前考虑到潜在的问题并让 LLM 组件相互协同更有可能让 RAG 达到好的效果。\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZEDEzpqmTYMv",
   "metadata": {
    "id": "ZEDEzpqmTYMv"
   },
   "source": [
    "### **第 3 步：** 将对话检索功能整合到我们的链中\n",
    "\n",
    "现在，我们已把检索器组件作为一个链了，可以像以前一样将其整合到现有的聊天系统中。具体来说，我们现在可以构建一个***保持在线（always-on）的 RAG*** 了，其中：\n",
    "* **默认情况下，检索器始终在检索上下文。**\n",
    "* **生成器根据检索到的上下文执行操作。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64abe478-9bcb-4802-a26e-dc5a1756e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Utility Runnables/Methods\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        if preface: print(preface, end=\"\")\n",
    "        pprint(x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## Optional; Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "uue5UY3_TcvF",
   "metadata": {
    "id": "uue5UY3_TcvF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">It seems that Beras lives in the Arctic. The cooler climate there is quite different from the warm climate found in</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the Rocky Mountains, which Beras was interested in learning more about.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mIt seems that Beras lives in the Arctic. The cooler climate there is quite different from the warm climate found in\u001B[0m\n",
       "\u001B[1;38;2;118;185;0mthe Rocky Mountains, which Beras was interested in learning more about.\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {question}\"\n",
    "    \"\\nAnswer the user conversationally. User is not aware of context.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'question': (lambda x:x)\n",
    "    }\n",
    "    | context_prompt\n",
    "    # | RPrint()\n",
    "    | instruct_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(chain.invoke(\"Where does Beras live?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FSIqTMuuTjIh",
   "metadata": {
    "id": "FSIqTMuuTjIh"
   },
   "source": [
    "多试几个调用，看看新配置的效果。无论您选择的是哪个模型，都可以先从下面的几个问题开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4jDJwrYpTmpd",
   "metadata": {
    "id": "4jDJwrYpTmpd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! The Rocky Mountains are a stunning range of mountains that span across North America. Unfortunately, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">without more context, I can't give you a precise location beyond that. However, if you're interested, you can </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">always look up more information online or watch documentaries about the Rocky Mountains to learn more!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mHello there! The Rocky Mountains are a stunning range of mountains that span across North America. Unfortunately, \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mwithout more context, I can't give you a precise location beyond that. However, if you're interested, you can \u001B[0m\n",
       "\u001B[1;38;2;118;185;0malways look up more information online or watch documentaries about the Rocky Mountains to learn more!\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "-artagLfTpBy",
   "metadata": {
    "id": "-artagLfTpBy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! The Rocky Mountains are a stunning range of mountains that stretch across North America. To answer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">your question about their location, they do not directly border California. In fact, they run from Canada all the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">way to New Mexico, passing through the states of Montana, Idaho, Wyoming, Colorado, and a small part of northern </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">New Mexico. I hope that helps clarify their location for you!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mHello there! The Rocky Mountains are a stunning range of mountains that stretch across North America. To answer \u001B[0m\n",
       "\u001B[1;38;2;118;185;0myour question about their location, they do not directly border California. In fact, they run from Canada all the \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mway to New Mexico, passing through the states of Montana, Idaho, Wyoming, Colorado, and a small part of northern \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mNew Mexico. I hope that helps clarify their location for you!\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains? Are they close to California?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "GDgjdfdpTrV5",
   "metadata": {
    "id": "GDgjdfdpTrV5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hi there! Beras is a big blue bear and based on the context, we know that they live in the arctic. However, the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">context doesn't provide details about the exact location or the distance between the Rocky Mountains and the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">arctic. The Rocky Mountains are a prominent mountain range in North America, and the arctic region is also quite </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">large. Therefore, it's not possible to provide an accurate distance without more specific information. I'm here to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">share that the Rocky Mountains are beautiful and majestic and stretch across North America, and I encourage Beras </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to learn more about them by doing some research online or watching documentaries.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mHi there! Beras is a big blue bear and based on the context, we know that they live in the arctic. However, the \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mcontext doesn't provide details about the exact location or the distance between the Rocky Mountains and the \u001B[0m\n",
       "\u001B[1;38;2;118;185;0marctic. The Rocky Mountains are a prominent mountain range in North America, and the arctic region is also quite \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mlarge. Therefore, it's not possible to provide an accurate distance without more specific information. I'm here to \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mshare that the Rocky Mountains are beautiful and majestic and stretch across North America, and I encourage Beras \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mto learn more about them by doing some research online or watching documentaries.\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"How far away is Beras from the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wp9-8CbT0L9",
   "metadata": {
    "id": "8wp9-8CbT0L9"
   },
   "source": [
    "<br>  \n",
    "\n",
    "您可能会注意到把这个保持在线（always-on）的检索节点放到循环里效果很不错，因为目前输入 LLM 的上下文仍然相对较小。有必要反复尝试嵌入大小、上下文限制等配置，来更好地预测模型表现，并衡量为提高性能值得做出何种努力。\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OnpOybOhUCTf",
   "metadata": {
    "id": "OnpOybOhUCTf"
   },
   "source": [
    "### **第 4 步：** 自动对话存储\n",
    "\n",
    "现在向量存储已经可以工作了，我们最后再做一个集成：加一个调用 `add_texts` 更新存储状态的运行时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "FsK6-AtRVdcZ",
   "metadata": {
    "id": "FsK6-AtRVdcZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While I'm sure the Rocky Mountains would provide a magnificent backdrop for enjoying ice cream, it's worth noting </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that the Rocky Mountains are primarily known for their natural beauty and outdoor activities. Hiking, mountain </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">biking, and wildlife viewing are popular activities. The Rocky Mountains are also home to a number of national </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">parks, such as Banff National Park in Canada and Rocky Mountain National Park in the United States. These parks </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">offer a range of activities for visitors of all ages and abilities. However, I'm always here to help you discover </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">more about the world, and if you have any other questions about the Rocky Mountains or any other topic, please </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">don't hesitate to ask!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mWhile I'm sure the Rocky Mountains would provide a magnificent backdrop for enjoying ice cream, it's worth noting \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mthat the Rocky Mountains are primarily known for their natural beauty and outdoor activities. Hiking, mountain \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mbiking, and wildlife viewing are popular activities. The Rocky Mountains are also home to a number of national \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mparks, such as Banff National Park in Canada and Rocky Mountain National Park in the United States. These parks \u001B[0m\n",
       "\u001B[1;38;2;118;185;0moffer a range of activities for visitors of all ages and abilities. However, I'm always here to help you discover \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mmore about the world, and if you have any other questions about the Rocky Mountains or any other topic, please \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mdon't hesitate to ask!\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While I can't truly guess your favorite food, Beras, from our conversation, it does seem that you are quite fond of</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ice cream! It's always fun to enjoy a cool treat in a beautiful setting, like the Rocky Mountains.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mWhile I can't truly guess your favorite food, Beras, from our conversation, it does seem that you are quite fond of\u001B[0m\n",
       "\u001B[1;38;2;118;185;0mice cream! It's always fun to enjoy a cool treat in a beautiful setting, like the Rocky Mountains.\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Well Beras, I must admit, I made an assumption there! You're absolutely right that your love for ice cream was just</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">an observation and not a fact. Your fondness for honey is indeed delightful, just as the Rocky Mountains are! It's </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">fascinating how diverse tastes can be, isn't it?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mWell Beras, I must admit, I made an assumption there! You're absolutely right that your love for ice cream was just\u001B[0m\n",
       "\u001B[1;38;2;118;185;0man observation and not a fact. Your fondness for honey is indeed delightful, just as the Rocky Mountains are! It's \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mfascinating how diverse tastes can be, isn't it?\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Well, based on our conversation, I can see that you indeed love ice cream! However, if I were to try and guess your</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">favorite food again, I would have to say honey, as you mentioned it being your actual favorite. But I'm an </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">assistant, so I don't have the ability to know for certain unless you tell me. :)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mWell, based on our conversation, I can see that you indeed love ice cream! However, if I were to try and guess your\u001B[0m\n",
       "\u001B[1;38;2;118;185;0mfavorite food again, I would have to say honey, as you mentioned it being your actual favorite. But I'm an \u001B[0m\n",
       "\u001B[1;38;2;118;185;0massistant, so I don't have the ability to know for certain unless you tell me. :\u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Reset knowledge base and define what it means to add more messages.\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([f\"User said {d.get('input')}\", f\"Agent said {d.get('output')}\"])\n",
    "    return d.get('output')\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    "    \"\\nAnswer the user conversationally. Make sure the conversation flows naturally.\\n\"\n",
    "    \"[Agent]\"\n",
    ")\n",
    "\n",
    "\n",
    "conv_chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'input': (lambda x:x)\n",
    "    }\n",
    "    | RunnableAssign({'output' : chat_prompt | instruct_llm | StrOutputParser()})\n",
    "    | partial(save_memory_and_get_output, vstore=convstore)\n",
    ")\n",
    "\n",
    "pprint(conv_chain.invoke(\"I'm glad you agree! I can't wait to get some ice cream there! It's such a good food!\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Can you guess what my favorite food is?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Actually, my favorite is honey! Not sure where you got that idea?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"I see! Fair enough! Do you know my favorite food now?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KRMW6G7NVSWF",
   "metadata": {
    "id": "KRMW6G7NVSWF"
   },
   "source": [
    "不同于将上下文注入 LLM 的更自动化的全文本（full-text）或基于规则的方法，这样可避免上下文长度失控。这种策略虽然称不上完全可靠，但对于非结构化的对话来说已经是一个巨大的改进了（甚至不需要借助一个强大的指令微调模型做槽位填充）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9TPkh3SaLbqh",
   "metadata": {
    "id": "9TPkh3SaLbqh"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 3 部分 [练习]：** 用 RAG 进行文档块检索\n",
    "\n",
    "鉴于我们之前对文档加载的探索，您应该已经熟悉对数据块嵌入和检索了。现在值得花点时间继续过一遍，因为把 RAG 用在文档上是一把双刃剑：它看起来似乎开箱即用，但想让它在实际应用中保持可靠的性能需要非常谨慎地优化。我们也借此机会回顾一下基本的 LCEL 技能！\n",
    "\n",
    "<br> \n",
    "\n",
    "### **练习：**\n",
    "\n",
    "您可能还记得之前我们用 [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv) 加载了一些比较短的文章：\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "docs = [\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL\n",
    "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct\n",
    "]\n",
    "```\n",
    "\n",
    "根据所学，选择几个论文，并开发一个能讨论这些论文的聊天机器人！\n",
    "\n",
    "<br>  \n",
    "\n",
    "虽然这是一项相当艰巨的任务，但下面将提供**大部分**实现过程。演示过后，许多必须的环节就已经实现好了，您真正的任务是将它们集成到最终的 `retrieval_chain`。您会在最后一个 notebook 把它们集成到链中来完成评估测试！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jSjfCtiQnj9e",
   "metadata": {
    "id": "jSjfCtiQnj9e"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **任务 1：** 载入并分块您的文档\n",
    "\n",
    "以下代码提供了一些可以载入到 RAG 链的默认论文。您可以根据需要选更多的论文，但要注意长文档的处理时间也更长。其中还有一些利于提高 RAG 性能的简化假设及处理步骤：\n",
    "\n",
    "* 文档仅截取“参考“”（References）部分之前的内容。防止系统考虑冗长和不重要的引用和附录。\n",
    "* 有一个能提供全局视角的列出所有可用文档的数据块。如果您的工作流并不是每次检索都提供元数据，那么这个数据块就会很有用，甚至可以在合适的时候作为更高优先级信息的一部分。\n",
    "* 此外，还会插入元数据条目以提供常规信息。理想情况下，会有一些融合进了元数据的跨文档数据块。\n",
    "\n",
    "**注意：** ***为执行评估，请至少放进一篇发表时间不超过一个月的论文！***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "S-3FBdT_lhVT",
   "metadata": {
    "id": "S-3FBdT_lhVT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Documents\n",
      "Chunking Documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sources and discrete reasoning</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Mistral 7B</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - ReAct: Synergizing Reasoning and Acting in Language Models</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - High-Resolution Image Synthesis with Latent Diffusion Models</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Learning Transferable Visual Models From Natural Language Supervision </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mAvailable Documents:\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - Attention Is All You Need\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \u001B[0m\n",
       "\u001B[1;38;2;118;185;0msources and discrete reasoning\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - Mistral 7B\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - ReAct: Synergizing Reasoning and Acting in Language Models\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - High-Resolution Image Synthesis with Latent Diffusion Models\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - Learning Transferable Visual Models From Natural Language Supervision \u001B[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0\n",
      " - # Chunks: 35\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-02'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Attention Is All You Need'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Kaiser, Illia Polosukhin'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">constituency parsing both with large and limited training data.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2023-08-02'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Attention Is All You Need'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz \u001B[0m\n",
       "\u001B[32mKaiser, Illia Polosukhin'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural \u001B[0m\n",
       "\u001B[32mnetworks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder \u001B[0m\n",
       "\u001B[32mthrough an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on \u001B[0m\n",
       "\u001B[32mattention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation\u001B[0m\n",
       "\u001B[32mtasks show these models to be\\nsuperior in quality while being more parallelizable and requiring \u001B[0m\n",
       "\u001B[32msignificantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation \u001B[0m\n",
       "\u001B[32mtask, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 \u001B[0m\n",
       "\u001B[32mEnglish-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 \u001B[0m\n",
       "\u001B[32mafter training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the \u001B[0m\n",
       "\u001B[32mliterature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish \u001B[0m\n",
       "\u001B[32mconstituency parsing both with large and limited training data.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1\n",
      " - # Chunks: 45\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2019-05-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Encoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">layer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">empirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">including\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">absolute improvement), SQuAD v1.1 question answering\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2019-05-24'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional \u001B[0m\n",
       "\u001B[32mEncoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to \u001B[0m\n",
       "\u001B[32mpre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right \u001B[0m\n",
       "\u001B[32mcontext in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output \u001B[0m\n",
       "\u001B[32mlayer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language \u001B[0m\n",
       "\u001B[32minference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and \u001B[0m\n",
       "\u001B[32mempirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, \u001B[0m\n",
       "\u001B[32mincluding\\npushing the GLUE score to 80.5% \u001B[0m\u001B[32m(\u001B[0m\u001B[32m7.7% point absolute improvement\u001B[0m\u001B[32m)\u001B[0m\u001B[32m, MultiNLI\\naccuracy to 86.7% \u001B[0m\u001B[32m(\u001B[0m\u001B[32m4.6% \u001B[0m\n",
       "\u001B[32mabsolute improvement\u001B[0m\u001B[32m)\u001B[0m\u001B[32m, SQuAD v1.1 question answering\\nTest F1 to 93.2 \u001B[0m\u001B[32m(\u001B[0m\u001B[32m1.5 point absolute improvement\u001B[0m\u001B[32m)\u001B[0m\u001B[32m and SQuAD \u001B[0m\n",
       "\u001B[32mv2.0 Test F1 to 83.1\\n\u001B[0m\u001B[32m(\u001B[0m\u001B[32m5.1 point absolute improvement\u001B[0m\u001B[32m)\u001B[0m\u001B[32m.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 2\n",
      " - # Chunks: 46\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-04-12'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">behind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">world knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">downstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">index\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">passages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2021-04-12'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, \u001B[0m\n",
       "\u001B[32mHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, \u001B[0m\n",
       "\u001B[32mand achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and\u001B[0m\n",
       "\u001B[32mprecisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags \u001B[0m\n",
       "\u001B[32mbehind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their \u001B[0m\n",
       "\u001B[32mworld knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism \u001B[0m\n",
       "\u001B[32mto\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive \u001B[0m\n",
       "\u001B[32mdownstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation \u001B[0m\u001B[32m(\u001B[0m\u001B[32mRAG\u001B[0m\u001B[32m)\u001B[0m\u001B[32m -- \u001B[0m\n",
       "\u001B[32mmodels which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG \u001B[0m\n",
       "\u001B[32mmodels where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector \u001B[0m\n",
       "\u001B[32mindex\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which \u001B[0m\n",
       "\u001B[32mconditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different \u001B[0m\n",
       "\u001B[32mpassages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set\u001B[0m\n",
       "\u001B[32mthe state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific \u001B[0m\n",
       "\u001B[32mretrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more \u001B[0m\n",
       "\u001B[32mspecific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 3\n",
      " - # Chunks: 40\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-05-01'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge sources and discrete reasoning'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Amnon Shashua, Moshe Tenenholtz'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Huge language models (LMs) have ushered in a new era for AI, serving as a\\ngateway to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">natural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">systems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge and Language (MRKL, pronounced \"miracle\") system,\\nsome of the technical challenges in implementing it, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2022-05-01'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external \u001B[0m\n",
       "\u001B[32mknowledge sources and discrete reasoning'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit\u001B[0m\n",
       "\u001B[32mBata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, \u001B[0m\n",
       "\u001B[32mAmnon Shashua, Moshe Tenenholtz'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Huge language models \u001B[0m\u001B[32m(\u001B[0m\u001B[32mLMs\u001B[0m\u001B[32m)\u001B[0m\u001B[32m have ushered in a new era for AI, serving as a\\ngateway to \u001B[0m\n",
       "\u001B[32mnatural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently \u001B[0m\n",
       "\u001B[32mlimited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a \u001B[0m\n",
       "\u001B[32msystems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to \u001B[0m\n",
       "\u001B[32mlinguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete \u001B[0m\n",
       "\u001B[32mknowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, \u001B[0m\n",
       "\u001B[32mKnowledge and Language \u001B[0m\u001B[32m(\u001B[0m\u001B[32mMRKL, pronounced \"miracle\"\u001B[0m\u001B[32m)\u001B[0m\u001B[32m system,\\nsome of the technical challenges in implementing it, \u001B[0m\n",
       "\u001B[32mand Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 4\n",
      " - # Chunks: 21\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-10-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Mistral 7B'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">under the Apache 2.0 license.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2023-10-10'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Mistral 7B'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, \u001B[0m\n",
       "\u001B[32mDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, \u001B[0m\n",
       "\u001B[32mMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior \u001B[0m\n",
       "\u001B[32mperformance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in\u001B[0m\n",
       "\u001B[32mreasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention \u001B[0m\u001B[32m(\u001B[0m\u001B[32mGQA\u001B[0m\u001B[32m)\u001B[0m\u001B[32m for \u001B[0m\n",
       "\u001B[32mfaster\\ninference, coupled with sliding window attention \u001B[0m\u001B[32m(\u001B[0m\u001B[32mSWA\u001B[0m\u001B[32m)\u001B[0m\u001B[32m to effectively handle\\nsequences of arbitrary length\u001B[0m\n",
       "\u001B[32mwith a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, \u001B[0m\n",
       "\u001B[32mthat surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released \u001B[0m\n",
       "\u001B[32munder the Apache 2.0 license.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 5\n",
      " - # Chunks: 44\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conversations with\\nhuman preferences are publicly available </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2023-12-24'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, \u001B[0m\n",
       "\u001B[32mZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Evaluating large language model \u001B[0m\u001B[32m(\u001B[0m\u001B[32mLLM\u001B[0m\u001B[32m)\u001B[0m\u001B[32m based chat assistants is challenging\\ndue to their broad \u001B[0m\n",
       "\u001B[32mcapabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore\u001B[0m\n",
       "\u001B[32musing strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and \u001B[0m\n",
       "\u001B[32mlimitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited \u001B[0m\n",
       "\u001B[32mreasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between \u001B[0m\n",
       "\u001B[32mLLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot \u001B[0m\n",
       "\u001B[32mArena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both \u001B[0m\n",
       "\u001B[32mcontrolled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement \u001B[0m\n",
       "\u001B[32mbetween humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which \u001B[0m\n",
       "\u001B[32mare otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement\u001B[0m\n",
       "\u001B[32meach other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K \u001B[0m\n",
       "\u001B[32mconversations with\\nhuman preferences are publicly available \u001B[0m\n",
       "\u001B[32mat\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 6\n",
      " - # Chunks: 58\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2025-02-14'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Shu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, Yuchi Ma'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation (RAG) has proven effective in integrating\\nexternal knowledge into </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">large language models (LLMs) for question-answer (QA)\\ntasks. The state-of-the-art RAG approaches often use the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">graph data as the\\nexternal data since they capture the rich semantic information and link\\nrelationships between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entities. However, existing graph-based RAG approaches\\ncannot accurately identify the relevant information from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the graph and also\\nconsume large numbers of tokens in the online retrieval process. To address\\nthese issues, we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">introduce a novel graph-based RAG approach, called Attributed\\nCommunity-based Hierarchical RAG (ArchRAG), by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">augmenting the question using\\nattributed communities, and also introducing a novel LLM-based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hierarchical\\nclustering method. To retrieve the most relevant information from the graph for\\nthe question, we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">build a novel hierarchical index structure for the attributed\\ncommunities and develop an effective online </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval method. Experimental\\nresults demonstrate that ArchRAG outperforms existing methods in terms of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">both\\naccuracy and token cost.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2025-02-14'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Shu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, Yuchi Ma'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Retrieval-Augmented Generation \u001B[0m\u001B[32m(\u001B[0m\u001B[32mRAG\u001B[0m\u001B[32m)\u001B[0m\u001B[32m has proven effective in integrating\\nexternal knowledge into \u001B[0m\n",
       "\u001B[32mlarge language models \u001B[0m\u001B[32m(\u001B[0m\u001B[32mLLMs\u001B[0m\u001B[32m)\u001B[0m\u001B[32m for question-answer \u001B[0m\u001B[32m(\u001B[0m\u001B[32mQA\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\\ntasks. The state-of-the-art RAG approaches often use the \u001B[0m\n",
       "\u001B[32mgraph data as the\\nexternal data since they capture the rich semantic information and link\\nrelationships between \u001B[0m\n",
       "\u001B[32mentities. However, existing graph-based RAG approaches\\ncannot accurately identify the relevant information from \u001B[0m\n",
       "\u001B[32mthe graph and also\\nconsume large numbers of tokens in the online retrieval process. To address\\nthese issues, we \u001B[0m\n",
       "\u001B[32mintroduce a novel graph-based RAG approach, called Attributed\\nCommunity-based Hierarchical RAG \u001B[0m\u001B[32m(\u001B[0m\u001B[32mArchRAG\u001B[0m\u001B[32m)\u001B[0m\u001B[32m, by \u001B[0m\n",
       "\u001B[32maugmenting the question using\\nattributed communities, and also introducing a novel LLM-based \u001B[0m\n",
       "\u001B[32mhierarchical\\nclustering method. To retrieve the most relevant information from the graph for\\nthe question, we \u001B[0m\n",
       "\u001B[32mbuild a novel hierarchical index structure for the attributed\\ncommunities and develop an effective online \u001B[0m\n",
       "\u001B[32mretrieval method. Experimental\\nresults demonstrate that ArchRAG outperforms existing methods in terms of \u001B[0m\n",
       "\u001B[32mboth\\naccuracy and token cost.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 7\n",
      " - # Chunks: 64\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2025-02-13'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Yiqian Huang, Shiqi Zhang, Xiaokui Xiao'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Graph-RAG constructs a knowledge graph from text chunks to improve retrieval\\nin Large Language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Model (LLM)-based question answering. It is particularly\\nuseful in domains such as biomedicine, law, and political</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">science, where\\nretrieval often requires multi-hop reasoning over proprietary documents. Some\\nexisting Graph-RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">systems construct KNN graphs based on text chunk relevance,\\nbut this coarse-grained approach fails to capture </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entity relationships within\\ntexts, leading to sub-par retrieval and generation quality. To address this,\\nrecent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">solutions leverage LLMs to extract entities and relationships from text\\nchunks, constructing triplet-based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge graphs. However, this approach\\nincurs significant indexing costs, especially for large document </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">collections.\\n  To ensure a good result accuracy while reducing the indexing cost, we propose\\nKET-RAG, a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multi-granular indexing framework. KET-RAG first identifies a small\\nset of key text chunks and leverages an LLM to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construct a knowledge graph\\nskeleton. It then builds a text-keyword bipartite graph from all text chunks,\\nserving</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as a lightweight alternative to a full knowledge graph. During\\nretrieval, KET-RAG searches both structures: it </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">follows the local search\\nstrategy of existing Graph-RAG systems on the skeleton while mimicking this\\nsearch on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the bipartite graph to improve retrieval quality. We evaluate eight\\nsolutions on two real-world datasets, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">demonstrating that KET-RAG outperforms\\nall competitors in indexing cost, retrieval effectiveness, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generation\\nquality. Notably, it achieves comparable or superior retrieval quality to\\nMicrosoft's Graph-RAG while </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reducing indexing costs by over an order of\\nmagnitude. Additionally, it improves the generation quality by up to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">32.4%\\nwhile lowering indexing costs by around 20%.\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2025-02-13'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Yiqian Huang, Shiqi Zhang, Xiaokui Xiao'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m\"Graph-RAG constructs a knowledge graph from text chunks to improve retrieval\\nin Large Language \u001B[0m\n",
       "\u001B[32mModel \u001B[0m\u001B[32m(\u001B[0m\u001B[32mLLM\u001B[0m\u001B[32m)\u001B[0m\u001B[32m-based question answering. It is particularly\\nuseful in domains such as biomedicine, law, and political\u001B[0m\n",
       "\u001B[32mscience, where\\nretrieval often requires multi-hop reasoning over proprietary documents. Some\\nexisting Graph-RAG \u001B[0m\n",
       "\u001B[32msystems construct KNN graphs based on text chunk relevance,\\nbut this coarse-grained approach fails to capture \u001B[0m\n",
       "\u001B[32mentity relationships within\\ntexts, leading to sub-par retrieval and generation quality. To address this,\\nrecent \u001B[0m\n",
       "\u001B[32msolutions leverage LLMs to extract entities and relationships from text\\nchunks, constructing triplet-based \u001B[0m\n",
       "\u001B[32mknowledge graphs. However, this approach\\nincurs significant indexing costs, especially for large document \u001B[0m\n",
       "\u001B[32mcollections.\\n  To ensure a good result accuracy while reducing the indexing cost, we propose\\nKET-RAG, a \u001B[0m\n",
       "\u001B[32mmulti-granular indexing framework. KET-RAG first identifies a small\\nset of key text chunks and leverages an LLM to\u001B[0m\n",
       "\u001B[32mconstruct a knowledge graph\\nskeleton. It then builds a text-keyword bipartite graph from all text chunks,\\nserving\u001B[0m\n",
       "\u001B[32mas a lightweight alternative to a full knowledge graph. During\\nretrieval, KET-RAG searches both structures: it \u001B[0m\n",
       "\u001B[32mfollows the local search\\nstrategy of existing Graph-RAG systems on the skeleton while mimicking this\\nsearch on \u001B[0m\n",
       "\u001B[32mthe bipartite graph to improve retrieval quality. We evaluate eight\\nsolutions on two real-world datasets, \u001B[0m\n",
       "\u001B[32mdemonstrating that KET-RAG outperforms\\nall competitors in indexing cost, retrieval effectiveness, and \u001B[0m\n",
       "\u001B[32mgeneration\\nquality. Notably, it achieves comparable or superior retrieval quality to\\nMicrosoft's Graph-RAG while \u001B[0m\n",
       "\u001B[32mreducing indexing costs by over an order of\\nmagnitude. Additionally, it improves the generation quality by up to \u001B[0m\n",
       "\u001B[32m32.4%\\nwhile lowering indexing costs by around 20%.\"\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 8\n",
      " - # Chunks: 123\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-03-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'ReAct: Synergizing Reasoning and Acting in Language Models'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2023-03-10'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'ReAct: Synergizing Reasoning and Acting in Language Models'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'While large language models \u001B[0m\u001B[32m(\u001B[0m\u001B[32mLLMs\u001B[0m\u001B[32m)\u001B[0m\u001B[32m have demonstrated impressive capabilities\\nacross tasks in \u001B[0m\n",
       "\u001B[32mlanguage understanding and interactive decision making, their\\nabilities for reasoning \u001B[0m\u001B[32m(\u001B[0m\u001B[32me.g. chain-of-thought \u001B[0m\n",
       "\u001B[32mprompting\u001B[0m\u001B[32m)\u001B[0m\u001B[32m and acting \u001B[0m\u001B[32m(\u001B[0m\u001B[32me.g.\\naction plan generation\u001B[0m\u001B[32m)\u001B[0m\u001B[32m have primarily been studied as separate topics. In \u001B[0m\n",
       "\u001B[32mthis\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an \u001B[0m\n",
       "\u001B[32minterleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, \u001B[0m\n",
       "\u001B[32mand update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, \u001B[0m\n",
       "\u001B[32msuch as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to \u001B[0m\n",
       "\u001B[32ma diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art \u001B[0m\n",
       "\u001B[32mbaselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting\u001B[0m\n",
       "\u001B[32mcomponents.\\nConcretely, on question answering \u001B[0m\u001B[32m(\u001B[0m\u001B[32mHotpotQA\u001B[0m\u001B[32m)\u001B[0m\u001B[32m and fact verification \u001B[0m\u001B[32m(\u001B[0m\u001B[32mFever\u001B[0m\u001B[32m)\u001B[0m\u001B[32m,\\nReAct overcomes issues of\u001B[0m\n",
       "\u001B[32mhallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia\u001B[0m\n",
       "\u001B[32mAPI, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without \u001B[0m\n",
       "\u001B[32mreasoning traces. On two interactive decision making\\nbenchmarks \u001B[0m\u001B[32m(\u001B[0m\u001B[32mALFWorld and WebShop\u001B[0m\u001B[32m)\u001B[0m\u001B[32m, ReAct outperforms \u001B[0m\n",
       "\u001B[32mimitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being\u001B[0m\n",
       "\u001B[32mprompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 9\n",
      " - # Chunks: 52\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-04-13'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'High-Resolution Image Synthesis with Latent Diffusion Models'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'By decomposing the image formation process into a sequential application of\\ndenoising </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">autoencoders, diffusion models (DMs) achieve state-of-the-art\\nsynthesis results on image data and beyond. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Additionally, their formulation\\nallows for a guiding mechanism to control the image generation process </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">without\\nretraining. However, since these models typically operate directly in pixel\\nspace, optimization of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">powerful DMs often consumes hundreds of GPU days and\\ninference is expensive due to sequential evaluations. To </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enable DM training on\\nlimited computational resources while retaining their quality and flexibility,\\nwe apply </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">them in the latent space of powerful pretrained autoencoders. In\\ncontrast to previous work, training diffusion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models on such a representation\\nallows for the first time to reach a near-optimal point between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">complexity\\nreduction and detail preservation, greatly boosting visual fidelity. By\\nintroducing cross-attention </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">layers into the model architecture, we turn\\ndiffusion models into powerful and flexible generators for general </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conditioning\\ninputs such as text or bounding boxes and high-resolution synthesis becomes\\npossible in a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">convolutional manner. Our latent diffusion models (LDMs) achieve\\na new state of the art for image inpainting and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">highly competitive performance\\non various tasks, including unconditional image generation, semantic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scene\\nsynthesis, and super-resolution, while significantly reducing computational\\nrequirements compared to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pixel-based DMs. Code is available at\\nhttps://github.com/CompVis/latent-diffusion .'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2022-04-13'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'High-Resolution Image Synthesis with Latent Diffusion Models'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'By decomposing the image formation process into a sequential application of\\ndenoising \u001B[0m\n",
       "\u001B[32mautoencoders, diffusion models \u001B[0m\u001B[32m(\u001B[0m\u001B[32mDMs\u001B[0m\u001B[32m)\u001B[0m\u001B[32m achieve state-of-the-art\\nsynthesis results on image data and beyond. \u001B[0m\n",
       "\u001B[32mAdditionally, their formulation\\nallows for a guiding mechanism to control the image generation process \u001B[0m\n",
       "\u001B[32mwithout\\nretraining. However, since these models typically operate directly in pixel\\nspace, optimization of \u001B[0m\n",
       "\u001B[32mpowerful DMs often consumes hundreds of GPU days and\\ninference is expensive due to sequential evaluations. To \u001B[0m\n",
       "\u001B[32menable DM training on\\nlimited computational resources while retaining their quality and flexibility,\\nwe apply \u001B[0m\n",
       "\u001B[32mthem in the latent space of powerful pretrained autoencoders. In\\ncontrast to previous work, training diffusion \u001B[0m\n",
       "\u001B[32mmodels on such a representation\\nallows for the first time to reach a near-optimal point between \u001B[0m\n",
       "\u001B[32mcomplexity\\nreduction and detail preservation, greatly boosting visual fidelity. By\\nintroducing cross-attention \u001B[0m\n",
       "\u001B[32mlayers into the model architecture, we turn\\ndiffusion models into powerful and flexible generators for general \u001B[0m\n",
       "\u001B[32mconditioning\\ninputs such as text or bounding boxes and high-resolution synthesis becomes\\npossible in a \u001B[0m\n",
       "\u001B[32mconvolutional manner. Our latent diffusion models \u001B[0m\u001B[32m(\u001B[0m\u001B[32mLDMs\u001B[0m\u001B[32m)\u001B[0m\u001B[32m achieve\\na new state of the art for image inpainting and \u001B[0m\n",
       "\u001B[32mhighly competitive performance\\non various tasks, including unconditional image generation, semantic \u001B[0m\n",
       "\u001B[32mscene\\nsynthesis, and super-resolution, while significantly reducing computational\\nrequirements compared to \u001B[0m\n",
       "\u001B[32mpixel-based DMs. Code is available at\\nhttps://github.com/CompVis/latent-diffusion .'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 10\n",
      " - # Chunks: 155\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-02-26'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Learning Transferable Visual Models From Natural Language Supervision'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'State-of-the-art computer vision systems are trained to predict a fixed set\\nof predetermined </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">object categories. This restricted form of supervision limits\\ntheir generality and usability since additional </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">labeled data is needed to\\nspecify any other visual concept. Learning directly from raw text about images\\nis a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">promising alternative which leverages a much broader source of\\nsupervision. We demonstrate that the simple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-training task of predicting\\nwhich caption goes with which image is an efficient and scalable way to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">learn\\nSOTA image representations from scratch on a dataset of 400 million (image,\\ntext) pairs collected from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">internet. After pre-training, natural language\\nis used to reference learned visual concepts (or describe new ones)</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enabling\\nzero-shot transfer of the model to downstream tasks. We study the performance\\nof this approach by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">benchmarking on over 30 different existing computer vision\\ndatasets, spanning tasks such as OCR, action </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recognition in videos,\\ngeo-localization, and many types of fine-grained object classification. The\\nmodel </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transfers non-trivially to most tasks and is often competitive with a\\nfully supervised baseline without the need </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for any dataset specific training.\\nFor instance, we match the accuracy of the original ResNet-50 on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ImageNet\\nzero-shot without needing to use any of the 1.28 million training examples it\\nwas trained on. We release</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">our code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2021-02-26'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Learning Transferable Visual Models From Natural Language Supervision'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish \u001B[0m\n",
       "\u001B[32mSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'State-of-the-art computer vision systems are trained to predict a fixed set\\nof predetermined \u001B[0m\n",
       "\u001B[32mobject categories. This restricted form of supervision limits\\ntheir generality and usability since additional \u001B[0m\n",
       "\u001B[32mlabeled data is needed to\\nspecify any other visual concept. Learning directly from raw text about images\\nis a \u001B[0m\n",
       "\u001B[32mpromising alternative which leverages a much broader source of\\nsupervision. We demonstrate that the simple \u001B[0m\n",
       "\u001B[32mpre-training task of predicting\\nwhich caption goes with which image is an efficient and scalable way to \u001B[0m\n",
       "\u001B[32mlearn\\nSOTA image representations from scratch on a dataset of 400 million \u001B[0m\u001B[32m(\u001B[0m\u001B[32mimage,\\ntext\u001B[0m\u001B[32m)\u001B[0m\u001B[32m pairs collected from the \u001B[0m\n",
       "\u001B[32minternet. After pre-training, natural language\\nis used to reference learned visual concepts \u001B[0m\u001B[32m(\u001B[0m\u001B[32mor describe new ones\u001B[0m\u001B[32m)\u001B[0m\n",
       "\u001B[32menabling\\nzero-shot transfer of the model to downstream tasks. We study the performance\\nof this approach by \u001B[0m\n",
       "\u001B[32mbenchmarking on over 30 different existing computer vision\\ndatasets, spanning tasks such as OCR, action \u001B[0m\n",
       "\u001B[32mrecognition in videos,\\ngeo-localization, and many types of fine-grained object classification. The\\nmodel \u001B[0m\n",
       "\u001B[32mtransfers non-trivially to most tasks and is often competitive with a\\nfully supervised baseline without the need \u001B[0m\n",
       "\u001B[32mfor any dataset specific training.\\nFor instance, we match the accuracy of the original ResNet-50 on \u001B[0m\n",
       "\u001B[32mImageNet\\nzero-shot without needing to use any of the 1.28 million training examples it\\nwas trained on. We release\u001B[0m\n",
       "\u001B[32mour code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
    ")\n",
    "\n",
    "## TODO: Please pick some papers and add them to the list as you'd like\n",
    "## NOTE: To re-use for the final assessment, make sure at least one paper is < 1 month old\n",
    "print(\"Loading Documents\")\n",
    "docs = [\n",
    "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
    "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
    "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
    "    ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
    "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
    "    ArxivLoader(query=\"2502.09891\").load(),\n",
    "    ArxivLoader(query=\"2502.09304\").load(),\n",
    "    ## Some longer papers\n",
    "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct Paper\n",
    "    ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
    "    ArxivLoader(query=\"2103.00020\").load(),  ## CLIP Paper\n",
    "    ## TODO: Feel free to add more\n",
    "]\n",
    "\n",
    "## Cut the paper short if references is included.\n",
    "## This is a standard string in papers.\n",
    "for doc in docs:\n",
    "    content = json.dumps(doc[0].page_content)\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]\n",
    "\n",
    "## Split the documents and also filter out stubs (overly short chunks)\n",
    "print(\"Chunking Documents\")\n",
    "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
    "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
    "\n",
    "## Make some custom Chunks to give big-picture details\n",
    "doc_string = \"Available Documents:\"\n",
    "doc_metadata = []\n",
    "for chunks in docs_chunks:\n",
    "    metadata = getattr(chunks[0], 'metadata', {})\n",
    "    doc_string += \"\\n - \" + metadata.get('Title')\n",
    "    doc_metadata += [str(metadata)]\n",
    "\n",
    "extra_chunks = [doc_string] + doc_metadata\n",
    "\n",
    "## Printing out some summary information for reference\n",
    "pprint(doc_string, '\\n')\n",
    "for i, chunks in enumerate(docs_chunks):\n",
    "    print(f\"Document {i}\")\n",
    "    print(f\" - # Chunks: {len(chunks)}\")\n",
    "    print(f\" - Metadata: \")\n",
    "    pprint(chunks[0].metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4pWU_OOnnrsT",
   "metadata": {
    "id": "4pWU_OOnnrsT"
   },
   "source": [
    "### **任务 2：** 构建文档向量存储\n",
    "\n",
    "我们现在已经有了所有组件，可以继续围绕它们创建索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lwwmr3aptwCg",
   "metadata": {
    "id": "lwwmr3aptwCg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Vector Stores\n",
      "CPU times: user 1.85 s, sys: 115 ms, total: 1.97 s\n",
      "Wall time: 54.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Constructing Vector Stores\")\n",
    "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
    "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j39JwCKubto0",
   "metadata": {
    "id": "j39JwCKubto0"
   },
   "source": [
    "<br>\n",
    "\n",
    "接着像下面这样把索引合并为一个："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "Q7us66iPVc70",
   "metadata": {
    "id": "Q7us66iPVc70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 695 chunks\n"
     ]
    }
   ],
   "source": [
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VU_VEx2mqJUK",
   "metadata": {
    "id": "VU_VEx2mqJUK"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **任务 3：[练习]** 实现 RAG 链\n",
    "\n",
    "终于，一切准备就绪，来实现 RAG 工作流吧！回顾一下，我们现在有：\n",
    "\n",
    "* 一种用向量存储从零创建对话记忆的方法（用 `default_FAISS()` 初始化）\n",
    "* 通过 `ArxivLoader` 预加载了包括文档信息的向量存储（存在 `docstore` 里）。\n",
    "\n",
    "再借助几个工具，就能集成您的链了！我们还提供了几个额外的便捷工具（`doc2str` 及 `RPrint`），您可以酌情使用。此外，一些启动提示词和结构已经定义好了。\n",
    "\n",
    "> **基于上述这些：** 实现 `retrieval_chain` 吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "-RXSrb1GcNff",
   "metadata": {
    "id": "-RXSrb1GcNff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about RAG!'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'history'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'[Quote from ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">.\\\\nRAG method\\\\nRetrieval method\\\\nQuestion\\\\nAttributed community\\\\nHierarchical index\\\\nRetrieval </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">filtering\\\\nZero-shot / CoT [27]\\\\nNone\\\\nSpecific\\\\nVanilla RAG\\\\nVector search\\\\nSpecific\\\\nGraphRAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[12]\\\\nTraverse\\\\nSpecific&amp;abstract\\\\nLightRAG [17]\\\\nExtract keywords + Vector search\\\\nAbstract\\\\nHippoRAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[18]\\\\nPPR\\\\nSpecific\\\\nArchRAG (ours)\\\\nHierarchical search\\\\nSpecific&amp;abstract\\\\nSpecifically, Microsoft first </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">proposes a graph-based RAG system\\\\ncalled GraphRAG [12], which first builds a knowledge graph (KG)\\\\nby extracting</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entities and relationships from the external corpus,\\\\nthen employs the Leiden method [54] to detect communities </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\\\ngenerates a summary for each community using LLMs\\n[Quote from KET-RAG: A Cost-Efficient Multi-Granular </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Indexing Framework for Graph-RAG] .\\\\n2.3\\\\nMicrosoft\\\\u2019s Graph-RAG\\\\nMicrosoft proposed the Graph-RAG system </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[8], which constructs a\\\\nknowledge graph index with multi-level communities and employs\\\\ntailored strategies for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">both local and global search. In this section,\\\\nwe focus on its indexing and retrieval operations for local </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">search,\\\\nwhich are relevant to our work.\\\\nAlgorithm 1 outlines the pseudo-code for constructing the graph\\\\nindex</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">G = (V\\\\n\\\\ud835\\\\udc52\\\\u222aV\\\\n\\\\ud835\\\\udc61, E), where V\\\\n\\\\ud835\\\\udc52and V\\\\n\\\\ud835\\\\udc61represent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entities and\\\\ntext chunks, respectively. Given a text chunk set T, KG-Index first\\\\nKET-RAG: A Cost-Efficient </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Multi-Granular Indexing Framework for Graph-RAG\\\\nConference acronym \\\\u2019XX, June 03\\\\u201305, 2018, Woodstock, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">NY\\\\nAlgorithm 1: KG-Index (T)\\\\nInput: The text chunk set T.\\\\nOutput: A TAG index G\\n[Quote from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . We refer to this decoding procedure as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, requiring many forward </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">passes. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that p\\\\u03b8(y|x, zi) \\\\u22480 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">where y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run additional forward passes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">once the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as \\\\u201cFast </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Decoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">disjoint\\\\n100-word chunks, to make a total of 21M documents\\n[Quote from Retrieval-Augmented Generation for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge-Intensive NLP Tasks] . In one approach, RAG-Sequence, the model uses the same document\\\\nto predict each </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">target token. The second approach, RAG-Token, can predict each target token based\\\\non a different document. In the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following, we formally introduce both models and then describe the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the training and decoding procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieved document to generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">latent variable that\\\\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation\\n'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'input'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Tell me about RAG!'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'history'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m''\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'context'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation\u001B[0m\u001B[32m]\u001B[0m\u001B[32m \u001B[0m\n",
       "\u001B[32m.\\\\nRAG method\\\\nRetrieval method\\\\nQuestion\\\\nAttributed community\\\\nHierarchical index\\\\nRetrieval \u001B[0m\n",
       "\u001B[32mfiltering\\\\nZero-shot / CoT \u001B[0m\u001B[32m[\u001B[0m\u001B[32m27\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nNone\\\\nSpecific\\\\nVanilla RAG\\\\nVector search\\\\nSpecific\\\\nGraphRAG \u001B[0m\n",
       "\u001B[32m[\u001B[0m\u001B[32m12\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nTraverse\\\\nSpecific&abstract\\\\nLightRAG \u001B[0m\u001B[32m[\u001B[0m\u001B[32m17\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nExtract keywords + Vector search\\\\nAbstract\\\\nHippoRAG \u001B[0m\n",
       "\u001B[32m[\u001B[0m\u001B[32m18\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nPPR\\\\nSpecific\\\\nArchRAG \u001B[0m\u001B[32m(\u001B[0m\u001B[32mours\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\\\\nHierarchical search\\\\nSpecific&abstract\\\\nSpecifically, Microsoft first \u001B[0m\n",
       "\u001B[32mproposes a graph-based RAG system\\\\ncalled GraphRAG \u001B[0m\u001B[32m[\u001B[0m\u001B[32m12\u001B[0m\u001B[32m]\u001B[0m\u001B[32m, which first builds a knowledge graph \u001B[0m\u001B[32m(\u001B[0m\u001B[32mKG\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\\\\nby extracting\u001B[0m\n",
       "\u001B[32mentities and relationships from the external corpus,\\\\nthen employs the Leiden method \u001B[0m\u001B[32m[\u001B[0m\u001B[32m54\u001B[0m\u001B[32m]\u001B[0m\u001B[32m to detect communities \u001B[0m\n",
       "\u001B[32mand\\\\ngenerates a summary for each community using LLMs\\n\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from KET-RAG: A Cost-Efficient Multi-Granular \u001B[0m\n",
       "\u001B[32mIndexing Framework for Graph-RAG\u001B[0m\u001B[32m]\u001B[0m\u001B[32m .\\\\n2.3\\\\nMicrosoft\\\\u2019s Graph-RAG\\\\nMicrosoft proposed the Graph-RAG system \u001B[0m\n",
       "\u001B[32m[\u001B[0m\u001B[32m8\u001B[0m\u001B[32m]\u001B[0m\u001B[32m, which constructs a\\\\nknowledge graph index with multi-level communities and employs\\\\ntailored strategies for \u001B[0m\n",
       "\u001B[32mboth local and global search. In this section,\\\\nwe focus on its indexing and retrieval operations for local \u001B[0m\n",
       "\u001B[32msearch,\\\\nwhich are relevant to our work.\\\\nAlgorithm 1 outlines the pseudo-code for constructing the graph\\\\nindex\u001B[0m\n",
       "\u001B[32mG = \u001B[0m\u001B[32m(\u001B[0m\u001B[32mV\\\\n\\\\ud835\\\\udc52\\\\u222aV\\\\n\\\\ud835\\\\udc61, E\u001B[0m\u001B[32m)\u001B[0m\u001B[32m, where V\\\\n\\\\ud835\\\\udc52and V\\\\n\\\\ud835\\\\udc61represent \u001B[0m\n",
       "\u001B[32mentities and\\\\ntext chunks, respectively. Given a text chunk set T, KG-Index first\\\\nKET-RAG: A Cost-Efficient \u001B[0m\n",
       "\u001B[32mMulti-Granular Indexing Framework for Graph-RAG\\\\nConference acronym \\\\u2019XX, June 03\\\\u201305, 2018, Woodstock, \u001B[0m\n",
       "\u001B[32mNY\\\\nAlgorithm 1: KG-Index \u001B[0m\u001B[32m(\u001B[0m\u001B[32mT\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\\\\nInput: The text chunk set T.\\\\nOutput: A TAG index G\\n\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from \u001B[0m\n",
       "\u001B[32mRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001B[0m\u001B[32m]\u001B[0m\u001B[32m . We refer to this decoding procedure as \u001B[0m\n",
       "\u001B[32m\\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, requiring many forward \u001B[0m\n",
       "\u001B[32mpasses. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that p\\\\u03b8\u001B[0m\u001B[32m(\u001B[0m\u001B[32my|x, zi\u001B[0m\u001B[32m)\u001B[0m\u001B[32m \\\\u22480 \u001B[0m\n",
       "\u001B[32mwhere y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run additional forward passes \u001B[0m\n",
       "\u001B[32monce the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as \\\\u201cFast \u001B[0m\n",
       "\u001B[32mDecoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all \u001B[0m\n",
       "\u001B[32mexperiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. \u001B[0m\u001B[32m[\u001B[0m\u001B[32m31\u001B[0m\u001B[32m]\u001B[0m\u001B[32m \u001B[0m\n",
       "\u001B[32mand\\\\nKarpukhin et al. \u001B[0m\u001B[32m[\u001B[0m\u001B[32m26\u001B[0m\u001B[32m]\u001B[0m\u001B[32m, we use the December 2018 dump. Each Wikipedia article is split into \u001B[0m\n",
       "\u001B[32mdisjoint\\\\n100-word chunks, to make a total of 21M documents\\n\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from Retrieval-Augmented Generation for \u001B[0m\n",
       "\u001B[32mKnowledge-Intensive NLP Tasks\u001B[0m\u001B[32m]\u001B[0m\u001B[32m . In one approach, RAG-Sequence, the model uses the same document\\\\nto predict each \u001B[0m\n",
       "\u001B[32mtarget token. The second approach, RAG-Token, can predict each target token based\\\\non a different document. In the\u001B[0m\n",
       "\u001B[32mfollowing, we formally introduce both models and then describe the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as \u001B[0m\n",
       "\u001B[32mthe training and decoding procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same \u001B[0m\n",
       "\u001B[32mretrieved document to generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single \u001B[0m\n",
       "\u001B[32mlatent variable that\\\\nis marginalized to get the seq2seq probability p\u001B[0m\u001B[32m(\u001B[0m\u001B[32my|x\u001B[0m\u001B[32m)\u001B[0m\u001B[32m via a top-K approximation\\n'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Conversation History Retrieval:\\n\\n\\n Document Retrieval:\\n[Quote from ArchRAG: Attributed Community-based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Hierarchical Retrieval-Augmented Generation] .\\\\nRAG method\\\\nRetrieval method\\\\nQuestion\\\\nAttributed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">community\\\\nHierarchical index\\\\nRetrieval filtering\\\\nZero-shot / CoT [27]\\\\nNone\\\\nSpecific\\\\nVanilla </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG\\\\nVector search\\\\nSpecific\\\\nGraphRAG [12]\\\\nTraverse\\\\nSpecific&amp;abstract\\\\nLightRAG [17]\\\\nExtract keywords + </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Vector search\\\\nAbstract\\\\nHippoRAG [18]\\\\nPPR\\\\nSpecific\\\\nArchRAG (ours)\\\\nHierarchical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">search\\\\nSpecific&amp;abstract\\\\nSpecifically, Microsoft first proposes a graph-based RAG system\\\\ncalled GraphRAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[12], which first builds a knowledge graph (KG)\\\\nby extracting entities and relationships from the external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpus,\\\\nthen employs the Leiden method [54] to detect communities and\\\\ngenerates a summary for each community </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using LLMs\\n[Quote from KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">.\\\\n2.3\\\\nMicrosoft\\\\u2019s Graph-RAG\\\\nMicrosoft proposed the Graph-RAG system [8], which constructs a\\\\nknowledge</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">graph index with multi-level communities and employs\\\\ntailored strategies for both local and global search. In </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this section,\\\\nwe focus on its indexing and retrieval operations for local search,\\\\nwhich are relevant to our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">work.\\\\nAlgorithm 1 outlines the pseudo-code for constructing the graph\\\\nindex G = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(V\\\\n\\\\ud835\\\\udc52\\\\u222aV\\\\n\\\\ud835\\\\udc61, E), where V\\\\n\\\\ud835\\\\udc52and V\\\\n\\\\ud835\\\\udc61represent entities </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\\\ntext chunks, respectively. Given a text chunk set T, KG-Index first\\\\nKET-RAG: A Cost-Efficient </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Multi-Granular Indexing Framework for Graph-RAG\\\\nConference acronym \\\\u2019XX, June 03\\\\u201305, 2018, Woodstock, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">NY\\\\nAlgorithm 1: KG-Index (T)\\\\nInput: The text chunk set T.\\\\nOutput: A TAG index G\\n[Quote from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . We refer to this decoding procedure as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, requiring many forward </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">passes. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that p\\\\u03b8(y|x, zi) \\\\u22480 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">where y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run additional forward passes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">once the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as \\\\u201cFast </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Decoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">disjoint\\\\n100-word chunks, to make a total of 21M documents\\n[Quote from Retrieval-Augmented Generation for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge-Intensive NLP Tasks] . In one approach, RAG-Sequence, the model uses the same document\\\\nto predict each </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">target token. The second approach, RAG-Token, can predict each target token based\\\\non a different document. In the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following, we formally introduce both models and then describe the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the training and decoding procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieved document to generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">latent variable that\\\\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation\\n\\n\\n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(Answer only from retrieval. Only cite sources that are used. Make your response conversational.)'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about RAG!'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;35mChatPromptValue\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;33mmessages\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[1;38;2;118;185;0m[\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;35mSystemMessage\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m            \u001B[0m\u001B[1;33mcontent\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m'You are a document chatbot. Help the user as they ask questions about documents. User messaged\u001B[0m\n",
       "\u001B[32mjust asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  \u001B[0m\n",
       "\u001B[32mConversation History Retrieval:\\n\\n\\n Document Retrieval:\\n\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from ArchRAG: Attributed Community-based \u001B[0m\n",
       "\u001B[32mHierarchical Retrieval-Augmented Generation\u001B[0m\u001B[32m]\u001B[0m\u001B[32m .\\\\nRAG method\\\\nRetrieval method\\\\nQuestion\\\\nAttributed \u001B[0m\n",
       "\u001B[32mcommunity\\\\nHierarchical index\\\\nRetrieval filtering\\\\nZero-shot / CoT \u001B[0m\u001B[32m[\u001B[0m\u001B[32m27\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nNone\\\\nSpecific\\\\nVanilla \u001B[0m\n",
       "\u001B[32mRAG\\\\nVector search\\\\nSpecific\\\\nGraphRAG \u001B[0m\u001B[32m[\u001B[0m\u001B[32m12\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nTraverse\\\\nSpecific&abstract\\\\nLightRAG \u001B[0m\u001B[32m[\u001B[0m\u001B[32m17\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nExtract keywords + \u001B[0m\n",
       "\u001B[32mVector search\\\\nAbstract\\\\nHippoRAG \u001B[0m\u001B[32m[\u001B[0m\u001B[32m18\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nPPR\\\\nSpecific\\\\nArchRAG \u001B[0m\u001B[32m(\u001B[0m\u001B[32mours\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\\\\nHierarchical \u001B[0m\n",
       "\u001B[32msearch\\\\nSpecific&abstract\\\\nSpecifically, Microsoft first proposes a graph-based RAG system\\\\ncalled GraphRAG \u001B[0m\n",
       "\u001B[32m[\u001B[0m\u001B[32m12\u001B[0m\u001B[32m]\u001B[0m\u001B[32m, which first builds a knowledge graph \u001B[0m\u001B[32m(\u001B[0m\u001B[32mKG\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\\\\nby extracting entities and relationships from the external \u001B[0m\n",
       "\u001B[32mcorpus,\\\\nthen employs the Leiden method \u001B[0m\u001B[32m[\u001B[0m\u001B[32m54\u001B[0m\u001B[32m]\u001B[0m\u001B[32m to detect communities and\\\\ngenerates a summary for each community \u001B[0m\n",
       "\u001B[32musing LLMs\\n\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG\u001B[0m\u001B[32m]\u001B[0m\u001B[32m \u001B[0m\n",
       "\u001B[32m.\\\\n2.3\\\\nMicrosoft\\\\u2019s Graph-RAG\\\\nMicrosoft proposed the Graph-RAG system \u001B[0m\u001B[32m[\u001B[0m\u001B[32m8\u001B[0m\u001B[32m]\u001B[0m\u001B[32m, which constructs a\\\\nknowledge\u001B[0m\n",
       "\u001B[32mgraph index with multi-level communities and employs\\\\ntailored strategies for both local and global search. In \u001B[0m\n",
       "\u001B[32mthis section,\\\\nwe focus on its indexing and retrieval operations for local search,\\\\nwhich are relevant to our \u001B[0m\n",
       "\u001B[32mwork.\\\\nAlgorithm 1 outlines the pseudo-code for constructing the graph\\\\nindex G = \u001B[0m\n",
       "\u001B[32m(\u001B[0m\u001B[32mV\\\\n\\\\ud835\\\\udc52\\\\u222aV\\\\n\\\\ud835\\\\udc61, E\u001B[0m\u001B[32m)\u001B[0m\u001B[32m, where V\\\\n\\\\ud835\\\\udc52and V\\\\n\\\\ud835\\\\udc61represent entities \u001B[0m\n",
       "\u001B[32mand\\\\ntext chunks, respectively. Given a text chunk set T, KG-Index first\\\\nKET-RAG: A Cost-Efficient \u001B[0m\n",
       "\u001B[32mMulti-Granular Indexing Framework for Graph-RAG\\\\nConference acronym \\\\u2019XX, June 03\\\\u201305, 2018, Woodstock, \u001B[0m\n",
       "\u001B[32mNY\\\\nAlgorithm 1: KG-Index \u001B[0m\u001B[32m(\u001B[0m\u001B[32mT\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\\\\nInput: The text chunk set T.\\\\nOutput: A TAG index G\\n\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from \u001B[0m\n",
       "\u001B[32mRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001B[0m\u001B[32m]\u001B[0m\u001B[32m . We refer to this decoding procedure as \u001B[0m\n",
       "\u001B[32m\\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, requiring many forward \u001B[0m\n",
       "\u001B[32mpasses. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that p\\\\u03b8\u001B[0m\u001B[32m(\u001B[0m\u001B[32my|x, zi\u001B[0m\u001B[32m)\u001B[0m\u001B[32m \\\\u22480 \u001B[0m\n",
       "\u001B[32mwhere y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run additional forward passes \u001B[0m\n",
       "\u001B[32monce the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as \\\\u201cFast \u001B[0m\n",
       "\u001B[32mDecoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all \u001B[0m\n",
       "\u001B[32mexperiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. \u001B[0m\u001B[32m[\u001B[0m\u001B[32m31\u001B[0m\u001B[32m]\u001B[0m\u001B[32m \u001B[0m\n",
       "\u001B[32mand\\\\nKarpukhin et al. \u001B[0m\u001B[32m[\u001B[0m\u001B[32m26\u001B[0m\u001B[32m]\u001B[0m\u001B[32m, we use the December 2018 dump. Each Wikipedia article is split into \u001B[0m\n",
       "\u001B[32mdisjoint\\\\n100-word chunks, to make a total of 21M documents\\n\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from Retrieval-Augmented Generation for \u001B[0m\n",
       "\u001B[32mKnowledge-Intensive NLP Tasks\u001B[0m\u001B[32m]\u001B[0m\u001B[32m . In one approach, RAG-Sequence, the model uses the same document\\\\nto predict each \u001B[0m\n",
       "\u001B[32mtarget token. The second approach, RAG-Token, can predict each target token based\\\\non a different document. In the\u001B[0m\n",
       "\u001B[32mfollowing, we formally introduce both models and then describe the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as \u001B[0m\n",
       "\u001B[32mthe training and decoding procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same \u001B[0m\n",
       "\u001B[32mretrieved document to generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single \u001B[0m\n",
       "\u001B[32mlatent variable that\\\\nis marginalized to get the seq2seq probability p\u001B[0m\u001B[32m(\u001B[0m\u001B[32my|x\u001B[0m\u001B[32m)\u001B[0m\u001B[32m via a top-K approximation\\n\\n\\n \u001B[0m\n",
       "\u001B[32m(\u001B[0m\u001B[32mAnswer only from retrieval. Only cite sources that are used. Make your response conversational.\u001B[0m\u001B[32m)\u001B[0m\u001B[32m'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;35mHumanMessage\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\u001B[1;33mcontent\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m'Tell me about RAG!'\u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m]\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m)\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG, or Retrieval-Augmented Generation, is a method used in natural language processing that combines retrieval and generation techniques to improve the performance of language models on knowledge-intensive tasks.\n",
      "\n",
      "The RAG method was first proposed by Microsoft and is described in the paper \"ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation.\" According to the paper, RAG involves building a knowledge graph from an external corpus and then using this graph to retrieve relevant information for a given input prompt. The retrieved information is then used to generate a response using large language models (LLMs).\n",
      "\n",
      "There are different variants of RAG, including vanilla RAG, GraphRAG, LightRAG, and ArchRAG. Vanilla RAG uses a simple vector search method to retrieve relevant information, while GraphRAG uses a graph-based approach to build a knowledge graph and retrieves information using a traversal method. LightRAG extracts keywords and uses vector search to retrieve relevant information, while ArchRAG uses a hierarchical search method.\n",
      "\n",
      "Microsoft's Graph-RAG system is further described in the paper \"KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG.\" This paper discusses the indexing and retrieval operations used in Graph-RAG for local search.\n",
      "\n",
      "RAG uses a decoding procedure called \"Thorough Decoding,\" which involves generating multiple candidate responses and selecting the most likely one. However, for longer output sequences, a more efficient decoding method called \"Fast Decoding\" can be used, which avoids the need to run additional forward passes once the candidate set has been generated.\n",
      "\n",
      "The RAG method can be applied to a wide range of knowledge-intensive tasks and has been shown to achieve state-of-the-art performance on certain tasks. It should be noted that RAG requires a large knowledge source, such as a Wikipedia dump, to function effectively.\n",
      "\n",
      "Sources:\n",
      "\n",
      "* \"ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation\"\n",
      "* \"KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG\"\n",
      "* \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\""
     ]
    }
   ],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
    "# instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "stream_chain = chat_prompt| RPrint() | instruct_llm | StrOutputParser()\n",
    "\n",
    "################################################################################################\n",
    "## BEGIN TODO: Implement the retrieval chain to make your system work!\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    ## TODO: Make sure to retrieve history & context from convstore & docstore, respectively.\n",
    "    ## HINT: Our solution uses RunnableAssign, itemgetter, long_reorder, and docs2str\n",
    "    | RunnableAssign({'history' : lambda d: None})\n",
    "    | RunnableAssign({'context' : lambda d: None})\n",
    ")\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    ## TODO: Make sure to retrieve history & context from convstore & docstore, respectively.\n",
    "    ## HINT: Our solution uses RunnableAssign, itemgetter, long_reorder, and docs2str\n",
    "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str})\n",
    "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
    "    | RPrint()\n",
    ")\n",
    "## END TODO\n",
    "################################################################################################\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ## First perform the retrieval based on the input message\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
    "\n",
    "\n",
    "## Start of Agent Event Loop\n",
    "test_question = \"Tell me about RAG!\"  ## <- modify as desired\n",
    "\n",
    "## Before you launch your gradio interface, make sure your thing works\n",
    "for response in chat_gen(test_question, return_buffer=False):\n",
    "    print(response, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9W7sC5Z6BfqM",
   "metadata": {
    "id": "9W7sC5Z6BfqM"
   },
   "source": [
    "### **任务 4：** 与 Gradio 聊天机器人交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fMP3l7QL2JWT",
   "metadata": {
    "id": "fMP3l7QL2JWT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.41.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://9559e568fdd1714603.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9559e568fdd1714603.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://9559e568fdd1714603.gradio.live\n",
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCb3RVVfbmQ0",
   "metadata": {
    "id": "yCb3RVVfbmQ0"
   },
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **第 4 部分：** 保存索引以用于评估\n",
    "\n",
    "实现 RAG 链后，请参考[官方文档](https://python.langchain.com/docs/integrations/vectorstores/faiss#saving-and-loading)保存您积累出来的向量存储。最后的评估会用到！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "Y4se5wQ4Afda",
   "metadata": {
    "id": "Y4se5wQ4Afda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    }
   ],
   "source": [
    "## Save and compress your index\n",
    "docstore.save_local(\"docstore_index\")\n",
    "!tar czvf docstore_index.tgz docstore_index\n",
    "\n",
    "!rm -rf docstore_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LsI7NivbIgFw",
   "metadata": {
    "id": "LsI7NivbIgFw"
   },
   "source": [
    "如果所有内容都已正确保存，就可以执行以下代码从 `tgz` 压缩文件拿到索引了（只要安装好了 pip 环境）。当您确认这个代码单元能拿到您的索引之后，把 `docstore_index.tgz` 下载下来，下个 notebook 会用到！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "Qs8820ucIu1t",
   "metadata": {
    "id": "Qs8820ucIu1t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n",
      ".\\n2.3\\nMicrosoft\\u2019s Graph-RAG\\nMicrosoft proposed the Graph-RAG system [8], which constructs a\\nknowledge graph index with multi-level communities and employs\\ntailored strategies for both local and global search. In this section,\\nwe focus on its indexing and retrieval operations for local search,\\nwhich are relevant to our work.\\nAlgorithm 1 outlines the pseudo-code for constructing the graph\\nindex G = (V\\n\\ud835\\udc52\\u222aV\\n\\ud835\\udc61, E), where V\\n\\ud835\\udc52and V\\n\\ud835\\udc61represent entities and\\ntext chunks, respectively. Given a text chunk set T, KG-Index first\\nKET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG\\nConference acronym \\u2019XX, June 03\\u201305, 2018, Woodstock, NY\\nAlgorithm 1: KG-Index (T)\\nInput: The text chunk set T.\\nOutput: A TAG index G\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "!tar xzvf docstore_index.tgz\n",
    "new_db = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = new_db.similarity_search(\"Testing the index\")\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "as_3vWJGKB2F",
   "metadata": {
    "id": "as_3vWJGKB2F"
   },
   "source": [
    "----\n",
    "\n",
    "## **第 5 部分：** 总结\n",
    "\n",
    "恭喜！如果您的 RAG 链能正常运行，就继续进入 08_evaluation.ipynb 进行 **RAG 评估**吧！\n",
    "\n",
    "### <font color=\"#76b900\">**非常好！**</font>\n",
    "\n",
    "### **接下来**：\n",
    "**[可选]** 回顾 notebook 顶部的“思考问题”。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
