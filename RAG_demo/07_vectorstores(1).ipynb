{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qk4Uw_iSr3Mc",
   "metadata": {
    "id": "Qk4Uw_iSr3Mc"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 7:** ä½¿ç”¨å‘é‡å­˜å‚¨å®ç°æ£€ç´¢å¢å¼ºç”Ÿæˆ</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "æˆ‘ä»¬åœ¨å‰é¢çš„ notebook ä¸­äº†è§£å¹¶å°è¯•äº†åµŒå…¥æ¨¡å‹ã€‚è®¨è®ºäº†å®ƒåœ¨é•¿æ–‡æ¡£æ¯”è¾ƒä¸­çš„åº”ç”¨ï¼Œå¹¶ä»¥å®ƒä¸ºä¸»å¹²å®ç°äº†åŸºäºè¯­ä¹‰çš„æ¯”è¾ƒã€‚æœ¬ notebook å°†æŠŠè¿™ä¸ªæ€è·¯ç”¨åˆ°æ£€ç´¢æ¨¡å‹ä¸Šï¼Œæ¢ç´¢å¦‚ä½•é *å‘é‡å­˜å‚¨*æ¥æ„å»ºè‡ªåŠ¨ä¿å­˜å’Œæ£€ç´¢ä¿¡æ¯çš„èŠå¤©æœºå™¨äººç³»ç»Ÿã€‚\n",
    "\n",
    "<br>\n",
    "\n",
    "### **å­¦ä¹ ç›®æ ‡ï¼š**\n",
    "\n",
    "* ç†è§£è¯­ä¹‰ç›¸ä¼¼åº¦ç³»ç»Ÿæ˜¯æ€ä¹ˆæ–¹ä¾¿åœ°å®ç°æ£€ç´¢çš„ã€‚\n",
    "* å­¦ä¼šå°†æ£€ç´¢æ¨¡å—æ•´åˆåˆ°èŠå¤©æ¨¡å‹ç³»ç»Ÿä¸­ï¼Œä»¥åˆ›å»ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·¥ä½œæµï¼Œç”¨äºå®Œæˆæ–‡æ¡£æ£€ç´¢æˆ–å¯¹è¯å†…å­˜ç¼“å†²ç­‰ä»»åŠ¡ã€‚\n",
    "\n",
    "<br>  \n",
    "\n",
    "### **æ€è€ƒé—®é¢˜ï¼š**\n",
    "\n",
    "* æœ¬ notebook ä¸ä¼šå°è¯•åŠ å…¥å±‚æ¬¡åŒ–æ¨ç†ï¼ˆhierachical reasoningï¼‰æˆ–éæœ´ç´ ï¼ˆnon-naiveï¼‰çš„ RAGï¼Œå¦‚è§„åˆ’æ™ºèƒ½ä½“ï¼ˆpalnning agentsï¼‰ã€‚æƒ³æƒ³éœ€è¦å¦‚ä½•è°ƒæ•´æ‰èƒ½è®©è¿™äº›ç»„ä»¶åœ¨ LCEL é“¾ä¸­è¿è¡Œã€‚\n",
    "* æ€è€ƒå°†å‘é‡å­˜å‚¨æ–¹æ¡ˆç”¨åœ¨è§„æ¨¡åŒ–éƒ¨ç½²çš„æœ€å¥½æ—¶æœºæ˜¯ä»€ä¹ˆï¼Œä»¥åŠä»€ä¹ˆæ—¶å€™éœ€è¦ç”¨ GPU è¿›è¡Œä¼˜åŒ–ã€‚\n",
    "\n",
    "<br>  \n",
    "\n",
    "### **Notebook ç‰ˆæƒå£°æ˜ï¼š**\n",
    "\n",
    "* æœ¬ notebook æ˜¯ [**NVIDIA æ·±åº¦å­¦ä¹ åŸ¹è®­ä¸­å¿ƒ**](https://www.nvidia.cn/training/)çš„è¯¾ç¨‹[**ã€Šæ„å»ºå¤§è¯­è¨€æ¨¡å‹ RAG æ™ºèƒ½ä½“ã€‹**](https://www.nvidia.cn/training/instructor-led-workshops/building-rag-agents-with-llms/)ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œæœªç» NVIDIA æˆæƒä¸å¾—åˆ†å‘ã€‚\n",
    "\n",
    "<br> \n",
    "\n",
    "### **ç¯å¢ƒè®¾ç½®ï¼š**"
   ]
  },
  {
   "cell_type": "code",
   "id": "5XmeiiOWtuxC",
   "metadata": {
    "id": "5XmeiiOWtuxC",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# %%capture\n",
    "## ^^ Comment out if you want to see the pip install process\n",
    "\n",
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37fe234-2bdb-4107-8483-efda9aa5e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf",
   "metadata": {
    "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## ç¬¬ 1 éƒ¨åˆ†ï¼šRAG å·¥ä½œæµæ¦‚è¿°\n",
    "\n",
    "æ­¤ notebook å°†æ¢ç´¢å¤šä¸ªèŒƒå¼å¹¶ç»™å‡ºå‚è€ƒä»£ç ï¼Œä»¥å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨æœ€å¸¸è§ä¸€äº›çš„æ£€ç´¢å¢å¼ºå·¥ä½œæµã€‚å…·ä½“æ¥è¯´å°†æ¶µç›–ä»¥ä¸‹éƒ¨åˆ†ï¼ˆæ¯ä¸ªéƒ¨åˆ†å„æœ‰ä¾§é‡ï¼‰ï¼š\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***é€‚ç”¨äºäº¤äº’å¼å¯¹è¯çš„å‘é‡å­˜å‚¨å·¥ä½œæµï¼š***\n",
    "* ä¸ºæ–°å¯¹è¯ç”Ÿæˆè¯­ä¹‰åµŒå…¥ã€‚\n",
    "* å°†æ¶ˆæ¯æ­£æ–‡æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä»¥ä¾›æ£€ç´¢ã€‚\n",
    "* åœ¨å‘é‡å­˜å‚¨ä¸­æŸ¥è¯¢ç›¸å…³æ¶ˆæ¯å¡«å……åˆ° LLM ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***å¤„ç†ä»»æ„æ–‡æ¡£çš„å·¥ä½œæµï¼š***\n",
    "* **å°†æ–‡æ¡£åˆ†å¿«å¹¶å¤„ç†æˆæœ‰ç”¨ä¿¡æ¯ã€‚**\n",
    "* ä¸ºæ¯ä¸ª**æ–°æ–‡æ¡£å—**ç”Ÿæˆè¯­ä¹‰åµŒå…¥ã€‚\n",
    "* å°†**å—æ­£æ–‡ï¼ˆchunk bodiesï¼‰**å­˜åˆ°å‘é‡å­˜å‚¨ä¸­ä»¥ä¾›æ£€ç´¢ã€‚\n",
    "* åœ¨å‘é‡å­˜å‚¨ä¸­æŸ¥è¯¢ç›¸å…³çš„**å—**ï¼Œç”¨æ¥å¡«å…… LLM ä¸Šä¸‹æ–‡ã€‚\n",
    "\t+ ***å¯é€‰ï¼š*ä¿®æ”¹/åˆæˆç»“æœä»¥è·å¾—æ›´å¥½çš„ LLM ç»“æœã€‚**\n",
    "\n",
    "<br>\n",
    "\n",
    "> **é€‚ç”¨äºä»»æ„æ–‡æ¡£ç›®å½•çš„æ‰©å±•å·¥ä½œæµï¼š**\n",
    "* å°†**æ¯ä¸ªæ–‡æ¡£**åˆ†ä¸ºå¤šä¸ªå—å¹¶å¤„ç†æˆæœ‰ç”¨çš„ä¿¡æ¯ã€‚\n",
    "* ä¸ºæ¯ä¸ªæ–°æ–‡æ¡£å—ç”Ÿæˆè¯­ä¹‰åµŒå…¥ã€‚\n",
    "* å°†å—æ­£æ–‡å­˜åˆ°**å¯æ‰©å±•çš„å‘é‡æ•°æ®åº“ä¸­ä»¥å®ç°å¿«é€Ÿæ£€ç´¢**ã€‚\n",
    "\t+ ***å¯é€‰ï¼š*åˆ©ç”¨æ›´å¤§ç³»ç»Ÿçš„å±‚æ¬¡åŒ–ç»“æ„æˆ–å…ƒæ•°æ®ç»“æ„ã€‚**\n",
    "* åœ¨**å‘é‡æ•°æ®åº“**ä¸­æŸ¥è¯¢ç›¸å…³çš„å—æ¥å¡«å…… LLM ä¸Šä¸‹æ–‡ã€‚\n",
    "\t+ *å¯é€‰ï¼š*ä¿®æ”¹/åˆæˆç»“æœä»¥è·å¾—æ›´å¥½çš„ LLM ç»“æœã€‚\n",
    "\n",
    "<br>  \n",
    "\n",
    "ä¸ RAG ç›¸å…³çš„ä¸€äº›é‡è¦æœ¯è¯­éƒ½å¯ä»¥åœ¨ [**LlamaIndex Concepts é¡µé¢**](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html) æŸ¥åˆ°ï¼Œè¿™æ˜¯å­¦ä¹  LlamaIndex åŠ è½½å’Œæ£€ç´¢ç­–ç•¥çš„å¾ˆå¥½çš„èµ„æºã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨åœ¨å­¦ä¹ æ­¤ notebook çš„è¿‡ç¨‹ä¸­å‚è€ƒå®ƒï¼Œå¹¶é¼“åŠ±æ‚¨åœ¨è¯¾åè¯•è¯• LlamaIndex äº²æ‰‹ä½“ä¼šå®ƒçš„ä¼˜ç¼ºç‚¹ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437",
   "metadata": {
    "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437"
   },
   "source": [
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/data_connection_langchain.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Retrieval | LangChain**ğŸ¦œï¸ğŸ”—](https://python.langchain.com/docs/modules/data_connection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XaZ20XoeSTD-",
   "metadata": {
    "id": "XaZ20XoeSTD-"
   },
   "source": [
    "----\n",
    "\n",
    "<br>  \n",
    "\n",
    "## **ç¬¬ 2 éƒ¨åˆ†ï¼š** ç”¨äºå¯¹è¯å†å²çš„ RAG\n",
    "\n",
    "åœ¨ä¹‹å‰çš„æ¢ç´¢ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†æ–‡æ¡£åµŒå…¥æ¨¡å‹çš„åŠŸèƒ½ï¼Œå¹¶ç”¨å®ƒæ¥åµŒå…¥ã€å­˜å‚¨å’Œæ¯”è¾ƒæ–‡æœ¬çš„è¯­ä¹‰å‘é‡è¡¨ç¤ºã€‚å°½ç®¡æˆ‘ä»¬å¯ä»¥åŠ¨æ‰‹å°†å…¶æ‰©å±•åˆ°å‘é‡å­˜å‚¨é¢†åŸŸï¼Œä½†å¦‚æœç”¨æ ‡å‡† API é…åˆæ¡†æ¶çš„è¯ï¼Œå°±èƒ½å‘ç°å®ƒå·²ç»æ›¿æˆ‘ä»¬å®Œæˆäº†å¾ˆå¤šç¹é‡çš„å·¥ä½œï¼\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LRx0XUf_Sdxw",
   "metadata": {
    "id": "LRx0XUf_Sdxw"
   },
   "source": [
    "### **ç¬¬ 1 æ­¥ï¼š** åˆ›å»ºä¸€æ®µå¯¹è¯\n",
    "\n",
    "æƒ³è±¡ä¸€æ®µ Llama-13B èŠå¤©æ™ºèƒ½ä½“å’Œä¸€åªåä¸º Beras çš„ç†Šä¹‹é—´çš„å¯¹è¯ã€‚è¿™æ®µå¯¹è¯åŒ…å«äº†å¤§é‡ç»†èŠ‚å’Œæ½œåœ¨çš„åˆ†æ”¯ï¼Œä¸ºæˆ‘ä»¬çš„ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„æ•°æ®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "IUfCuMkoShWI",
   "metadata": {
    "id": "IUfCuMkoShWI"
   },
   "outputs": [],
   "source": [
    "conversation = [  ## This conversation was generated partially by an AI system, and modified to exhibit desirable properties\n",
    "    \"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\",\n",
    "    \"[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America\",\n",
    "    \"[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\",\n",
    "    \"[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for you!\"\n",
    "    \"[Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.\",\n",
    "    \"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching documentaries about them.\"\n",
    "    \"[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just curious, ya know!\",\n",
    "    \"[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains and their significance!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tDL2tAo2Skh2",
   "metadata": {
    "id": "tDL2tAo2Skh2"
   },
   "source": [
    "ä»ç„¶å¯ä»¥ç”¨ä¸Šä¸€ä¸ª notebook çš„æ‰‹åŠ¨åµŒå…¥ç­–ç•¥ï¼Œä½†æˆ‘ä»¬å®Œå…¨å¯ä»¥è®©å‘é‡æ•°æ®åº“æ›¿æˆ‘ä»¬åšï¼\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5hIp943mSqGZ",
   "metadata": {
    "id": "5hIp943mSqGZ"
   },
   "source": [
    "### **ç¬¬ 2 æ­¥ï¼š** æ„å»ºå‘é‡å­˜å‚¨æ£€ç´¢å™¨\n",
    "\n",
    "ä¸ºäº†æµç¨‹åŒ–å¯¹è¯ä¸­çš„ç›¸ä¼¼æ€§æŸ¥è¯¢ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‘é‡å­˜å‚¨æ¥å¸®åŠ©æˆ‘ä»¬è¿½è¸ªæ–‡æœ¬ï¼**å‘é‡å­˜å‚¨**ï¼ˆVector Storesï¼‰æˆ–è€…å«å‘é‡å­˜å‚¨ç³»ç»Ÿï¼Œå¯¹åµŒå…¥/æ¯”è¾ƒç­–ç•¥çš„å¤§éƒ¨åˆ†åº•å±‚ç»†èŠ‚åšäº†æŠ½è±¡ï¼Œä¸ºåŠ è½½å’Œæ¯”è¾ƒå‘é‡æä¾›äº†ä¸€ä¸ªç®€æ´çš„æ¥å£ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pnaOBgexS-kp",
   "metadata": {
    "id": "pnaOBgexS-kp"
   },
   "source": [
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/vector_stores.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Vector Stores | LangChain**ğŸ¦œï¸ğŸ”—](https://python.langchain.com/docs/modules/data_connection/vectorstores/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DwZUh6kgS5Ki",
   "metadata": {
    "id": "DwZUh6kgS5Ki"
   },
   "source": [
    "<br>\n",
    "\n",
    "é™¤äº†å€ŸåŠ© API ç®€åŒ–æµç¨‹å¤–ï¼Œå‘é‡å­˜å‚¨è¿˜åœ¨èƒŒåå®ç°äº†è¿æ¥å™¨ï¼ˆconnectorï¼‰ã€é›†æˆï¼ˆintegrationï¼‰å’Œä¼˜åŒ–ã€‚æˆ‘ä»¬å°†ä» [**FAISS å‘é‡å­˜å‚¨**](https://python.langchain.com/docs/integrations/vectorstores/faiss)å¼€å§‹ï¼Œå®ƒé›†æˆäº†å…¼å®¹ LangChain çš„åµŒå…¥æ¨¡å‹ [**FAISS (Facebook AI Similarity Search)**](https://github.com/facebookresearch/faiss)ï¼Œä»è€Œå…è®¸åœ¨æœ¬åœ°å®ç°å¿«é€Ÿå¯æ‰©å±•çš„æµç¨‹ï¼\n",
    "\n",
    "\n",
    "**å…·ä½“æ¥è¯´ï¼š**\n",
    "\n",
    "1. æˆ‘ä»¬å¯ä»¥é€šè¿‡ `from_texts` æ„é€ å™¨å°†å¯¹è¯è¾“å…¥åˆ° [**FAISS å‘é‡å­˜å‚¨**](https://python.langchain.com/docs/integrations/vectorstores/faiss)ã€‚è¿™æ ·æˆ‘ä»¬çš„å¯¹è¯æ•°æ®å’ŒåµŒå…¥æ¨¡å‹å°±ä¼šç”¨æ¥åˆ›å»ºç´¢å¼•ã€‚\n",
    "2. ç„¶åï¼Œè¿™ä¸ªå‘é‡å­˜å‚¨å°±å¯ä»¥ä½œä¸ºæ£€ç´¢å™¨ï¼Œæ”¯æŒç”¨ LangChain è¿è¡Œæ—¶ API æ¥æ£€ç´¢æ–‡æ¡£ã€‚\n",
    "\n",
    "ä»¥ä¸‹å†…å®¹å±•ç¤ºäº†å¦‚ä½•æ„å»º FAISS å‘é‡å­˜å‚¨å¹¶ä½¿ç”¨ LangChain `vectorstore` API å°†å…¶ä½œä¸ºæ£€ç´¢å™¨ä½¿ç”¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1kE2-ejoTKKU",
   "metadata": {
    "id": "1kE2-ejoTKKU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.5 ms, sys: 11.8 ms, total: 71.3 ms\n",
      "Wall time: 810 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ^^ This cell will be timed to see how long the conversation embedding takes\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## Streamlined from_texts FAISS vectorstore construction from text list\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "retriever = convstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muN66v5PW5dW",
   "metadata": {
    "id": "muN66v5PW5dW"
   },
   "source": [
    "ç°åœ¨ï¼Œæ£€ç´¢å™¨å¯ä»¥åƒä»»ä½•å…¶ä»–å¯è¿è¡Œçš„ LangChain ä¸€æ ·ç”¨äºæŸ¥è¯¢å‘é‡å­˜å‚¨ä¸­çš„æŸäº›ç›¸å…³æ–‡æ¡£ï¼š\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "kNZJTnlEWVYh",
   "metadata": {
    "id": "kNZJTnlEWVYh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you![Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m[\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m\"\u001B[0m\u001B[32m[\u001B[0m\u001B[32mUser\u001B[0m\u001B[32m]\u001B[0m\u001B[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001B[0m\n",
       "\u001B[32mrocky mountains?\"\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m'\u001B[0m\u001B[32m[\u001B[0m\u001B[32mAgent\u001B[0m\u001B[32m]\u001B[0m\u001B[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001B[0m\n",
       "\u001B[32mand their significance!'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m'\u001B[0m\u001B[32m[\u001B[0m\u001B[32mAgent\u001B[0m\u001B[32m]\u001B[0m\u001B[32m I hope you get to visit them someday, Beras! It would be a great adventure for \u001B[0m\n",
       "\u001B[32myou!\u001B[0m\u001B[32m[\u001B[0m\u001B[32mBeras\u001B[0m\u001B[32m]\u001B[0m\u001B[32m Thank you for the suggestion! Ill definitely keep it in mind for the future.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m\"\u001B[0m\u001B[32m[\u001B[0m\u001B[32mAgent\u001B[0m\u001B[32m]\u001B[0m\u001B[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001B[0m\n",
       "\u001B[32monline or watching documentaries about them.\u001B[0m\u001B[32m[\u001B[0m\u001B[32mBeras\u001B[0m\u001B[32m]\u001B[0m\u001B[32m I live in the arctic, so I'm not used to the warm climate \u001B[0m\n",
       "\u001B[32mthere. I was just curious, ya know!\"\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m]\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "SE1eDZTEWScC",
   "metadata": {
    "id": "SE1eDZTEWScC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across North America'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m[\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m'\u001B[0m\u001B[32m[\u001B[0m\u001B[32mAgent\u001B[0m\u001B[32m]\u001B[0m\u001B[32m The Rocky Mountains are a beautiful and majestic range of mountains that stretch \u001B[0m\n",
       "\u001B[32macross North America'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m\"\u001B[0m\u001B[32m[\u001B[0m\u001B[32mAgent\u001B[0m\u001B[32m]\u001B[0m\u001B[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001B[0m\n",
       "\u001B[32monline or watching documentaries about them.\u001B[0m\u001B[32m[\u001B[0m\u001B[32mBeras\u001B[0m\u001B[32m]\u001B[0m\u001B[32m I live in the arctic, so I'm not used to the warm climate \u001B[0m\n",
       "\u001B[32mthere. I was just curious, ya know!\"\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m\"\u001B[0m\u001B[32m[\u001B[0m\u001B[32mUser\u001B[0m\u001B[32m]\u001B[0m\u001B[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001B[0m\n",
       "\u001B[32mrocky mountains?\"\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;35mDocument\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;33mpage_content\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m'\u001B[0m\u001B[32m[\u001B[0m\u001B[32mAgent\u001B[0m\u001B[32m]\u001B[0m\u001B[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001B[0m\n",
       "\u001B[32mand their significance!'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m]\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtNCEXLYTVf4",
   "metadata": {
    "id": "mtNCEXLYTVf4"
   },
   "source": [
    "å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæ£€ç´¢å·¥å…·ä»æˆ‘ä»¬çš„æŸ¥è¯¢ä¸­æ‰¾åˆ°äº†ä¸€äº›è¯­ä¹‰ç›¸å…³çš„æ–‡æ¡£ã€‚æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ï¼Œä¸æ˜¯æ‰€æœ‰æ–‡æ¡£éƒ½æœ‰ç”¨æˆ–æ¸…æ™°ã€‚æ¯”å¦‚ï¼Œå¦‚æœä¸æ˜¯å‡ºäºä¸Šä¸‹æ–‡ï¼Œæ£€ç´¢è¯¢é—®*â€œæ‚¨çš„å§“åâ€*æ—¶æŠŠ*â€œBerasâ€*æ£€ç´¢å‡ºæ¥å¯èƒ½ä¸æ˜¯ä¸ªå¥½äº‹ã€‚æå‰è€ƒè™‘åˆ°æ½œåœ¨çš„é—®é¢˜å¹¶è®© LLM ç»„ä»¶ç›¸äº’ååŒæ›´æœ‰å¯èƒ½è®© RAG è¾¾åˆ°å¥½çš„æ•ˆæœã€‚\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZEDEzpqmTYMv",
   "metadata": {
    "id": "ZEDEzpqmTYMv"
   },
   "source": [
    "### **ç¬¬ 3 æ­¥ï¼š** å°†å¯¹è¯æ£€ç´¢åŠŸèƒ½æ•´åˆåˆ°æˆ‘ä»¬çš„é“¾ä¸­\n",
    "\n",
    "ç°åœ¨ï¼Œæˆ‘ä»¬å·²æŠŠæ£€ç´¢å™¨ç»„ä»¶ä½œä¸ºä¸€ä¸ªé“¾äº†ï¼Œå¯ä»¥åƒä»¥å‰ä¸€æ ·å°†å…¶æ•´åˆåˆ°ç°æœ‰çš„èŠå¤©ç³»ç»Ÿä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥æ„å»ºä¸€ä¸ª***ä¿æŒåœ¨çº¿ï¼ˆalways-onï¼‰çš„ RAG*** äº†ï¼Œå…¶ä¸­ï¼š\n",
    "* **é»˜è®¤æƒ…å†µä¸‹ï¼Œæ£€ç´¢å™¨å§‹ç»ˆåœ¨æ£€ç´¢ä¸Šä¸‹æ–‡ã€‚**\n",
    "* **ç”Ÿæˆå™¨æ ¹æ®æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ‰§è¡Œæ“ä½œã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64abe478-9bcb-4802-a26e-dc5a1756e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Utility Runnables/Methods\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        if preface: print(preface, end=\"\")\n",
    "        pprint(x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## Optional; Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "uue5UY3_TcvF",
   "metadata": {
    "id": "uue5UY3_TcvF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">It seems that Beras lives in the Arctic. The cooler climate there is quite different from the warm climate found in</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the Rocky Mountains, which Beras was interested in learning more about.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mIt seems that Beras lives in the Arctic. The cooler climate there is quite different from the warm climate found in\u001B[0m\n",
       "\u001B[1;38;2;118;185;0mthe Rocky Mountains, which Beras was interested in learning more about.\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {question}\"\n",
    "    \"\\nAnswer the user conversationally. User is not aware of context.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'question': (lambda x:x)\n",
    "    }\n",
    "    | context_prompt\n",
    "    # | RPrint()\n",
    "    | instruct_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(chain.invoke(\"Where does Beras live?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FSIqTMuuTjIh",
   "metadata": {
    "id": "FSIqTMuuTjIh"
   },
   "source": [
    "å¤šè¯•å‡ ä¸ªè°ƒç”¨ï¼Œçœ‹çœ‹æ–°é…ç½®çš„æ•ˆæœã€‚æ— è®ºæ‚¨é€‰æ‹©çš„æ˜¯å“ªä¸ªæ¨¡å‹ï¼Œéƒ½å¯ä»¥å…ˆä»ä¸‹é¢çš„å‡ ä¸ªé—®é¢˜å¼€å§‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4jDJwrYpTmpd",
   "metadata": {
    "id": "4jDJwrYpTmpd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! The Rocky Mountains are a stunning range of mountains that span across North America. Unfortunately, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">without more context, I can't give you a precise location beyond that. However, if you're interested, you can </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">always look up more information online or watch documentaries about the Rocky Mountains to learn more!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mHello there! The Rocky Mountains are a stunning range of mountains that span across North America. Unfortunately, \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mwithout more context, I can't give you a precise location beyond that. However, if you're interested, you can \u001B[0m\n",
       "\u001B[1;38;2;118;185;0malways look up more information online or watch documentaries about the Rocky Mountains to learn more!\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "-artagLfTpBy",
   "metadata": {
    "id": "-artagLfTpBy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! The Rocky Mountains are a stunning range of mountains that stretch across North America. To answer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">your question about their location, they do not directly border California. In fact, they run from Canada all the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">way to New Mexico, passing through the states of Montana, Idaho, Wyoming, Colorado, and a small part of northern </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">New Mexico. I hope that helps clarify their location for you!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mHello there! The Rocky Mountains are a stunning range of mountains that stretch across North America. To answer \u001B[0m\n",
       "\u001B[1;38;2;118;185;0myour question about their location, they do not directly border California. In fact, they run from Canada all the \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mway to New Mexico, passing through the states of Montana, Idaho, Wyoming, Colorado, and a small part of northern \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mNew Mexico. I hope that helps clarify their location for you!\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains? Are they close to California?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "GDgjdfdpTrV5",
   "metadata": {
    "id": "GDgjdfdpTrV5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hi there! Beras is a big blue bear and based on the context, we know that they live in the arctic. However, the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">context doesn't provide details about the exact location or the distance between the Rocky Mountains and the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">arctic. The Rocky Mountains are a prominent mountain range in North America, and the arctic region is also quite </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">large. Therefore, it's not possible to provide an accurate distance without more specific information. I'm here to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">share that the Rocky Mountains are beautiful and majestic and stretch across North America, and I encourage Beras </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to learn more about them by doing some research online or watching documentaries.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mHi there! Beras is a big blue bear and based on the context, we know that they live in the arctic. However, the \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mcontext doesn't provide details about the exact location or the distance between the Rocky Mountains and the \u001B[0m\n",
       "\u001B[1;38;2;118;185;0marctic. The Rocky Mountains are a prominent mountain range in North America, and the arctic region is also quite \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mlarge. Therefore, it's not possible to provide an accurate distance without more specific information. I'm here to \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mshare that the Rocky Mountains are beautiful and majestic and stretch across North America, and I encourage Beras \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mto learn more about them by doing some research online or watching documentaries.\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"How far away is Beras from the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wp9-8CbT0L9",
   "metadata": {
    "id": "8wp9-8CbT0L9"
   },
   "source": [
    "<br>  \n",
    "\n",
    "æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°æŠŠè¿™ä¸ªä¿æŒåœ¨çº¿ï¼ˆalways-onï¼‰çš„æ£€ç´¢èŠ‚ç‚¹æ”¾åˆ°å¾ªç¯é‡Œæ•ˆæœå¾ˆä¸é”™ï¼Œå› ä¸ºç›®å‰è¾“å…¥ LLM çš„ä¸Šä¸‹æ–‡ä»ç„¶ç›¸å¯¹è¾ƒå°ã€‚æœ‰å¿…è¦åå¤å°è¯•åµŒå…¥å¤§å°ã€ä¸Šä¸‹æ–‡é™åˆ¶ç­‰é…ç½®ï¼Œæ¥æ›´å¥½åœ°é¢„æµ‹æ¨¡å‹è¡¨ç°ï¼Œå¹¶è¡¡é‡ä¸ºæé«˜æ€§èƒ½å€¼å¾—åšå‡ºä½•ç§åŠªåŠ›ã€‚\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OnpOybOhUCTf",
   "metadata": {
    "id": "OnpOybOhUCTf"
   },
   "source": [
    "### **ç¬¬ 4 æ­¥ï¼š** è‡ªåŠ¨å¯¹è¯å­˜å‚¨\n",
    "\n",
    "ç°åœ¨å‘é‡å­˜å‚¨å·²ç»å¯ä»¥å·¥ä½œäº†ï¼Œæˆ‘ä»¬æœ€åå†åšä¸€ä¸ªé›†æˆï¼šåŠ ä¸€ä¸ªè°ƒç”¨ `add_texts` æ›´æ–°å­˜å‚¨çŠ¶æ€çš„è¿è¡Œæ—¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "FsK6-AtRVdcZ",
   "metadata": {
    "id": "FsK6-AtRVdcZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While I'm sure the Rocky Mountains would provide a magnificent backdrop for enjoying ice cream, it's worth noting </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that the Rocky Mountains are primarily known for their natural beauty and outdoor activities. Hiking, mountain </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">biking, and wildlife viewing are popular activities. The Rocky Mountains are also home to a number of national </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">parks, such as Banff National Park in Canada and Rocky Mountain National Park in the United States. These parks </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">offer a range of activities for visitors of all ages and abilities. However, I'm always here to help you discover </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">more about the world, and if you have any other questions about the Rocky Mountains or any other topic, please </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">don't hesitate to ask!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mWhile I'm sure the Rocky Mountains would provide a magnificent backdrop for enjoying ice cream, it's worth noting \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mthat the Rocky Mountains are primarily known for their natural beauty and outdoor activities. Hiking, mountain \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mbiking, and wildlife viewing are popular activities. The Rocky Mountains are also home to a number of national \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mparks, such as Banff National Park in Canada and Rocky Mountain National Park in the United States. These parks \u001B[0m\n",
       "\u001B[1;38;2;118;185;0moffer a range of activities for visitors of all ages and abilities. However, I'm always here to help you discover \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mmore about the world, and if you have any other questions about the Rocky Mountains or any other topic, please \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mdon't hesitate to ask!\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While I can't truly guess your favorite food, Beras, from our conversation, it does seem that you are quite fond of</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ice cream! It's always fun to enjoy a cool treat in a beautiful setting, like the Rocky Mountains.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mWhile I can't truly guess your favorite food, Beras, from our conversation, it does seem that you are quite fond of\u001B[0m\n",
       "\u001B[1;38;2;118;185;0mice cream! It's always fun to enjoy a cool treat in a beautiful setting, like the Rocky Mountains.\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Well Beras, I must admit, I made an assumption there! You're absolutely right that your love for ice cream was just</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">an observation and not a fact. Your fondness for honey is indeed delightful, just as the Rocky Mountains are! It's </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">fascinating how diverse tastes can be, isn't it?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mWell Beras, I must admit, I made an assumption there! You're absolutely right that your love for ice cream was just\u001B[0m\n",
       "\u001B[1;38;2;118;185;0man observation and not a fact. Your fondness for honey is indeed delightful, just as the Rocky Mountains are! It's \u001B[0m\n",
       "\u001B[1;38;2;118;185;0mfascinating how diverse tastes can be, isn't it?\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Well, based on our conversation, I can see that you indeed love ice cream! However, if I were to try and guess your</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">favorite food again, I would have to say honey, as you mentioned it being your actual favorite. But I'm an </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">assistant, so I don't have the ability to know for certain unless you tell me. :)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mWell, based on our conversation, I can see that you indeed love ice cream! However, if I were to try and guess your\u001B[0m\n",
       "\u001B[1;38;2;118;185;0mfavorite food again, I would have to say honey, as you mentioned it being your actual favorite. But I'm an \u001B[0m\n",
       "\u001B[1;38;2;118;185;0massistant, so I don't have the ability to know for certain unless you tell me. :\u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Reset knowledge base and define what it means to add more messages.\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([f\"User said {d.get('input')}\", f\"Agent said {d.get('output')}\"])\n",
    "    return d.get('output')\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    "    \"\\nAnswer the user conversationally. Make sure the conversation flows naturally.\\n\"\n",
    "    \"[Agent]\"\n",
    ")\n",
    "\n",
    "\n",
    "conv_chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'input': (lambda x:x)\n",
    "    }\n",
    "    | RunnableAssign({'output' : chat_prompt | instruct_llm | StrOutputParser()})\n",
    "    | partial(save_memory_and_get_output, vstore=convstore)\n",
    ")\n",
    "\n",
    "pprint(conv_chain.invoke(\"I'm glad you agree! I can't wait to get some ice cream there! It's such a good food!\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Can you guess what my favorite food is?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Actually, my favorite is honey! Not sure where you got that idea?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"I see! Fair enough! Do you know my favorite food now?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KRMW6G7NVSWF",
   "metadata": {
    "id": "KRMW6G7NVSWF"
   },
   "source": [
    "ä¸åŒäºå°†ä¸Šä¸‹æ–‡æ³¨å…¥ LLM çš„æ›´è‡ªåŠ¨åŒ–çš„å…¨æ–‡æœ¬ï¼ˆfull-textï¼‰æˆ–åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œè¿™æ ·å¯é¿å…ä¸Šä¸‹æ–‡é•¿åº¦å¤±æ§ã€‚è¿™ç§ç­–ç•¥è™½ç„¶ç§°ä¸ä¸Šå®Œå…¨å¯é ï¼Œä½†å¯¹äºéç»“æ„åŒ–çš„å¯¹è¯æ¥è¯´å·²ç»æ˜¯ä¸€ä¸ªå·¨å¤§çš„æ”¹è¿›äº†ï¼ˆç”šè‡³ä¸éœ€è¦å€ŸåŠ©ä¸€ä¸ªå¼ºå¤§çš„æŒ‡ä»¤å¾®è°ƒæ¨¡å‹åšæ§½ä½å¡«å……ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9TPkh3SaLbqh",
   "metadata": {
    "id": "9TPkh3SaLbqh"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **ç¬¬ 3 éƒ¨åˆ† [ç»ƒä¹ ]ï¼š** ç”¨ RAG è¿›è¡Œæ–‡æ¡£å—æ£€ç´¢\n",
    "\n",
    "é‰´äºæˆ‘ä»¬ä¹‹å‰å¯¹æ–‡æ¡£åŠ è½½çš„æ¢ç´¢ï¼Œæ‚¨åº”è¯¥å·²ç»ç†Ÿæ‚‰å¯¹æ•°æ®å—åµŒå…¥å’Œæ£€ç´¢äº†ã€‚ç°åœ¨å€¼å¾—èŠ±ç‚¹æ—¶é—´ç»§ç»­è¿‡ä¸€éï¼Œå› ä¸ºæŠŠ RAG ç”¨åœ¨æ–‡æ¡£ä¸Šæ˜¯ä¸€æŠŠåŒåˆƒå‰‘ï¼šå®ƒçœ‹èµ·æ¥ä¼¼ä¹å¼€ç®±å³ç”¨ï¼Œä½†æƒ³è®©å®ƒåœ¨å®é™…åº”ç”¨ä¸­ä¿æŒå¯é çš„æ€§èƒ½éœ€è¦éå¸¸è°¨æ…åœ°ä¼˜åŒ–ã€‚æˆ‘ä»¬ä¹Ÿå€Ÿæ­¤æœºä¼šå›é¡¾ä¸€ä¸‹åŸºæœ¬çš„ LCEL æŠ€èƒ½ï¼\n",
    "\n",
    "<br> \n",
    "\n",
    "### **ç»ƒä¹ ï¼š**\n",
    "\n",
    "æ‚¨å¯èƒ½è¿˜è®°å¾—ä¹‹å‰æˆ‘ä»¬ç”¨ [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv) åŠ è½½äº†ä¸€äº›æ¯”è¾ƒçŸ­çš„æ–‡ç« ï¼š\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "docs = [\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL\n",
    "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct\n",
    "]\n",
    "```\n",
    "\n",
    "æ ¹æ®æ‰€å­¦ï¼Œé€‰æ‹©å‡ ä¸ªè®ºæ–‡ï¼Œå¹¶å¼€å‘ä¸€ä¸ªèƒ½è®¨è®ºè¿™äº›è®ºæ–‡çš„èŠå¤©æœºå™¨äººï¼\n",
    "\n",
    "<br>  \n",
    "\n",
    "è™½ç„¶è¿™æ˜¯ä¸€é¡¹ç›¸å½“è‰°å·¨çš„ä»»åŠ¡ï¼Œä½†ä¸‹é¢å°†æä¾›**å¤§éƒ¨åˆ†**å®ç°è¿‡ç¨‹ã€‚æ¼”ç¤ºè¿‡åï¼Œè®¸å¤šå¿…é¡»çš„ç¯èŠ‚å°±å·²ç»å®ç°å¥½äº†ï¼Œæ‚¨çœŸæ­£çš„ä»»åŠ¡æ˜¯å°†å®ƒä»¬é›†æˆåˆ°æœ€ç»ˆçš„ `retrieval_chain`ã€‚æ‚¨ä¼šåœ¨æœ€åä¸€ä¸ª notebook æŠŠå®ƒä»¬é›†æˆåˆ°é“¾ä¸­æ¥å®Œæˆè¯„ä¼°æµ‹è¯•ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jSjfCtiQnj9e",
   "metadata": {
    "id": "jSjfCtiQnj9e"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **ä»»åŠ¡ 1ï¼š** è½½å…¥å¹¶åˆ†å—æ‚¨çš„æ–‡æ¡£\n",
    "\n",
    "ä»¥ä¸‹ä»£ç æä¾›äº†ä¸€äº›å¯ä»¥è½½å…¥åˆ° RAG é“¾çš„é»˜è®¤è®ºæ–‡ã€‚æ‚¨å¯ä»¥æ ¹æ®éœ€è¦é€‰æ›´å¤šçš„è®ºæ–‡ï¼Œä½†è¦æ³¨æ„é•¿æ–‡æ¡£çš„å¤„ç†æ—¶é—´ä¹Ÿæ›´é•¿ã€‚å…¶ä¸­è¿˜æœ‰ä¸€äº›åˆ©äºæé«˜ RAG æ€§èƒ½çš„ç®€åŒ–å‡è®¾åŠå¤„ç†æ­¥éª¤ï¼š\n",
    "\n",
    "* æ–‡æ¡£ä»…æˆªå–â€œå‚è€ƒâ€œâ€ï¼ˆReferencesï¼‰éƒ¨åˆ†ä¹‹å‰çš„å†…å®¹ã€‚é˜²æ­¢ç³»ç»Ÿè€ƒè™‘å†—é•¿å’Œä¸é‡è¦çš„å¼•ç”¨å’Œé™„å½•ã€‚\n",
    "* æœ‰ä¸€ä¸ªèƒ½æä¾›å…¨å±€è§†è§’çš„åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ–‡æ¡£çš„æ•°æ®å—ã€‚å¦‚æœæ‚¨çš„å·¥ä½œæµå¹¶ä¸æ˜¯æ¯æ¬¡æ£€ç´¢éƒ½æä¾›å…ƒæ•°æ®ï¼Œé‚£ä¹ˆè¿™ä¸ªæ•°æ®å—å°±ä¼šå¾ˆæœ‰ç”¨ï¼Œç”šè‡³å¯ä»¥åœ¨åˆé€‚çš„æ—¶å€™ä½œä¸ºæ›´é«˜ä¼˜å…ˆçº§ä¿¡æ¯çš„ä¸€éƒ¨åˆ†ã€‚\n",
    "* æ­¤å¤–ï¼Œè¿˜ä¼šæ’å…¥å…ƒæ•°æ®æ¡ç›®ä»¥æä¾›å¸¸è§„ä¿¡æ¯ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œä¼šæœ‰ä¸€äº›èåˆè¿›äº†å…ƒæ•°æ®çš„è·¨æ–‡æ¡£æ•°æ®å—ã€‚\n",
    "\n",
    "**æ³¨æ„ï¼š** ***ä¸ºæ‰§è¡Œè¯„ä¼°ï¼Œè¯·è‡³å°‘æ”¾è¿›ä¸€ç¯‡å‘è¡¨æ—¶é—´ä¸è¶…è¿‡ä¸€ä¸ªæœˆçš„è®ºæ–‡ï¼***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "S-3FBdT_lhVT",
   "metadata": {
    "id": "S-3FBdT_lhVT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Documents\n",
      "Chunking Documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sources and discrete reasoning</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Mistral 7B</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - ReAct: Synergizing Reasoning and Acting in Language Models</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - High-Resolution Image Synthesis with Latent Diffusion Models</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Learning Transferable Visual Models From Natural Language Supervision </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0mAvailable Documents:\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - Attention Is All You Need\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \u001B[0m\n",
       "\u001B[1;38;2;118;185;0msources and discrete reasoning\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - Mistral 7B\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - ReAct: Synergizing Reasoning and Acting in Language Models\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - High-Resolution Image Synthesis with Latent Diffusion Models\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m - Learning Transferable Visual Models From Natural Language Supervision \u001B[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0\n",
      " - # Chunks: 35\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-02'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Attention Is All You Need'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Kaiser, Illia Polosukhin'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">constituency parsing both with large and limited training data.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2023-08-02'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Attention Is All You Need'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz \u001B[0m\n",
       "\u001B[32mKaiser, Illia Polosukhin'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural \u001B[0m\n",
       "\u001B[32mnetworks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder \u001B[0m\n",
       "\u001B[32mthrough an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on \u001B[0m\n",
       "\u001B[32mattention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation\u001B[0m\n",
       "\u001B[32mtasks show these models to be\\nsuperior in quality while being more parallelizable and requiring \u001B[0m\n",
       "\u001B[32msignificantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation \u001B[0m\n",
       "\u001B[32mtask, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 \u001B[0m\n",
       "\u001B[32mEnglish-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 \u001B[0m\n",
       "\u001B[32mafter training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the \u001B[0m\n",
       "\u001B[32mliterature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish \u001B[0m\n",
       "\u001B[32mconstituency parsing both with large and limited training data.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1\n",
      " - # Chunks: 45\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2019-05-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Encoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">layer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">empirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">including\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">absolute improvement), SQuAD v1.1 question answering\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2019-05-24'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional \u001B[0m\n",
       "\u001B[32mEncoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to \u001B[0m\n",
       "\u001B[32mpre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right \u001B[0m\n",
       "\u001B[32mcontext in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output \u001B[0m\n",
       "\u001B[32mlayer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language \u001B[0m\n",
       "\u001B[32minference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and \u001B[0m\n",
       "\u001B[32mempirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, \u001B[0m\n",
       "\u001B[32mincluding\\npushing the GLUE score to 80.5% \u001B[0m\u001B[32m(\u001B[0m\u001B[32m7.7% point absolute improvement\u001B[0m\u001B[32m)\u001B[0m\u001B[32m, MultiNLI\\naccuracy to 86.7% \u001B[0m\u001B[32m(\u001B[0m\u001B[32m4.6% \u001B[0m\n",
       "\u001B[32mabsolute improvement\u001B[0m\u001B[32m)\u001B[0m\u001B[32m, SQuAD v1.1 question answering\\nTest F1 to 93.2 \u001B[0m\u001B[32m(\u001B[0m\u001B[32m1.5 point absolute improvement\u001B[0m\u001B[32m)\u001B[0m\u001B[32m and SQuAD \u001B[0m\n",
       "\u001B[32mv2.0 Test F1 to 83.1\\n\u001B[0m\u001B[32m(\u001B[0m\u001B[32m5.1 point absolute improvement\u001B[0m\u001B[32m)\u001B[0m\u001B[32m.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 2\n",
      " - # Chunks: 46\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-04-12'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, Douwe Kiela'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">behind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">world knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">downstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">index\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">passages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2021-04-12'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, \u001B[0m\n",
       "\u001B[32mHeinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, Douwe Kiela'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, \u001B[0m\n",
       "\u001B[32mand achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and\u001B[0m\n",
       "\u001B[32mprecisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags \u001B[0m\n",
       "\u001B[32mbehind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their \u001B[0m\n",
       "\u001B[32mworld knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism \u001B[0m\n",
       "\u001B[32mto\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive \u001B[0m\n",
       "\u001B[32mdownstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation \u001B[0m\u001B[32m(\u001B[0m\u001B[32mRAG\u001B[0m\u001B[32m)\u001B[0m\u001B[32m -- \u001B[0m\n",
       "\u001B[32mmodels which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG \u001B[0m\n",
       "\u001B[32mmodels where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector \u001B[0m\n",
       "\u001B[32mindex\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which \u001B[0m\n",
       "\u001B[32mconditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different \u001B[0m\n",
       "\u001B[32mpassages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set\u001B[0m\n",
       "\u001B[32mthe state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific \u001B[0m\n",
       "\u001B[32mretrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more \u001B[0m\n",
       "\u001B[32mspecific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 3\n",
      " - # Chunks: 40\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-05-01'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge sources and discrete reasoning'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Amnon Shashua, Moshe Tenenholtz'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Huge language models (LMs) have ushered in a new era for AI, serving as a\\ngateway to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">natural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">systems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge and Language (MRKL, pronounced \"miracle\") system,\\nsome of the technical challenges in implementing it, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2022-05-01'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external \u001B[0m\n",
       "\u001B[32mknowledge sources and discrete reasoning'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit\u001B[0m\n",
       "\u001B[32mBata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, \u001B[0m\n",
       "\u001B[32mAmnon Shashua, Moshe Tenenholtz'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Huge language models \u001B[0m\u001B[32m(\u001B[0m\u001B[32mLMs\u001B[0m\u001B[32m)\u001B[0m\u001B[32m have ushered in a new era for AI, serving as a\\ngateway to \u001B[0m\n",
       "\u001B[32mnatural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently \u001B[0m\n",
       "\u001B[32mlimited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a \u001B[0m\n",
       "\u001B[32msystems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to \u001B[0m\n",
       "\u001B[32mlinguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete \u001B[0m\n",
       "\u001B[32mknowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, \u001B[0m\n",
       "\u001B[32mKnowledge and Language \u001B[0m\u001B[32m(\u001B[0m\u001B[32mMRKL, pronounced \"miracle\"\u001B[0m\u001B[32m)\u001B[0m\u001B[32m system,\\nsome of the technical challenges in implementing it, \u001B[0m\n",
       "\u001B[32mand Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 4\n",
      " - # Chunks: 21\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-10-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Mistral 7B'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, LÃ©lio Renard Lavaud, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">under the Apache 2.0 license.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2023-10-10'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Mistral 7B'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, \u001B[0m\n",
       "\u001B[32mDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, LÃ©lio Renard Lavaud, \u001B[0m\n",
       "\u001B[32mMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior \u001B[0m\n",
       "\u001B[32mperformance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in\u001B[0m\n",
       "\u001B[32mreasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention \u001B[0m\u001B[32m(\u001B[0m\u001B[32mGQA\u001B[0m\u001B[32m)\u001B[0m\u001B[32m for \u001B[0m\n",
       "\u001B[32mfaster\\ninference, coupled with sliding window attention \u001B[0m\u001B[32m(\u001B[0m\u001B[32mSWA\u001B[0m\u001B[32m)\u001B[0m\u001B[32m to effectively handle\\nsequences of arbitrary length\u001B[0m\n",
       "\u001B[32mwith a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, \u001B[0m\n",
       "\u001B[32mthat surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released \u001B[0m\n",
       "\u001B[32munder the Apache 2.0 license.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 5\n",
      " - # Chunks: 44\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conversations with\\nhuman preferences are publicly available </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2023-12-24'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, \u001B[0m\n",
       "\u001B[32mZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Evaluating large language model \u001B[0m\u001B[32m(\u001B[0m\u001B[32mLLM\u001B[0m\u001B[32m)\u001B[0m\u001B[32m based chat assistants is challenging\\ndue to their broad \u001B[0m\n",
       "\u001B[32mcapabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore\u001B[0m\n",
       "\u001B[32musing strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and \u001B[0m\n",
       "\u001B[32mlimitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited \u001B[0m\n",
       "\u001B[32mreasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between \u001B[0m\n",
       "\u001B[32mLLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot \u001B[0m\n",
       "\u001B[32mArena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both \u001B[0m\n",
       "\u001B[32mcontrolled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement \u001B[0m\n",
       "\u001B[32mbetween humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which \u001B[0m\n",
       "\u001B[32mare otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement\u001B[0m\n",
       "\u001B[32meach other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K \u001B[0m\n",
       "\u001B[32mconversations with\\nhuman preferences are publicly available \u001B[0m\n",
       "\u001B[32mat\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 6\n",
      " - # Chunks: 58\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2025-02-14'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Shu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, Yuchi Ma'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation (RAG) has proven effective in integrating\\nexternal knowledge into </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">large language models (LLMs) for question-answer (QA)\\ntasks. The state-of-the-art RAG approaches often use the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">graph data as the\\nexternal data since they capture the rich semantic information and link\\nrelationships between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entities. However, existing graph-based RAG approaches\\ncannot accurately identify the relevant information from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the graph and also\\nconsume large numbers of tokens in the online retrieval process. To address\\nthese issues, we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">introduce a novel graph-based RAG approach, called Attributed\\nCommunity-based Hierarchical RAG (ArchRAG), by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">augmenting the question using\\nattributed communities, and also introducing a novel LLM-based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hierarchical\\nclustering method. To retrieve the most relevant information from the graph for\\nthe question, we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">build a novel hierarchical index structure for the attributed\\ncommunities and develop an effective online </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval method. Experimental\\nresults demonstrate that ArchRAG outperforms existing methods in terms of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">both\\naccuracy and token cost.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2025-02-14'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Shu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, Yuchi Ma'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Retrieval-Augmented Generation \u001B[0m\u001B[32m(\u001B[0m\u001B[32mRAG\u001B[0m\u001B[32m)\u001B[0m\u001B[32m has proven effective in integrating\\nexternal knowledge into \u001B[0m\n",
       "\u001B[32mlarge language models \u001B[0m\u001B[32m(\u001B[0m\u001B[32mLLMs\u001B[0m\u001B[32m)\u001B[0m\u001B[32m for question-answer \u001B[0m\u001B[32m(\u001B[0m\u001B[32mQA\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\\ntasks. The state-of-the-art RAG approaches often use the \u001B[0m\n",
       "\u001B[32mgraph data as the\\nexternal data since they capture the rich semantic information and link\\nrelationships between \u001B[0m\n",
       "\u001B[32mentities. However, existing graph-based RAG approaches\\ncannot accurately identify the relevant information from \u001B[0m\n",
       "\u001B[32mthe graph and also\\nconsume large numbers of tokens in the online retrieval process. To address\\nthese issues, we \u001B[0m\n",
       "\u001B[32mintroduce a novel graph-based RAG approach, called Attributed\\nCommunity-based Hierarchical RAG \u001B[0m\u001B[32m(\u001B[0m\u001B[32mArchRAG\u001B[0m\u001B[32m)\u001B[0m\u001B[32m, by \u001B[0m\n",
       "\u001B[32maugmenting the question using\\nattributed communities, and also introducing a novel LLM-based \u001B[0m\n",
       "\u001B[32mhierarchical\\nclustering method. To retrieve the most relevant information from the graph for\\nthe question, we \u001B[0m\n",
       "\u001B[32mbuild a novel hierarchical index structure for the attributed\\ncommunities and develop an effective online \u001B[0m\n",
       "\u001B[32mretrieval method. Experimental\\nresults demonstrate that ArchRAG outperforms existing methods in terms of \u001B[0m\n",
       "\u001B[32mboth\\naccuracy and token cost.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 7\n",
      " - # Chunks: 64\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2025-02-13'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Yiqian Huang, Shiqi Zhang, Xiaokui Xiao'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Graph-RAG constructs a knowledge graph from text chunks to improve retrieval\\nin Large Language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Model (LLM)-based question answering. It is particularly\\nuseful in domains such as biomedicine, law, and political</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">science, where\\nretrieval often requires multi-hop reasoning over proprietary documents. Some\\nexisting Graph-RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">systems construct KNN graphs based on text chunk relevance,\\nbut this coarse-grained approach fails to capture </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entity relationships within\\ntexts, leading to sub-par retrieval and generation quality. To address this,\\nrecent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">solutions leverage LLMs to extract entities and relationships from text\\nchunks, constructing triplet-based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge graphs. However, this approach\\nincurs significant indexing costs, especially for large document </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">collections.\\n  To ensure a good result accuracy while reducing the indexing cost, we propose\\nKET-RAG, a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multi-granular indexing framework. KET-RAG first identifies a small\\nset of key text chunks and leverages an LLM to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">construct a knowledge graph\\nskeleton. It then builds a text-keyword bipartite graph from all text chunks,\\nserving</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as a lightweight alternative to a full knowledge graph. During\\nretrieval, KET-RAG searches both structures: it </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">follows the local search\\nstrategy of existing Graph-RAG systems on the skeleton while mimicking this\\nsearch on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the bipartite graph to improve retrieval quality. We evaluate eight\\nsolutions on two real-world datasets, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">demonstrating that KET-RAG outperforms\\nall competitors in indexing cost, retrieval effectiveness, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generation\\nquality. Notably, it achieves comparable or superior retrieval quality to\\nMicrosoft's Graph-RAG while </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reducing indexing costs by over an order of\\nmagnitude. Additionally, it improves the generation quality by up to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">32.4%\\nwhile lowering indexing costs by around 20%.\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2025-02-13'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Yiqian Huang, Shiqi Zhang, Xiaokui Xiao'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m\"Graph-RAG constructs a knowledge graph from text chunks to improve retrieval\\nin Large Language \u001B[0m\n",
       "\u001B[32mModel \u001B[0m\u001B[32m(\u001B[0m\u001B[32mLLM\u001B[0m\u001B[32m)\u001B[0m\u001B[32m-based question answering. It is particularly\\nuseful in domains such as biomedicine, law, and political\u001B[0m\n",
       "\u001B[32mscience, where\\nretrieval often requires multi-hop reasoning over proprietary documents. Some\\nexisting Graph-RAG \u001B[0m\n",
       "\u001B[32msystems construct KNN graphs based on text chunk relevance,\\nbut this coarse-grained approach fails to capture \u001B[0m\n",
       "\u001B[32mentity relationships within\\ntexts, leading to sub-par retrieval and generation quality. To address this,\\nrecent \u001B[0m\n",
       "\u001B[32msolutions leverage LLMs to extract entities and relationships from text\\nchunks, constructing triplet-based \u001B[0m\n",
       "\u001B[32mknowledge graphs. However, this approach\\nincurs significant indexing costs, especially for large document \u001B[0m\n",
       "\u001B[32mcollections.\\n  To ensure a good result accuracy while reducing the indexing cost, we propose\\nKET-RAG, a \u001B[0m\n",
       "\u001B[32mmulti-granular indexing framework. KET-RAG first identifies a small\\nset of key text chunks and leverages an LLM to\u001B[0m\n",
       "\u001B[32mconstruct a knowledge graph\\nskeleton. It then builds a text-keyword bipartite graph from all text chunks,\\nserving\u001B[0m\n",
       "\u001B[32mas a lightweight alternative to a full knowledge graph. During\\nretrieval, KET-RAG searches both structures: it \u001B[0m\n",
       "\u001B[32mfollows the local search\\nstrategy of existing Graph-RAG systems on the skeleton while mimicking this\\nsearch on \u001B[0m\n",
       "\u001B[32mthe bipartite graph to improve retrieval quality. We evaluate eight\\nsolutions on two real-world datasets, \u001B[0m\n",
       "\u001B[32mdemonstrating that KET-RAG outperforms\\nall competitors in indexing cost, retrieval effectiveness, and \u001B[0m\n",
       "\u001B[32mgeneration\\nquality. Notably, it achieves comparable or superior retrieval quality to\\nMicrosoft's Graph-RAG while \u001B[0m\n",
       "\u001B[32mreducing indexing costs by over an order of\\nmagnitude. Additionally, it improves the generation quality by up to \u001B[0m\n",
       "\u001B[32m32.4%\\nwhile lowering indexing costs by around 20%.\"\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 8\n",
      " - # Chunks: 123\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-03-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'ReAct: Synergizing Reasoning and Acting in Language Models'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2023-03-10'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'ReAct: Synergizing Reasoning and Acting in Language Models'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'While large language models \u001B[0m\u001B[32m(\u001B[0m\u001B[32mLLMs\u001B[0m\u001B[32m)\u001B[0m\u001B[32m have demonstrated impressive capabilities\\nacross tasks in \u001B[0m\n",
       "\u001B[32mlanguage understanding and interactive decision making, their\\nabilities for reasoning \u001B[0m\u001B[32m(\u001B[0m\u001B[32me.g. chain-of-thought \u001B[0m\n",
       "\u001B[32mprompting\u001B[0m\u001B[32m)\u001B[0m\u001B[32m and acting \u001B[0m\u001B[32m(\u001B[0m\u001B[32me.g.\\naction plan generation\u001B[0m\u001B[32m)\u001B[0m\u001B[32m have primarily been studied as separate topics. In \u001B[0m\n",
       "\u001B[32mthis\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an \u001B[0m\n",
       "\u001B[32minterleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, \u001B[0m\n",
       "\u001B[32mand update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, \u001B[0m\n",
       "\u001B[32msuch as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to \u001B[0m\n",
       "\u001B[32ma diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art \u001B[0m\n",
       "\u001B[32mbaselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting\u001B[0m\n",
       "\u001B[32mcomponents.\\nConcretely, on question answering \u001B[0m\u001B[32m(\u001B[0m\u001B[32mHotpotQA\u001B[0m\u001B[32m)\u001B[0m\u001B[32m and fact verification \u001B[0m\u001B[32m(\u001B[0m\u001B[32mFever\u001B[0m\u001B[32m)\u001B[0m\u001B[32m,\\nReAct overcomes issues of\u001B[0m\n",
       "\u001B[32mhallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia\u001B[0m\n",
       "\u001B[32mAPI, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without \u001B[0m\n",
       "\u001B[32mreasoning traces. On two interactive decision making\\nbenchmarks \u001B[0m\u001B[32m(\u001B[0m\u001B[32mALFWorld and WebShop\u001B[0m\u001B[32m)\u001B[0m\u001B[32m, ReAct outperforms \u001B[0m\n",
       "\u001B[32mimitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being\u001B[0m\n",
       "\u001B[32mprompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 9\n",
      " - # Chunks: 52\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-04-13'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'High-Resolution Image Synthesis with Latent Diffusion Models'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'By decomposing the image formation process into a sequential application of\\ndenoising </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">autoencoders, diffusion models (DMs) achieve state-of-the-art\\nsynthesis results on image data and beyond. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Additionally, their formulation\\nallows for a guiding mechanism to control the image generation process </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">without\\nretraining. However, since these models typically operate directly in pixel\\nspace, optimization of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">powerful DMs often consumes hundreds of GPU days and\\ninference is expensive due to sequential evaluations. To </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enable DM training on\\nlimited computational resources while retaining their quality and flexibility,\\nwe apply </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">them in the latent space of powerful pretrained autoencoders. In\\ncontrast to previous work, training diffusion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models on such a representation\\nallows for the first time to reach a near-optimal point between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">complexity\\nreduction and detail preservation, greatly boosting visual fidelity. By\\nintroducing cross-attention </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">layers into the model architecture, we turn\\ndiffusion models into powerful and flexible generators for general </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conditioning\\ninputs such as text or bounding boxes and high-resolution synthesis becomes\\npossible in a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">convolutional manner. Our latent diffusion models (LDMs) achieve\\na new state of the art for image inpainting and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">highly competitive performance\\non various tasks, including unconditional image generation, semantic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scene\\nsynthesis, and super-resolution, while significantly reducing computational\\nrequirements compared to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pixel-based DMs. Code is available at\\nhttps://github.com/CompVis/latent-diffusion .'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2022-04-13'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'High-Resolution Image Synthesis with Latent Diffusion Models'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'By decomposing the image formation process into a sequential application of\\ndenoising \u001B[0m\n",
       "\u001B[32mautoencoders, diffusion models \u001B[0m\u001B[32m(\u001B[0m\u001B[32mDMs\u001B[0m\u001B[32m)\u001B[0m\u001B[32m achieve state-of-the-art\\nsynthesis results on image data and beyond. \u001B[0m\n",
       "\u001B[32mAdditionally, their formulation\\nallows for a guiding mechanism to control the image generation process \u001B[0m\n",
       "\u001B[32mwithout\\nretraining. However, since these models typically operate directly in pixel\\nspace, optimization of \u001B[0m\n",
       "\u001B[32mpowerful DMs often consumes hundreds of GPU days and\\ninference is expensive due to sequential evaluations. To \u001B[0m\n",
       "\u001B[32menable DM training on\\nlimited computational resources while retaining their quality and flexibility,\\nwe apply \u001B[0m\n",
       "\u001B[32mthem in the latent space of powerful pretrained autoencoders. In\\ncontrast to previous work, training diffusion \u001B[0m\n",
       "\u001B[32mmodels on such a representation\\nallows for the first time to reach a near-optimal point between \u001B[0m\n",
       "\u001B[32mcomplexity\\nreduction and detail preservation, greatly boosting visual fidelity. By\\nintroducing cross-attention \u001B[0m\n",
       "\u001B[32mlayers into the model architecture, we turn\\ndiffusion models into powerful and flexible generators for general \u001B[0m\n",
       "\u001B[32mconditioning\\ninputs such as text or bounding boxes and high-resolution synthesis becomes\\npossible in a \u001B[0m\n",
       "\u001B[32mconvolutional manner. Our latent diffusion models \u001B[0m\u001B[32m(\u001B[0m\u001B[32mLDMs\u001B[0m\u001B[32m)\u001B[0m\u001B[32m achieve\\na new state of the art for image inpainting and \u001B[0m\n",
       "\u001B[32mhighly competitive performance\\non various tasks, including unconditional image generation, semantic \u001B[0m\n",
       "\u001B[32mscene\\nsynthesis, and super-resolution, while significantly reducing computational\\nrequirements compared to \u001B[0m\n",
       "\u001B[32mpixel-based DMs. Code is available at\\nhttps://github.com/CompVis/latent-diffusion .'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 10\n",
      " - # Chunks: 155\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-02-26'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Learning Transferable Visual Models From Natural Language Supervision'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'State-of-the-art computer vision systems are trained to predict a fixed set\\nof predetermined </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">object categories. This restricted form of supervision limits\\ntheir generality and usability since additional </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">labeled data is needed to\\nspecify any other visual concept. Learning directly from raw text about images\\nis a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">promising alternative which leverages a much broader source of\\nsupervision. We demonstrate that the simple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-training task of predicting\\nwhich caption goes with which image is an efficient and scalable way to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">learn\\nSOTA image representations from scratch on a dataset of 400 million (image,\\ntext) pairs collected from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">internet. After pre-training, natural language\\nis used to reference learned visual concepts (or describe new ones)</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">enabling\\nzero-shot transfer of the model to downstream tasks. We study the performance\\nof this approach by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">benchmarking on over 30 different existing computer vision\\ndatasets, spanning tasks such as OCR, action </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recognition in videos,\\ngeo-localization, and many types of fine-grained object classification. The\\nmodel </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transfers non-trivially to most tasks and is often competitive with a\\nfully supervised baseline without the need </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for any dataset specific training.\\nFor instance, we match the accuracy of the original ResNet-50 on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ImageNet\\nzero-shot without needing to use any of the 1.28 million training examples it\\nwas trained on. We release</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">our code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Published'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'2021-02-26'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Title'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Learning Transferable Visual Models From Natural Language Supervision'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Authors'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish \u001B[0m\n",
       "\u001B[32mSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'Summary'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'State-of-the-art computer vision systems are trained to predict a fixed set\\nof predetermined \u001B[0m\n",
       "\u001B[32mobject categories. This restricted form of supervision limits\\ntheir generality and usability since additional \u001B[0m\n",
       "\u001B[32mlabeled data is needed to\\nspecify any other visual concept. Learning directly from raw text about images\\nis a \u001B[0m\n",
       "\u001B[32mpromising alternative which leverages a much broader source of\\nsupervision. We demonstrate that the simple \u001B[0m\n",
       "\u001B[32mpre-training task of predicting\\nwhich caption goes with which image is an efficient and scalable way to \u001B[0m\n",
       "\u001B[32mlearn\\nSOTA image representations from scratch on a dataset of 400 million \u001B[0m\u001B[32m(\u001B[0m\u001B[32mimage,\\ntext\u001B[0m\u001B[32m)\u001B[0m\u001B[32m pairs collected from the \u001B[0m\n",
       "\u001B[32minternet. After pre-training, natural language\\nis used to reference learned visual concepts \u001B[0m\u001B[32m(\u001B[0m\u001B[32mor describe new ones\u001B[0m\u001B[32m)\u001B[0m\n",
       "\u001B[32menabling\\nzero-shot transfer of the model to downstream tasks. We study the performance\\nof this approach by \u001B[0m\n",
       "\u001B[32mbenchmarking on over 30 different existing computer vision\\ndatasets, spanning tasks such as OCR, action \u001B[0m\n",
       "\u001B[32mrecognition in videos,\\ngeo-localization, and many types of fine-grained object classification. The\\nmodel \u001B[0m\n",
       "\u001B[32mtransfers non-trivially to most tasks and is often competitive with a\\nfully supervised baseline without the need \u001B[0m\n",
       "\u001B[32mfor any dataset specific training.\\nFor instance, we match the accuracy of the original ResNet-50 on \u001B[0m\n",
       "\u001B[32mImageNet\\nzero-shot without needing to use any of the 1.28 million training examples it\\nwas trained on. We release\u001B[0m\n",
       "\u001B[32mour code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
    ")\n",
    "\n",
    "## TODO: Please pick some papers and add them to the list as you'd like\n",
    "## NOTE: To re-use for the final assessment, make sure at least one paper is < 1 month old\n",
    "print(\"Loading Documents\")\n",
    "docs = [\n",
    "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
    "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
    "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
    "    ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
    "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
    "    ArxivLoader(query=\"2502.09891\").load(),\n",
    "    ArxivLoader(query=\"2502.09304\").load(),\n",
    "    ## Some longer papers\n",
    "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct Paper\n",
    "    ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
    "    ArxivLoader(query=\"2103.00020\").load(),  ## CLIP Paper\n",
    "    ## TODO: Feel free to add more\n",
    "]\n",
    "\n",
    "## Cut the paper short if references is included.\n",
    "## This is a standard string in papers.\n",
    "for doc in docs:\n",
    "    content = json.dumps(doc[0].page_content)\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]\n",
    "\n",
    "## Split the documents and also filter out stubs (overly short chunks)\n",
    "print(\"Chunking Documents\")\n",
    "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
    "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
    "\n",
    "## Make some custom Chunks to give big-picture details\n",
    "doc_string = \"Available Documents:\"\n",
    "doc_metadata = []\n",
    "for chunks in docs_chunks:\n",
    "    metadata = getattr(chunks[0], 'metadata', {})\n",
    "    doc_string += \"\\n - \" + metadata.get('Title')\n",
    "    doc_metadata += [str(metadata)]\n",
    "\n",
    "extra_chunks = [doc_string] + doc_metadata\n",
    "\n",
    "## Printing out some summary information for reference\n",
    "pprint(doc_string, '\\n')\n",
    "for i, chunks in enumerate(docs_chunks):\n",
    "    print(f\"Document {i}\")\n",
    "    print(f\" - # Chunks: {len(chunks)}\")\n",
    "    print(f\" - Metadata: \")\n",
    "    pprint(chunks[0].metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4pWU_OOnnrsT",
   "metadata": {
    "id": "4pWU_OOnnrsT"
   },
   "source": [
    "### **ä»»åŠ¡ 2ï¼š** æ„å»ºæ–‡æ¡£å‘é‡å­˜å‚¨\n",
    "\n",
    "æˆ‘ä»¬ç°åœ¨å·²ç»æœ‰äº†æ‰€æœ‰ç»„ä»¶ï¼Œå¯ä»¥ç»§ç»­å›´ç»•å®ƒä»¬åˆ›å»ºç´¢å¼•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lwwmr3aptwCg",
   "metadata": {
    "id": "lwwmr3aptwCg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Vector Stores\n",
      "CPU times: user 1.85 s, sys: 115 ms, total: 1.97 s\n",
      "Wall time: 54.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Constructing Vector Stores\")\n",
    "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
    "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j39JwCKubto0",
   "metadata": {
    "id": "j39JwCKubto0"
   },
   "source": [
    "<br>\n",
    "\n",
    "æ¥ç€åƒä¸‹é¢è¿™æ ·æŠŠç´¢å¼•åˆå¹¶ä¸ºä¸€ä¸ªï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "Q7us66iPVc70",
   "metadata": {
    "id": "Q7us66iPVc70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 695 chunks\n"
     ]
    }
   ],
   "source": [
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VU_VEx2mqJUK",
   "metadata": {
    "id": "VU_VEx2mqJUK"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **ä»»åŠ¡ 3ï¼š[ç»ƒä¹ ]** å®ç° RAG é“¾\n",
    "\n",
    "ç»ˆäºï¼Œä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼Œæ¥å®ç° RAG å·¥ä½œæµå§ï¼å›é¡¾ä¸€ä¸‹ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ï¼š\n",
    "\n",
    "* ä¸€ç§ç”¨å‘é‡å­˜å‚¨ä»é›¶åˆ›å»ºå¯¹è¯è®°å¿†çš„æ–¹æ³•ï¼ˆç”¨ `default_FAISS()` åˆå§‹åŒ–ï¼‰\n",
    "* é€šè¿‡ `ArxivLoader` é¢„åŠ è½½äº†åŒ…æ‹¬æ–‡æ¡£ä¿¡æ¯çš„å‘é‡å­˜å‚¨ï¼ˆå­˜åœ¨ `docstore` é‡Œï¼‰ã€‚\n",
    "\n",
    "å†å€ŸåŠ©å‡ ä¸ªå·¥å…·ï¼Œå°±èƒ½é›†æˆæ‚¨çš„é“¾äº†ï¼æˆ‘ä»¬è¿˜æä¾›äº†å‡ ä¸ªé¢å¤–çš„ä¾¿æ·å·¥å…·ï¼ˆ`doc2str` åŠ `RPrint`ï¼‰ï¼Œæ‚¨å¯ä»¥é…Œæƒ…ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œä¸€äº›å¯åŠ¨æç¤ºè¯å’Œç»“æ„å·²ç»å®šä¹‰å¥½äº†ã€‚\n",
    "\n",
    "> **åŸºäºä¸Šè¿°è¿™äº›ï¼š** å®ç° `retrieval_chain` å§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "-RXSrb1GcNff",
   "metadata": {
    "id": "-RXSrb1GcNff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about RAG!'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'history'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'[Quote from ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">.\\\\nRAG method\\\\nRetrieval method\\\\nQuestion\\\\nAttributed community\\\\nHierarchical index\\\\nRetrieval </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">filtering\\\\nZero-shot / CoT [27]\\\\nNone\\\\nSpecific\\\\nVanilla RAG\\\\nVector search\\\\nSpecific\\\\nGraphRAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[12]\\\\nTraverse\\\\nSpecific&amp;abstract\\\\nLightRAG [17]\\\\nExtract keywords + Vector search\\\\nAbstract\\\\nHippoRAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[18]\\\\nPPR\\\\nSpecific\\\\nArchRAG (ours)\\\\nHierarchical search\\\\nSpecific&amp;abstract\\\\nSpecifically, Microsoft first </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">proposes a graph-based RAG system\\\\ncalled GraphRAG [12], which first builds a knowledge graph (KG)\\\\nby extracting</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entities and relationships from the external corpus,\\\\nthen employs the Leiden method [54] to detect communities </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\\\ngenerates a summary for each community using LLMs\\n[Quote from KET-RAG: A Cost-Efficient Multi-Granular </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Indexing Framework for Graph-RAG] .\\\\n2.3\\\\nMicrosoft\\\\u2019s Graph-RAG\\\\nMicrosoft proposed the Graph-RAG system </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[8], which constructs a\\\\nknowledge graph index with multi-level communities and employs\\\\ntailored strategies for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">both local and global search. In this section,\\\\nwe focus on its indexing and retrieval operations for local </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">search,\\\\nwhich are relevant to our work.\\\\nAlgorithm 1 outlines the pseudo-code for constructing the graph\\\\nindex</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">G = (V\\\\n\\\\ud835\\\\udc52\\\\u222aV\\\\n\\\\ud835\\\\udc61, E), where V\\\\n\\\\ud835\\\\udc52and V\\\\n\\\\ud835\\\\udc61represent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">entities and\\\\ntext chunks, respectively. Given a text chunk set T, KG-Index first\\\\nKET-RAG: A Cost-Efficient </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Multi-Granular Indexing Framework for Graph-RAG\\\\nConference acronym \\\\u2019XX, June 03\\\\u201305, 2018, Woodstock, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">NY\\\\nAlgorithm 1: KG-Index (T)\\\\nInput: The text chunk set T.\\\\nOutput: A TAG index G\\n[Quote from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . We refer to this decoding procedure as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, requiring many forward </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">passes. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that p\\\\u03b8(y|x, zi) \\\\u22480 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">where y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run additional forward passes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">once the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as \\\\u201cFast </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Decoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">disjoint\\\\n100-word chunks, to make a total of 21M documents\\n[Quote from Retrieval-Augmented Generation for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge-Intensive NLP Tasks] . In one approach, RAG-Sequence, the model uses the same document\\\\nto predict each </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">target token. The second approach, RAG-Token, can predict each target token based\\\\non a different document. In the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following, we formally introduce both models and then describe the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the training and decoding procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieved document to generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">latent variable that\\\\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation\\n'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;38;2;118;185;0m{\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'input'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'Tell me about RAG!'\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'history'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m''\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[32m'context'\u001B[0m\u001B[1;38;2;118;185;0m: \u001B[0m\u001B[32m'\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation\u001B[0m\u001B[32m]\u001B[0m\u001B[32m \u001B[0m\n",
       "\u001B[32m.\\\\nRAG method\\\\nRetrieval method\\\\nQuestion\\\\nAttributed community\\\\nHierarchical index\\\\nRetrieval \u001B[0m\n",
       "\u001B[32mfiltering\\\\nZero-shot / CoT \u001B[0m\u001B[32m[\u001B[0m\u001B[32m27\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nNone\\\\nSpecific\\\\nVanilla RAG\\\\nVector search\\\\nSpecific\\\\nGraphRAG \u001B[0m\n",
       "\u001B[32m[\u001B[0m\u001B[32m12\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nTraverse\\\\nSpecific&abstract\\\\nLightRAG \u001B[0m\u001B[32m[\u001B[0m\u001B[32m17\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nExtract keywords + Vector search\\\\nAbstract\\\\nHippoRAG \u001B[0m\n",
       "\u001B[32m[\u001B[0m\u001B[32m18\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nPPR\\\\nSpecific\\\\nArchRAG \u001B[0m\u001B[32m(\u001B[0m\u001B[32mours\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\\\\nHierarchical search\\\\nSpecific&abstract\\\\nSpecifically, Microsoft first \u001B[0m\n",
       "\u001B[32mproposes a graph-based RAG system\\\\ncalled GraphRAG \u001B[0m\u001B[32m[\u001B[0m\u001B[32m12\u001B[0m\u001B[32m]\u001B[0m\u001B[32m, which first builds a knowledge graph \u001B[0m\u001B[32m(\u001B[0m\u001B[32mKG\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\\\\nby extracting\u001B[0m\n",
       "\u001B[32mentities and relationships from the external corpus,\\\\nthen employs the Leiden method \u001B[0m\u001B[32m[\u001B[0m\u001B[32m54\u001B[0m\u001B[32m]\u001B[0m\u001B[32m to detect communities \u001B[0m\n",
       "\u001B[32mand\\\\ngenerates a summary for each community using LLMs\\n\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from KET-RAG: A Cost-Efficient Multi-Granular \u001B[0m\n",
       "\u001B[32mIndexing Framework for Graph-RAG\u001B[0m\u001B[32m]\u001B[0m\u001B[32m .\\\\n2.3\\\\nMicrosoft\\\\u2019s Graph-RAG\\\\nMicrosoft proposed the Graph-RAG system \u001B[0m\n",
       "\u001B[32m[\u001B[0m\u001B[32m8\u001B[0m\u001B[32m]\u001B[0m\u001B[32m, which constructs a\\\\nknowledge graph index with multi-level communities and employs\\\\ntailored strategies for \u001B[0m\n",
       "\u001B[32mboth local and global search. In this section,\\\\nwe focus on its indexing and retrieval operations for local \u001B[0m\n",
       "\u001B[32msearch,\\\\nwhich are relevant to our work.\\\\nAlgorithm 1 outlines the pseudo-code for constructing the graph\\\\nindex\u001B[0m\n",
       "\u001B[32mG = \u001B[0m\u001B[32m(\u001B[0m\u001B[32mV\\\\n\\\\ud835\\\\udc52\\\\u222aV\\\\n\\\\ud835\\\\udc61, E\u001B[0m\u001B[32m)\u001B[0m\u001B[32m, where V\\\\n\\\\ud835\\\\udc52and V\\\\n\\\\ud835\\\\udc61represent \u001B[0m\n",
       "\u001B[32mentities and\\\\ntext chunks, respectively. Given a text chunk set T, KG-Index first\\\\nKET-RAG: A Cost-Efficient \u001B[0m\n",
       "\u001B[32mMulti-Granular Indexing Framework for Graph-RAG\\\\nConference acronym \\\\u2019XX, June 03\\\\u201305, 2018, Woodstock, \u001B[0m\n",
       "\u001B[32mNY\\\\nAlgorithm 1: KG-Index \u001B[0m\u001B[32m(\u001B[0m\u001B[32mT\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\\\\nInput: The text chunk set T.\\\\nOutput: A TAG index G\\n\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from \u001B[0m\n",
       "\u001B[32mRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001B[0m\u001B[32m]\u001B[0m\u001B[32m . We refer to this decoding procedure as \u001B[0m\n",
       "\u001B[32m\\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, requiring many forward \u001B[0m\n",
       "\u001B[32mpasses. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that p\\\\u03b8\u001B[0m\u001B[32m(\u001B[0m\u001B[32my|x, zi\u001B[0m\u001B[32m)\u001B[0m\u001B[32m \\\\u22480 \u001B[0m\n",
       "\u001B[32mwhere y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run additional forward passes \u001B[0m\n",
       "\u001B[32monce the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as \\\\u201cFast \u001B[0m\n",
       "\u001B[32mDecoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all \u001B[0m\n",
       "\u001B[32mexperiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. \u001B[0m\u001B[32m[\u001B[0m\u001B[32m31\u001B[0m\u001B[32m]\u001B[0m\u001B[32m \u001B[0m\n",
       "\u001B[32mand\\\\nKarpukhin et al. \u001B[0m\u001B[32m[\u001B[0m\u001B[32m26\u001B[0m\u001B[32m]\u001B[0m\u001B[32m, we use the December 2018 dump. Each Wikipedia article is split into \u001B[0m\n",
       "\u001B[32mdisjoint\\\\n100-word chunks, to make a total of 21M documents\\n\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from Retrieval-Augmented Generation for \u001B[0m\n",
       "\u001B[32mKnowledge-Intensive NLP Tasks\u001B[0m\u001B[32m]\u001B[0m\u001B[32m . In one approach, RAG-Sequence, the model uses the same document\\\\nto predict each \u001B[0m\n",
       "\u001B[32mtarget token. The second approach, RAG-Token, can predict each target token based\\\\non a different document. In the\u001B[0m\n",
       "\u001B[32mfollowing, we formally introduce both models and then describe the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as \u001B[0m\n",
       "\u001B[32mthe training and decoding procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same \u001B[0m\n",
       "\u001B[32mretrieved document to generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single \u001B[0m\n",
       "\u001B[32mlatent variable that\\\\nis marginalized to get the seq2seq probability p\u001B[0m\u001B[32m(\u001B[0m\u001B[32my|x\u001B[0m\u001B[32m)\u001B[0m\u001B[32m via a top-K approximation\\n'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Conversation History Retrieval:\\n\\n\\n Document Retrieval:\\n[Quote from ArchRAG: Attributed Community-based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Hierarchical Retrieval-Augmented Generation] .\\\\nRAG method\\\\nRetrieval method\\\\nQuestion\\\\nAttributed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">community\\\\nHierarchical index\\\\nRetrieval filtering\\\\nZero-shot / CoT [27]\\\\nNone\\\\nSpecific\\\\nVanilla </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG\\\\nVector search\\\\nSpecific\\\\nGraphRAG [12]\\\\nTraverse\\\\nSpecific&amp;abstract\\\\nLightRAG [17]\\\\nExtract keywords + </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Vector search\\\\nAbstract\\\\nHippoRAG [18]\\\\nPPR\\\\nSpecific\\\\nArchRAG (ours)\\\\nHierarchical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">search\\\\nSpecific&amp;abstract\\\\nSpecifically, Microsoft first proposes a graph-based RAG system\\\\ncalled GraphRAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[12], which first builds a knowledge graph (KG)\\\\nby extracting entities and relationships from the external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpus,\\\\nthen employs the Leiden method [54] to detect communities and\\\\ngenerates a summary for each community </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using LLMs\\n[Quote from KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">.\\\\n2.3\\\\nMicrosoft\\\\u2019s Graph-RAG\\\\nMicrosoft proposed the Graph-RAG system [8], which constructs a\\\\nknowledge</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">graph index with multi-level communities and employs\\\\ntailored strategies for both local and global search. In </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this section,\\\\nwe focus on its indexing and retrieval operations for local search,\\\\nwhich are relevant to our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">work.\\\\nAlgorithm 1 outlines the pseudo-code for constructing the graph\\\\nindex G = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(V\\\\n\\\\ud835\\\\udc52\\\\u222aV\\\\n\\\\ud835\\\\udc61, E), where V\\\\n\\\\ud835\\\\udc52and V\\\\n\\\\ud835\\\\udc61represent entities </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\\\ntext chunks, respectively. Given a text chunk set T, KG-Index first\\\\nKET-RAG: A Cost-Efficient </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Multi-Granular Indexing Framework for Graph-RAG\\\\nConference acronym \\\\u2019XX, June 03\\\\u201305, 2018, Woodstock, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">NY\\\\nAlgorithm 1: KG-Index (T)\\\\nInput: The text chunk set T.\\\\nOutput: A TAG index G\\n[Quote from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . We refer to this decoding procedure as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, requiring many forward </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">passes. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that p\\\\u03b8(y|x, zi) \\\\u22480 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">where y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run additional forward passes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">once the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as \\\\u201cFast </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Decoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">disjoint\\\\n100-word chunks, to make a total of 21M documents\\n[Quote from Retrieval-Augmented Generation for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge-Intensive NLP Tasks] . In one approach, RAG-Sequence, the model uses the same document\\\\nto predict each </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">target token. The second approach, RAG-Token, can predict each target token based\\\\non a different document. In the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following, we formally introduce both models and then describe the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the training and decoding procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieved document to generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">latent variable that\\\\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation\\n\\n\\n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(Answer only from retrieval. Only cite sources that are used. Make your response conversational.)'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about RAG!'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;35mChatPromptValue\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;33mmessages\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[1;38;2;118;185;0m[\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;35mSystemMessage\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m            \u001B[0m\u001B[1;33mcontent\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m'You are a document chatbot. Help the user as they ask questions about documents. User messaged\u001B[0m\n",
       "\u001B[32mjust asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  \u001B[0m\n",
       "\u001B[32mConversation History Retrieval:\\n\\n\\n Document Retrieval:\\n\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from ArchRAG: Attributed Community-based \u001B[0m\n",
       "\u001B[32mHierarchical Retrieval-Augmented Generation\u001B[0m\u001B[32m]\u001B[0m\u001B[32m .\\\\nRAG method\\\\nRetrieval method\\\\nQuestion\\\\nAttributed \u001B[0m\n",
       "\u001B[32mcommunity\\\\nHierarchical index\\\\nRetrieval filtering\\\\nZero-shot / CoT \u001B[0m\u001B[32m[\u001B[0m\u001B[32m27\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nNone\\\\nSpecific\\\\nVanilla \u001B[0m\n",
       "\u001B[32mRAG\\\\nVector search\\\\nSpecific\\\\nGraphRAG \u001B[0m\u001B[32m[\u001B[0m\u001B[32m12\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nTraverse\\\\nSpecific&abstract\\\\nLightRAG \u001B[0m\u001B[32m[\u001B[0m\u001B[32m17\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nExtract keywords + \u001B[0m\n",
       "\u001B[32mVector search\\\\nAbstract\\\\nHippoRAG \u001B[0m\u001B[32m[\u001B[0m\u001B[32m18\u001B[0m\u001B[32m]\u001B[0m\u001B[32m\\\\nPPR\\\\nSpecific\\\\nArchRAG \u001B[0m\u001B[32m(\u001B[0m\u001B[32mours\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\\\\nHierarchical \u001B[0m\n",
       "\u001B[32msearch\\\\nSpecific&abstract\\\\nSpecifically, Microsoft first proposes a graph-based RAG system\\\\ncalled GraphRAG \u001B[0m\n",
       "\u001B[32m[\u001B[0m\u001B[32m12\u001B[0m\u001B[32m]\u001B[0m\u001B[32m, which first builds a knowledge graph \u001B[0m\u001B[32m(\u001B[0m\u001B[32mKG\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\\\\nby extracting entities and relationships from the external \u001B[0m\n",
       "\u001B[32mcorpus,\\\\nthen employs the Leiden method \u001B[0m\u001B[32m[\u001B[0m\u001B[32m54\u001B[0m\u001B[32m]\u001B[0m\u001B[32m to detect communities and\\\\ngenerates a summary for each community \u001B[0m\n",
       "\u001B[32musing LLMs\\n\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG\u001B[0m\u001B[32m]\u001B[0m\u001B[32m \u001B[0m\n",
       "\u001B[32m.\\\\n2.3\\\\nMicrosoft\\\\u2019s Graph-RAG\\\\nMicrosoft proposed the Graph-RAG system \u001B[0m\u001B[32m[\u001B[0m\u001B[32m8\u001B[0m\u001B[32m]\u001B[0m\u001B[32m, which constructs a\\\\nknowledge\u001B[0m\n",
       "\u001B[32mgraph index with multi-level communities and employs\\\\ntailored strategies for both local and global search. In \u001B[0m\n",
       "\u001B[32mthis section,\\\\nwe focus on its indexing and retrieval operations for local search,\\\\nwhich are relevant to our \u001B[0m\n",
       "\u001B[32mwork.\\\\nAlgorithm 1 outlines the pseudo-code for constructing the graph\\\\nindex G = \u001B[0m\n",
       "\u001B[32m(\u001B[0m\u001B[32mV\\\\n\\\\ud835\\\\udc52\\\\u222aV\\\\n\\\\ud835\\\\udc61, E\u001B[0m\u001B[32m)\u001B[0m\u001B[32m, where V\\\\n\\\\ud835\\\\udc52and V\\\\n\\\\ud835\\\\udc61represent entities \u001B[0m\n",
       "\u001B[32mand\\\\ntext chunks, respectively. Given a text chunk set T, KG-Index first\\\\nKET-RAG: A Cost-Efficient \u001B[0m\n",
       "\u001B[32mMulti-Granular Indexing Framework for Graph-RAG\\\\nConference acronym \\\\u2019XX, June 03\\\\u201305, 2018, Woodstock, \u001B[0m\n",
       "\u001B[32mNY\\\\nAlgorithm 1: KG-Index \u001B[0m\u001B[32m(\u001B[0m\u001B[32mT\u001B[0m\u001B[32m)\u001B[0m\u001B[32m\\\\nInput: The text chunk set T.\\\\nOutput: A TAG index G\\n\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from \u001B[0m\n",
       "\u001B[32mRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001B[0m\u001B[32m]\u001B[0m\u001B[32m . We refer to this decoding procedure as \u001B[0m\n",
       "\u001B[32m\\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, requiring many forward \u001B[0m\n",
       "\u001B[32mpasses. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that p\\\\u03b8\u001B[0m\u001B[32m(\u001B[0m\u001B[32my|x, zi\u001B[0m\u001B[32m)\u001B[0m\u001B[32m \\\\u22480 \u001B[0m\n",
       "\u001B[32mwhere y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run additional forward passes \u001B[0m\n",
       "\u001B[32monce the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as \\\\u201cFast \u001B[0m\n",
       "\u001B[32mDecoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all \u001B[0m\n",
       "\u001B[32mexperiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. \u001B[0m\u001B[32m[\u001B[0m\u001B[32m31\u001B[0m\u001B[32m]\u001B[0m\u001B[32m \u001B[0m\n",
       "\u001B[32mand\\\\nKarpukhin et al. \u001B[0m\u001B[32m[\u001B[0m\u001B[32m26\u001B[0m\u001B[32m]\u001B[0m\u001B[32m, we use the December 2018 dump. Each Wikipedia article is split into \u001B[0m\n",
       "\u001B[32mdisjoint\\\\n100-word chunks, to make a total of 21M documents\\n\u001B[0m\u001B[32m[\u001B[0m\u001B[32mQuote from Retrieval-Augmented Generation for \u001B[0m\n",
       "\u001B[32mKnowledge-Intensive NLP Tasks\u001B[0m\u001B[32m]\u001B[0m\u001B[32m . In one approach, RAG-Sequence, the model uses the same document\\\\nto predict each \u001B[0m\n",
       "\u001B[32mtarget token. The second approach, RAG-Token, can predict each target token based\\\\non a different document. In the\u001B[0m\n",
       "\u001B[32mfollowing, we formally introduce both models and then describe the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as \u001B[0m\n",
       "\u001B[32mthe training and decoding procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same \u001B[0m\n",
       "\u001B[32mretrieved document to generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single \u001B[0m\n",
       "\u001B[32mlatent variable that\\\\nis marginalized to get the seq2seq probability p\u001B[0m\u001B[32m(\u001B[0m\u001B[32my|x\u001B[0m\u001B[32m)\u001B[0m\u001B[32m via a top-K approximation\\n\\n\\n \u001B[0m\n",
       "\u001B[32m(\u001B[0m\u001B[32mAnswer only from retrieval. Only cite sources that are used. Make your response conversational.\u001B[0m\u001B[32m)\u001B[0m\u001B[32m'\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\u001B[1;38;2;118;185;0m,\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m        \u001B[0m\u001B[1;35mHumanMessage\u001B[0m\u001B[1;38;2;118;185;0m(\u001B[0m\u001B[1;33mcontent\u001B[0m\u001B[1;38;2;118;185;0m=\u001B[0m\u001B[32m'Tell me about RAG!'\u001B[0m\u001B[1;38;2;118;185;0m)\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m    \u001B[0m\u001B[1;38;2;118;185;0m]\u001B[0m\n",
       "\u001B[1;38;2;118;185;0m)\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG, or Retrieval-Augmented Generation, is a method used in natural language processing that combines retrieval and generation techniques to improve the performance of language models on knowledge-intensive tasks.\n",
      "\n",
      "The RAG method was first proposed by Microsoft and is described in the paper \"ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation.\" According to the paper, RAG involves building a knowledge graph from an external corpus and then using this graph to retrieve relevant information for a given input prompt. The retrieved information is then used to generate a response using large language models (LLMs).\n",
      "\n",
      "There are different variants of RAG, including vanilla RAG, GraphRAG, LightRAG, and ArchRAG. Vanilla RAG uses a simple vector search method to retrieve relevant information, while GraphRAG uses a graph-based approach to build a knowledge graph and retrieves information using a traversal method. LightRAG extracts keywords and uses vector search to retrieve relevant information, while ArchRAG uses a hierarchical search method.\n",
      "\n",
      "Microsoft's Graph-RAG system is further described in the paper \"KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG.\" This paper discusses the indexing and retrieval operations used in Graph-RAG for local search.\n",
      "\n",
      "RAG uses a decoding procedure called \"Thorough Decoding,\" which involves generating multiple candidate responses and selecting the most likely one. However, for longer output sequences, a more efficient decoding method called \"Fast Decoding\" can be used, which avoids the need to run additional forward passes once the candidate set has been generated.\n",
      "\n",
      "The RAG method can be applied to a wide range of knowledge-intensive tasks and has been shown to achieve state-of-the-art performance on certain tasks. It should be noted that RAG requires a large knowledge source, such as a Wikipedia dump, to function effectively.\n",
      "\n",
      "Sources:\n",
      "\n",
      "* \"ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation\"\n",
      "* \"KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG\"\n",
      "* \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\""
     ]
    }
   ],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
    "# instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "stream_chain = chat_prompt| RPrint() | instruct_llm | StrOutputParser()\n",
    "\n",
    "################################################################################################\n",
    "## BEGIN TODO: Implement the retrieval chain to make your system work!\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    ## TODO: Make sure to retrieve history & context from convstore & docstore, respectively.\n",
    "    ## HINT: Our solution uses RunnableAssign, itemgetter, long_reorder, and docs2str\n",
    "    | RunnableAssign({'history' : lambda d: None})\n",
    "    | RunnableAssign({'context' : lambda d: None})\n",
    ")\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    ## TODO: Make sure to retrieve history & context from convstore & docstore, respectively.\n",
    "    ## HINT: Our solution uses RunnableAssign, itemgetter, long_reorder, and docs2str\n",
    "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str})\n",
    "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
    "    | RPrint()\n",
    ")\n",
    "## END TODO\n",
    "################################################################################################\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ## First perform the retrieval based on the input message\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
    "\n",
    "\n",
    "## Start of Agent Event Loop\n",
    "test_question = \"Tell me about RAG!\"  ## <- modify as desired\n",
    "\n",
    "## Before you launch your gradio interface, make sure your thing works\n",
    "for response in chat_gen(test_question, return_buffer=False):\n",
    "    print(response, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9W7sC5Z6BfqM",
   "metadata": {
    "id": "9W7sC5Z6BfqM"
   },
   "source": [
    "### **ä»»åŠ¡ 4ï¼š** ä¸ Gradio èŠå¤©æœºå™¨äººäº¤äº’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fMP3l7QL2JWT",
   "metadata": {
    "id": "fMP3l7QL2JWT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.41.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://9559e568fdd1714603.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9559e568fdd1714603.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://9559e568fdd1714603.gradio.live\n",
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCb3RVVfbmQ0",
   "metadata": {
    "id": "yCb3RVVfbmQ0"
   },
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **ç¬¬ 4 éƒ¨åˆ†ï¼š** ä¿å­˜ç´¢å¼•ä»¥ç”¨äºè¯„ä¼°\n",
    "\n",
    "å®ç° RAG é“¾åï¼Œè¯·å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/integrations/vectorstores/faiss#saving-and-loading)ä¿å­˜æ‚¨ç§¯ç´¯å‡ºæ¥çš„å‘é‡å­˜å‚¨ã€‚æœ€åçš„è¯„ä¼°ä¼šç”¨åˆ°ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "Y4se5wQ4Afda",
   "metadata": {
    "id": "Y4se5wQ4Afda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    }
   ],
   "source": [
    "## Save and compress your index\n",
    "docstore.save_local(\"docstore_index\")\n",
    "!tar czvf docstore_index.tgz docstore_index\n",
    "\n",
    "!rm -rf docstore_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LsI7NivbIgFw",
   "metadata": {
    "id": "LsI7NivbIgFw"
   },
   "source": [
    "å¦‚æœæ‰€æœ‰å†…å®¹éƒ½å·²æ­£ç¡®ä¿å­˜ï¼Œå°±å¯ä»¥æ‰§è¡Œä»¥ä¸‹ä»£ç ä» `tgz` å‹ç¼©æ–‡ä»¶æ‹¿åˆ°ç´¢å¼•äº†ï¼ˆåªè¦å®‰è£…å¥½äº† pip ç¯å¢ƒï¼‰ã€‚å½“æ‚¨ç¡®è®¤è¿™ä¸ªä»£ç å•å…ƒèƒ½æ‹¿åˆ°æ‚¨çš„ç´¢å¼•ä¹‹åï¼ŒæŠŠ `docstore_index.tgz` ä¸‹è½½ä¸‹æ¥ï¼Œä¸‹ä¸ª notebook ä¼šç”¨åˆ°ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "Qs8820ucIu1t",
   "metadata": {
    "id": "Qs8820ucIu1t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n",
      ".\\n2.3\\nMicrosoft\\u2019s Graph-RAG\\nMicrosoft proposed the Graph-RAG system [8], which constructs a\\nknowledge graph index with multi-level communities and employs\\ntailored strategies for both local and global search. In this section,\\nwe focus on its indexing and retrieval operations for local search,\\nwhich are relevant to our work.\\nAlgorithm 1 outlines the pseudo-code for constructing the graph\\nindex G = (V\\n\\ud835\\udc52\\u222aV\\n\\ud835\\udc61, E), where V\\n\\ud835\\udc52and V\\n\\ud835\\udc61represent entities and\\ntext chunks, respectively. Given a text chunk set T, KG-Index first\\nKET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG\\nConference acronym \\u2019XX, June 03\\u201305, 2018, Woodstock, NY\\nAlgorithm 1: KG-Index (T)\\nInput: The text chunk set T.\\nOutput: A TAG index G\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "!tar xzvf docstore_index.tgz\n",
    "new_db = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = new_db.similarity_search(\"Testing the index\")\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "as_3vWJGKB2F",
   "metadata": {
    "id": "as_3vWJGKB2F"
   },
   "source": [
    "----\n",
    "\n",
    "## **ç¬¬ 5 éƒ¨åˆ†ï¼š** æ€»ç»“\n",
    "\n",
    "æ­å–œï¼å¦‚æœæ‚¨çš„ RAG é“¾èƒ½æ­£å¸¸è¿è¡Œï¼Œå°±ç»§ç»­è¿›å…¥ 08_evaluation.ipynb è¿›è¡Œ **RAG è¯„ä¼°**å§ï¼\n",
    "\n",
    "### <font color=\"#76b900\">**éå¸¸å¥½ï¼**</font>\n",
    "\n",
    "### **æ¥ä¸‹æ¥**ï¼š\n",
    "**[å¯é€‰]** å›é¡¾ notebook é¡¶éƒ¨çš„â€œæ€è€ƒé—®é¢˜â€ã€‚"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
